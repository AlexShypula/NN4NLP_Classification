{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Classifier4.ipynb","provenance":[{"file_id":"1ufq2JpnXay_JsRo4vhhIGSUvjRw5er7O","timestamp":1580232037354},{"file_id":"1uIeQCgKK4IzV6nCuridHnIOjGxlEtQ8f","timestamp":1579903800063},{"file_id":"1pMsQwNuwLvpMdHY_Vj9c467qvQCMAyJi","timestamp":1579901507622}]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e9f0478fddcc4c48a1d17c40b1aa2a5e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d0847889d153422ab3c615eabbb2e44d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9f8202e8f79141208304bb6224ca5a83","IPY_MODEL_d29e532824e44b7cbd583eecb3b8b1c3"]}},"d0847889d153422ab3c615eabbb2e44d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9f8202e8f79141208304bb6224ca5a83":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6b1515b7012e4b9889544bc4e455f20a","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"danger","max":2000000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1999996,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9ac567cecb1346b188f5fc41b4120858"}},"d29e532824e44b7cbd583eecb3b8b1c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6de4cc2b40a649f19b31c7b9466d06b8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 1999996/2000000 [02:04&lt;00:00, 16021.30it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6b9d8c4a28694ff7be68aeec1490d718"}},"6b1515b7012e4b9889544bc4e455f20a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9ac567cecb1346b188f5fc41b4120858":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6de4cc2b40a649f19b31c7b9466d06b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6b9d8c4a28694ff7be68aeec1490d718":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e2a3e79a7ef04081aa6326195ab7b57c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e79e20e83fb147f58de6a3b4395751d1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_853cfe128c95493780f8cd9e89ca9c74","IPY_MODEL_1318283686394b75981d77f991018544"]}},"e79e20e83fb147f58de6a3b4395751d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"853cfe128c95493780f8cd9e89ca9c74":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5f60ad28b974487a8cfdf33b83cabf03","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"danger","max":3968,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_63e9fa5186154cb29928ddba8466884a"}},"1318283686394b75981d77f991018544":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9e0b2d05d1a14b71b2017f7e9246eea9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"  0% 0/3968 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_63c1da1c433f4387afb1a11ae1a9ce5c"}},"5f60ad28b974487a8cfdf33b83cabf03":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"63e9fa5186154cb29928ddba8466884a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9e0b2d05d1a14b71b2017f7e9246eea9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"63c1da1c433f4387afb1a11ae1a9ce5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"wb2bLnjL3W88","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import torchtext\n","\n","from torchtext.data import TabularDataset, Field, Iterator\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import Vocab\n","from torch import optim \n","from torch.optim import lr_scheduler\n","import matplotlib.pyplot as plt\n","\n","import copy\n","import time\n","from collections import namedtuple\n","from tqdm import tqdm, tqdm_notebook"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2CDc4bD1pJu9","colab_type":"code","colab":{}},"source":["del get_tokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPoW2wf8pLqU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tUsj2ZFXmTBF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"965184ff-8e8b-452a-84b2-c40c41e0ef91","executionInfo":{"status":"error","timestamp":1580256987839,"user_tz":300,"elapsed":3950,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["from collections import Counter, OrderedDict\n","from itertools import chain\n","import six\n","import torch\n","from tqdm import tqdm\n","\n","from torchtext.data.dataset import Dataset\n","from torchtext.data.pipeline import Pipeline\n","#from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import Vocab, SubwordVocab\n","\n","from functools import partial\n","\n","\n","def _spacy_tokenize(x, spacy):\n","    return [tok.text for tok in spacy.tokenizer(x)]\n","\n","\n","_patterns = [r'\\'',\n","             r'\\\"',\n","             r'\\.',\n","             r'<br \\/>',\n","             r',',\n","             r'\\(',\n","             r'\\)',\n","             r'\\!',\n","             r'\\?',\n","             r'\\;',\n","             r'\\:',\n","             r'\\s+']\n","\n","_replacements = [' \\'  ',\n","                 '',\n","                 ' . ',\n","                 ' ',\n","                 ' , ',\n","                 ' ( ',\n","                 ' ) ',\n","                 ' ! ',\n","                 ' ? ',\n","                 ' ',\n","                 ' ',\n","                 ' ']\n","\n","_patterns_dict = list((re.compile(p), r) for p, r in zip(_patterns, _replacements))\n","\n","\n","def _basic_english_normalize(line):\n","    r\"\"\"\n","    Basic normalization for a line of text.\n","    Normalization includes\n","    - lowercasing\n","    - complete some basic text normalization for English words as follows:\n","        add spaces before and after '\\''\n","        remove '\\\"',\n","        add spaces before and after '.'\n","        replace '<br \\/>'with single space\n","        add spaces before and after ','\n","        add spaces before and after '('\n","        add spaces before and after ')'\n","        add spaces before and after '!'\n","        add spaces before and after '?'\n","        replace ';' with single space\n","        replace ':' with single space\n","        replace multiple spaces with single space\n","    Returns a list of tokens after splitting on whitespace.\n","    \"\"\"\n","\n","    line = line.lower()\n","    for pattern_re, replaced_str in _patterns_dict:\n","        line = pattern_re.sub(replaced_str, line)\n","    return line.split()\n","\n","\n","\n","def get_tokenizer(tokenizer, language='en'):\n","    r\"\"\"\n","    Generate tokenizer function for a string sentence.\n","    Arguments:\n","        tokenizer: the name of tokenizer function. If None, it returns split()\n","            function, which splits the string sentence by space.\n","            If basic_english, it returns _basic_english_normalize() function,\n","            which normalize the string first and split by space. If a callable\n","            function, it will return the function. If a tokenizer library\n","            (e.g. spacy, moses, toktok, revtok, subword), it returns the\n","            corresponding library.\n","        language: Default en\n","    Examples:\n","        >>> import torchtext\n","        >>> from torchtext.data import get_tokenizer\n","        >>> tokenizer = get_tokenizer(\"basic_english\")\n","        >>> tokens = tokenizer(\"You can now install TorchText using pip!\")\n","        >>> tokens\n","        >>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\n","    \"\"\"\n","\n","    # default tokenizer is string.split(), added as a module function for serialization\n","    if tokenizer is None:\n","        return _split_tokenizer\n","\n","    if tokenizer == \"basic_english\":\n","        if language != 'en':\n","            raise ValueError(\"Basic normalization is only available for Enlish(en)\")\n","        return _basic_english_normalize\n","\n","    # simply return if a function is passed\n","    if callable(tokenizer):\n","        return tokenizer\n","\n","    if tokenizer == \"spacy\":\n","        try:\n","            import spacy\n","            spacy = spacy.load(language)\n","            return partial(_spacy_tokenize, spacy=spacy)\n","        except ImportError:\n","            print(\"Please install SpaCy. \"\n","                  \"See the docs at https://spacy.io for more information.\")\n","            raise\n","        except AttributeError:\n","            print(\"Please install SpaCy and the SpaCy {} tokenizer. \"\n","                  \"See the docs at https://spacy.io for more \"\n","                  \"information.\".format(language))\n","            raise\n","    elif tokenizer == \"moses\":\n","        try:\n","            from sacremoses import MosesTokenizer\n","            moses_tokenizer = MosesTokenizer()\n","            return moses_tokenizer.tokenize\n","        except ImportError:\n","            print(\"Please install SacreMoses. \"\n","                  \"See the docs at https://github.com/alvations/sacremoses \"\n","                  \"for more information.\")\n","            raise\n","    elif tokenizer == \"toktok\":\n","        try:\n","            from nltk.tokenize.toktok import ToktokTokenizer\n","            toktok = ToktokTokenizer()\n","            return toktok.tokenize\n","        except ImportError:\n","            print(\"Please install NLTK. \"\n","                  \"See the docs at https://nltk.org  for more information.\")\n","            raise\n","    elif tokenizer == 'revtok':\n","        try:\n","            import revtok\n","            return revtok.tokenize\n","        except ImportError:\n","            print(\"Please install revtok.\")\n","            raise\n","    elif tokenizer == 'subword':\n","        try:\n","            import revtok\n","            return partial(revtok.tokenize, decap=True)\n","        except ImportError:\n","            print(\"Please install revtok.\")\n","            raise\n","    raise ValueError(\"Requested tokenizer {}, valid choices are a \"\n","                     \"callable that takes a single string as input, \"\n","                     \"\\\"revtok\\\" for the revtok reversible tokenizer, \"\n","                     \"\\\"subword\\\" for the revtok caps-aware tokenizer, \"\n","                     \"\\\"spacy\\\" for the SpaCy English tokenizer, or \"\n","                     \"\\\"moses\\\" for the NLTK port of the Moses tokenization \"\n","                     \"script.\".format(tokenizer))\n","\n","\n","def dtype_to_attr(dtype):\n","    # convert torch.dtype to dtype string id\n","    # e.g. torch.int32 -> \"int32\"\n","    # used for serialization\n","    _, dtype = str(dtype).split('.')\n","    return dtype\n","\n","def is_tokenizer_serializable(tokenizer, language):\n","    \"\"\"Extend with other tokenizers which are found to not be serializable\n","    \"\"\"\n","    if tokenizer == 'spacy':\n","        return False\n","    return True\n","\n","class RawField(object):\n","    \"\"\" Defines a general datatype.\n","    Every dataset consists of one or more types of data. For instance, a text\n","    classification dataset contains sentences and their classes, while a\n","    machine translation dataset contains paired examples of text in two\n","    languages. Each of these types of data is represented by a RawField object.\n","    A RawField object does not assume any property of the data type and\n","    it holds parameters relating to how a datatype should be processed.\n","    Attributes:\n","        preprocessing: The Pipeline that will be applied to examples\n","            using this field before creating an example.\n","            Default: None.\n","        postprocessing: A Pipeline that will be applied to a list of examples\n","            using this field before assigning to a batch.\n","            Function signature: (batch(list)) -> object\n","            Default: None.\n","        is_target: Whether this field is a target variable.\n","            Affects iteration over batches. Default: False\n","    \"\"\"\n","\n","    def __init__(self, preprocessing=None, postprocessing=None, is_target=False):\n","        self.preprocessing = preprocessing\n","        self.postprocessing = postprocessing\n","        self.is_target = is_target\n","\n","    def preprocess(self, x):\n","        \"\"\" Preprocess an example if the `preprocessing` Pipeline is provided. \"\"\"\n","        if self.preprocessing is not None:\n","            return self.preprocessing(x)\n","        else:\n","            return x\n","\n","    def process(self, batch, *args, **kwargs):\n","        \"\"\" Process a list of examples to create a batch.\n","        Postprocess the batch with user-provided Pipeline.\n","        Args:\n","            batch (list(object)): A list of object from a batch of examples.\n","        Returns:\n","            object: Processed object given the input and custom\n","            postprocessing Pipeline.\n","        \"\"\"\n","        if self.postprocessing is not None:\n","            batch = self.postprocessing(batch)\n","        return batch\n","\n","\n","class Field(RawField):\n","    \"\"\"Defines a datatype together with instructions for converting to Tensor.\n","    Field class models common text processing datatypes that can be represented\n","    by tensors.  It holds a Vocab object that defines the set of possible values\n","    for elements of the field and their corresponding numerical representations.\n","    The Field object also holds other parameters relating to how a datatype\n","    should be numericalized, such as a tokenization method and the kind of\n","    Tensor that should be produced.\n","    If a Field is shared between two columns in a dataset (e.g., question and\n","    answer in a QA dataset), then they will have a shared vocabulary.\n","    Attributes:\n","        sequential: Whether the datatype represents sequential data. If False,\n","            no tokenization is applied. Default: True.\n","        use_vocab: Whether to use a Vocab object. If False, the data in this\n","            field should already be numerical. Default: True.\n","        init_token: A token that will be prepended to every example using this\n","            field, or None for no initial token. Default: None.\n","        eos_token: A token that will be appended to every example using this\n","            field, or None for no end-of-sentence token. Default: None.\n","        fix_length: A fixed length that all examples using this field will be\n","            padded to, or None for flexible sequence lengths. Default: None.\n","        dtype: The torch.dtype class that represents a batch of examples\n","            of this kind of data. Default: torch.long.\n","        preprocessing: The Pipeline that will be applied to examples\n","            using this field after tokenizing but before numericalizing. Many\n","            Datasets replace this attribute with a custom preprocessor.\n","            Default: None.\n","        postprocessing: A Pipeline that will be applied to examples using\n","            this field after numericalizing but before the numbers are turned\n","            into a Tensor. The pipeline function takes the batch as a list, and\n","            the field's Vocab.\n","            Default: None.\n","        lower: Whether to lowercase the text in this field. Default: False.\n","        tokenize: The function used to tokenize strings using this field into\n","            sequential examples. If \"spacy\", the SpaCy tokenizer is\n","            used. If a non-serializable function is passed as an argument,\n","            the field will not be able to be serialized. Default: string.split.\n","        tokenizer_language: The language of the tokenizer to be constructed.\n","            Various languages currently supported only in SpaCy.\n","        include_lengths: Whether to return a tuple of a padded minibatch and\n","            a list containing the lengths of each examples, or just a padded\n","            minibatch. Default: False.\n","        batch_first: Whether to produce tensors with the batch dimension first.\n","            Default: False.\n","        pad_token: The string token used as padding. Default: \"<pad>\".\n","        unk_token: The string token used to represent OOV words. Default: \"<unk>\".\n","        pad_first: Do the padding of the sequence at the beginning. Default: False.\n","        truncate_first: Do the truncating of the sequence at the beginning. Default: False\n","        stop_words: Tokens to discard during the preprocessing step. Default: None\n","        is_target: Whether this field is a target variable.\n","            Affects iteration over batches. Default: False\n","    \"\"\"\n","\n","    vocab_cls = Vocab\n","    # Dictionary mapping PyTorch tensor dtypes to the appropriate Python\n","    # numeric type.\n","    dtypes = {\n","        torch.float32: float,\n","        torch.float: float,\n","        torch.float64: float,\n","        torch.double: float,\n","        torch.float16: float,\n","        torch.half: float,\n","\n","        torch.uint8: int,\n","        torch.int8: int,\n","        torch.int16: int,\n","        torch.short: int,\n","        torch.int32: int,\n","        torch.int: int,\n","        torch.int64: int,\n","        torch.long: int,\n","    }\n","\n","    ignore = ['dtype', 'tokenize']\n","\n","    def __init__(self, sequential=True, use_vocab=True, init_token=None,\n","                 eos_token=None, fix_length=None, dtype=torch.long,\n","                 preprocessing=None, postprocessing=None, lower=False,\n","                 tokenize=None, tokenizer_language='en', include_lengths=False,\n","                 batch_first=False, pad_token=\"<pad>\", unk_token=\"<unk>\",\n","                 pad_first=False, truncate_first=False, stop_words=None,\n","                 is_target=False):\n","        self.sequential = sequential\n","        self.use_vocab = use_vocab\n","        self.init_token = init_token\n","        self.eos_token = eos_token\n","        self.unk_token = unk_token\n","        self.fix_length = fix_length\n","        self.dtype = dtype\n","        self.preprocessing = preprocessing\n","        self.postprocessing = postprocessing\n","        self.lower = lower\n","        # store params to construct tokenizer for serialization\n","        # in case the tokenizer isn't picklable (e.g. spacy)\n","        self.tokenizer_args = (tokenize, tokenizer_language)\n","        self.tokenize = get_tokenizer(tokenize, tokenizer_language)\n","        self.include_lengths = include_lengths\n","        self.batch_first = batch_first\n","        self.pad_token = pad_token if self.sequential else None\n","        self.pad_first = pad_first\n","        self.truncate_first = truncate_first\n","        try:\n","            self.stop_words = set(stop_words) if stop_words is not None else None\n","        except TypeError:\n","            raise ValueError(\"Stop words must be convertible to a set\")\n","        self.is_target = is_target\n","\n","    def __getstate__(self):\n","        str_type = dtype_to_attr(self.dtype)\n","        if is_tokenizer_serializable(*self.tokenizer_args):\n","            tokenize = self.tokenize\n","        else:\n","            # signal to restore in `__setstate__`\n","            tokenize = None\n","        attrs = {k: v for k, v in self.__dict__.items() if k not in self.ignore}\n","        attrs['dtype'] = str_type\n","        attrs['tokenize'] = tokenize\n","\n","        return attrs\n","\n","    def __setstate__(self, state):\n","        state['dtype'] = getattr(torch, state['dtype'])\n","        if not state['tokenize']:\n","            state['tokenize'] = get_tokenizer(*state['tokenizer_args'])\n","        self.__dict__.update(state)\n","\n","    def __hash__(self):\n","        # we don't expect this to be called often\n","        return 42\n","\n","    def __eq__(self, other):\n","        if not isinstance(other, RawField):\n","            return False\n","\n","        return self.__dict__ == other.__dict__\n","\n","    def preprocess(self, x):\n","        \"\"\"Load a single example using this field, tokenizing if necessary.\n","        If the input is a Python 2 `str`, it will be converted to Unicode\n","        first. If `sequential=True`, it will be tokenized. Then the input\n","        will be optionally lowercased and passed to the user-provided\n","        `preprocessing` Pipeline.\"\"\"\n","        if (six.PY2 and isinstance(x, six.string_types)\n","                and not isinstance(x, six.text_type)):\n","            x = Pipeline(lambda s: six.text_type(s, encoding='utf-8'))(x)\n","        if self.sequential and isinstance(x, six.text_type):\n","            x = self.tokenize(x.rstrip('\\n'))\n","        if self.lower:\n","            x = Pipeline(six.text_type.lower)(x)\n","        if self.sequential and self.use_vocab and self.stop_words is not None:\n","            x = [w for w in x if w not in self.stop_words]\n","        if self.preprocessing is not None:\n","            return self.preprocessing(x)\n","        else:\n","            return x\n","\n","    def process(self, batch, device=None):\n","        \"\"\" Process a list of examples to create a torch.Tensor.\n","        Pad, numericalize, and postprocess a batch and create a tensor.\n","        Args:\n","            batch (list(object)): A list of object from a batch of examples.\n","        Returns:\n","            torch.autograd.Variable: Processed object given the input\n","            and custom postprocessing Pipeline.\n","        \"\"\"\n","        padded = self.pad(batch)\n","        tensor = self.numericalize(padded, device=device)\n","        return tensor\n","\n","    def pad(self, minibatch):\n","        \"\"\"Pad a batch of examples using this field.\n","        Pads to self.fix_length if provided, otherwise pads to the length of\n","        the longest example in the batch. Prepends self.init_token and appends\n","        self.eos_token if those attributes are not None. Returns a tuple of the\n","        padded list and a list containing lengths of each example if\n","        `self.include_lengths` is `True` and `self.sequential` is `True`, else just\n","        returns the padded list. If `self.sequential` is `False`, no padding is applied.\n","        \"\"\"\n","        minibatch = list(minibatch)\n","        if not self.sequential:\n","            return minibatch\n","        if self.fix_length is None:\n","            max_len = max(len(x) for x in minibatch)\n","        else:\n","            max_len = self.fix_length + (\n","                self.init_token, self.eos_token).count(None) - 2\n","        padded, lengths = [], []\n","        for x in minibatch:\n","            if self.pad_first:\n","                padded.append(\n","                    [self.pad_token] * max(0, max_len - len(x))\n","                    + ([] if self.init_token is None else [self.init_token])\n","                    + list(x[-max_len:] if self.truncate_first else x[:max_len])\n","                    + ([] if self.eos_token is None else [self.eos_token]))\n","            else:\n","                padded.append(\n","                    ([] if self.init_token is None else [self.init_token])\n","                    + list(x[-max_len:] if self.truncate_first else x[:max_len])\n","                    + ([] if self.eos_token is None else [self.eos_token])\n","                    + [self.pad_token] * max(0, max_len - len(x)))\n","            lengths.append(len(padded[-1]) - max(0, max_len - len(x)))\n","        if self.include_lengths:\n","            return (padded, lengths)\n","        return padded\n","\n","    def build_vocab(self, *args, **kwargs):\n","        \"\"\"Construct the Vocab object for this field from one or more datasets.\n","        Arguments:\n","            Positional arguments: Dataset objects or other iterable data\n","                sources from which to construct the Vocab object that\n","                represents the set of possible values for this field. If\n","                a Dataset object is provided, all columns corresponding\n","                to this field are used; individual columns can also be\n","                provided directly.\n","            Remaining keyword arguments: Passed to the constructor of Vocab.\n","        \"\"\"\n","        counter = Counter()\n","        sources = []\n","        for arg in args:\n","            if isinstance(arg, Dataset):\n","                sources += [getattr(arg, name) for name, field in\n","                            arg.fields.items() if field is self]\n","            else:\n","                sources.append(arg)\n","        for data in sources:\n","            for x in data:\n","                if not self.sequential:\n","                    x = [x]\n","                try:\n","                    counter.update(x)\n","                except TypeError:\n","                    counter.update(chain.from_iterable(x))\n","        specials = list(OrderedDict.fromkeys(\n","            tok for tok in [self.unk_token, self.pad_token, self.init_token,\n","                            self.eos_token] + kwargs.pop('specials', [])\n","            if tok is not None))\n","        self.vocab = self.vocab_cls(counter, specials=specials, **kwargs)\n","\n","    def numericalize(self, arr, device=None):\n","        \"\"\"Turn a batch of examples that use this field into a Variable.\n","        If the field has include_lengths=True, a tensor of lengths will be\n","        included in the return value.\n","        Arguments:\n","            arr (List[List[str]], or tuple of (List[List[str]], List[int])):\n","                List of tokenized and padded examples, or tuple of List of\n","                tokenized and padded examples and List of lengths of each\n","                example if self.include_lengths is True.\n","            device (str or torch.device): A string or instance of `torch.device`\n","                specifying which device the Variables are going to be created on.\n","                If left as default, the tensors will be created on cpu. Default: None.\n","        \"\"\"\n","        if self.include_lengths and not isinstance(arr, tuple):\n","            raise ValueError(\"Field has include_lengths set to True, but \"\n","                             \"input data is not a tuple of \"\n","                             \"(data batch, batch lengths).\")\n","        if isinstance(arr, tuple):\n","            arr, lengths = arr\n","            lengths = torch.tensor(lengths, dtype=self.dtype, device=device)\n","\n","        if self.use_vocab:\n","            if self.sequential:\n","                arr = [[self.vocab.stoi[x] for x in ex] for ex in arr]\n","            else:\n","                arr = [self.vocab.stoi[x] for x in arr]\n","\n","            if self.postprocessing is not None:\n","                arr = self.postprocessing(arr, self.vocab)\n","        else:\n","            if self.dtype not in self.dtypes:\n","                raise ValueError(\n","                    \"Specified Field dtype {} can not be used with \"\n","                    \"use_vocab=False because we do not know how to numericalize it. \"\n","                    \"Please raise an issue at \"\n","                    \"https://github.com/pytorch/text/issues\".format(self.dtype))\n","            numericalization_func = self.dtypes[self.dtype]\n","            # It doesn't make sense to explicitly coerce to a numeric type if\n","            # the data is sequential, since it's unclear how to coerce padding tokens\n","            # to a numeric type.\n","            if not self.sequential:\n","                arr = [numericalization_func(x) if isinstance(x, six.string_types)\n","                       else x for x in arr]\n","            if self.postprocessing is not None:\n","                arr = self.postprocessing(arr, None)\n","\n","        print(f\"arr is {arr}\")\n","\n","        var = torch.tensor(arr, dtype=self.dtype, device=device)\n","\n","        if self.sequential and not self.batch_first:\n","            var.t_()\n","        if self.sequential:\n","            var = var.contiguous()\n","\n","        if self.include_lengths:\n","            return var, lengths\n","        return var\n","\n","\n","class ReversibleField(Field):\n","    def __init__(self, **kwargs):\n","        if kwargs.get('tokenize') is list:\n","            self.use_revtok = False\n","        else:\n","            self.use_revtok = True\n","        if kwargs.get('tokenize') is None:\n","            kwargs['tokenize'] = 'revtok'\n","        if 'unk_token' not in kwargs:\n","            kwargs['unk_token'] = ' UNK '\n","        super(ReversibleField, self).__init__(**kwargs)\n","\n","    def reverse(self, batch):\n","        if self.use_revtok:\n","            try:\n","                import revtok\n","            except ImportError:\n","                print(\"Please install revtok.\")\n","                raise\n","        if not self.batch_first:\n","            batch = batch.t()\n","        with torch.cuda.device_of(batch):\n","            batch = batch.tolist()\n","        batch = [[self.vocab.itos[ind] for ind in ex] for ex in batch]  # denumericalize\n","\n","        def trim(s, t):\n","            sentence = []\n","            for w in s:\n","                if w == t:\n","                    break\n","                sentence.append(w)\n","            return sentence\n","\n","        batch = [trim(ex, self.eos_token) for ex in batch]  # trim past frst eos\n","\n","        def filter_special(tok):\n","            return tok not in (self.init_token, self.pad_token)\n","\n","        batch = [filter(filter_special, ex) for ex in batch]\n","        if self.use_revtok:\n","            return [revtok.detokenize(ex) for ex in batch]\n","        return [''.join(ex) for ex in batch]\n","\n","\n","class SubwordField(ReversibleField):\n","    vocab_cls = SubwordVocab\n","\n","    def __init__(self, **kwargs):\n","        kwargs['tokenize'] = 'subword'\n","        if 'unk_token' not in kwargs:\n","            kwargs['unk_token'] = '�'\n","        super(SubwordField, self).__init__(**kwargs)\n","\n","    def segment(self, *args):\n","        \"\"\"Segment one or more datasets with this subword field.\n","        Arguments:\n","            Positional arguments: Dataset objects or other indexable\n","                mutable sequences to segment. If a Dataset object is provided,\n","                all columns corresponding to this field are used; individual\n","                columns can also be provided directly.\n","        \"\"\"\n","        sources = []\n","        for arg in args:\n","            if isinstance(arg, Dataset):\n","                sources += [getattr(arg, name) for name, field in\n","                            arg.fields.items() if field is self]\n","            else:\n","                sources.append(arg)\n","        for data in sources:\n","            for x in tqdm(data, 'segmenting'):\n","                x[:] = self.vocab.segment(x)\n","\n","\n","class NestedField(Field):\n","    \"\"\"A nested field.\n","    A nested field holds another field (called *nesting field*), accepts an untokenized\n","    string or a list string tokens and groups and treats them as one field as described\n","    by the nesting field. Every token will be preprocessed, padded, etc. in the manner\n","    specified by the nesting field. Note that this means a nested field always has\n","    ``sequential=True``. The two fields' vocabularies will be shared. Their\n","    numericalization results will be stacked into a single tensor. And NestedField will\n","    share the same include_lengths with nesting_field, so one shouldn't specify the\n","    include_lengths in the nesting_field. This field is\n","    primarily used to implement character embeddings. See ``tests/data/test_field.py``\n","    for examples on how to use this field.\n","    Arguments:\n","        nesting_field (Field): A field contained in this nested field.\n","        use_vocab (bool): Whether to use a Vocab object. If False, the data in this\n","            field should already be numerical. Default: ``True``.\n","        init_token (str): A token that will be prepended to every example using this\n","            field, or None for no initial token. Default: ``None``.\n","        eos_token (str): A token that will be appended to every example using this\n","            field, or None for no end-of-sentence token. Default: ``None``.\n","        fix_length (int): A fixed length that all examples using this field will be\n","            padded to, or ``None`` for flexible sequence lengths. Default: ``None``.\n","        dtype: The torch.dtype class that represents a batch of examples\n","            of this kind of data. Default: ``torch.long``.\n","        preprocessing (Pipeline): The Pipeline that will be applied to examples\n","            using this field after tokenizing but before numericalizing. Many\n","            Datasets replace this attribute with a custom preprocessor.\n","            Default: ``None``.\n","        postprocessing (Pipeline): A Pipeline that will be applied to examples using\n","            this field after numericalizing but before the numbers are turned\n","            into a Tensor. The pipeline function takes the batch as a list, and\n","            the field's Vocab. Default: ``None``.\n","        include_lengths: Whether to return a tuple of a padded minibatch and\n","            a list containing the lengths of each examples, or just a padded\n","            minibatch. Default: False.\n","        tokenize: The function used to tokenize strings using this field into\n","            sequential examples. If \"spacy\", the SpaCy tokenizer is\n","            used. If a non-serializable function is passed as an argument,\n","            the field will not be able to be serialized. Default: string.split.\n","        tokenizer_language: The language of the tokenizer to be constructed.\n","            Various languages currently supported only in SpaCy.\n","        pad_token (str): The string token used as padding. If ``nesting_field`` is\n","            sequential, this will be set to its ``pad_token``. Default: ``\"<pad>\"``.\n","        pad_first (bool): Do the padding of the sequence at the beginning. Default:\n","            ``False``.\n","    \"\"\"\n","\n","    def __init__(self, nesting_field, use_vocab=True, init_token=None, eos_token=None,\n","                 fix_length=None, dtype=torch.long, preprocessing=None,\n","                 postprocessing=None, tokenize=None, tokenizer_language='en',\n","                 include_lengths=False, pad_token='<pad>',\n","                 pad_first=False, truncate_first=False):\n","        if isinstance(nesting_field, NestedField):\n","            raise ValueError('nesting field must not be another NestedField')\n","        if nesting_field.include_lengths:\n","            raise ValueError('nesting field cannot have include_lengths=True')\n","\n","        if nesting_field.sequential:\n","            pad_token = nesting_field.pad_token\n","        super(NestedField, self).__init__(\n","            use_vocab=use_vocab,\n","            init_token=init_token,\n","            eos_token=eos_token,\n","            fix_length=fix_length,\n","            dtype=dtype,\n","            preprocessing=preprocessing,\n","            postprocessing=postprocessing,\n","            lower=nesting_field.lower,\n","            tokenize=tokenize,\n","            tokenizer_language=tokenizer_language,\n","            batch_first=True,\n","            pad_token=pad_token,\n","            unk_token=nesting_field.unk_token,\n","            pad_first=pad_first,\n","            truncate_first=truncate_first,\n","            include_lengths=include_lengths\n","        )\n","        self.nesting_field = nesting_field\n","        # in case the user forget to do that\n","        self.nesting_field.batch_first = True\n","\n","    def preprocess(self, xs):\n","        \"\"\"Preprocess a single example.\n","        Firstly, tokenization and the supplied preprocessing pipeline is applied. Since\n","        this field is always sequential, the result is a list. Then, each element of\n","        the list is preprocessed using ``self.nesting_field.preprocess`` and the resulting\n","        list is returned.\n","        Arguments:\n","            xs (list or str): The input to preprocess.\n","        Returns:\n","            list: The preprocessed list.\n","        \"\"\"\n","        return [self.nesting_field.preprocess(x)\n","                for x in super(NestedField, self).preprocess(xs)]\n","\n","    def pad(self, minibatch):\n","        \"\"\"Pad a batch of examples using this field.\n","        If ``self.nesting_field.sequential`` is ``False``, each example in the batch must\n","        be a list of string tokens, and pads them as if by a ``Field`` with\n","        ``sequential=True``. Otherwise, each example must be a list of list of tokens.\n","        Using ``self.nesting_field``, pads the list of tokens to\n","        ``self.nesting_field.fix_length`` if provided, or otherwise to the length of the\n","        longest list of tokens in the batch. Next, using this field, pads the result by\n","        filling short examples with ``self.nesting_field.pad_token``.\n","        Example:\n","            >>> import pprint\n","            >>> pp = pprint.PrettyPrinter(indent=4)\n","            >>>\n","            >>> nesting_field = Field(pad_token='<c>', init_token='<w>', eos_token='</w>')\n","            >>> field = NestedField(nesting_field, init_token='<s>', eos_token='</s>')\n","            >>> minibatch = [\n","            ...     [list('john'), list('loves'), list('mary')],\n","            ...     [list('mary'), list('cries')],\n","            ... ]\n","            >>> padded = field.pad(minibatch)\n","            >>> pp.pprint(padded)\n","            [   [   ['<w>', '<s>', '</w>', '<c>', '<c>', '<c>', '<c>'],\n","                    ['<w>', 'j', 'o', 'h', 'n', '</w>', '<c>'],\n","                    ['<w>', 'l', 'o', 'v', 'e', 's', '</w>'],\n","                    ['<w>', 'm', 'a', 'r', 'y', '</w>', '<c>'],\n","                    ['<w>', '</s>', '</w>', '<c>', '<c>', '<c>', '<c>']],\n","                [   ['<w>', '<s>', '</w>', '<c>', '<c>', '<c>', '<c>'],\n","                    ['<w>', 'm', 'a', 'r', 'y', '</w>', '<c>'],\n","                    ['<w>', 'c', 'r', 'i', 'e', 's', '</w>'],\n","                    ['<w>', '</s>', '</w>', '<c>', '<c>', '<c>', '<c>'],\n","                    ['<c>', '<c>', '<c>', '<c>', '<c>', '<c>', '<c>']]]\n","        Arguments:\n","            minibatch (list): Each element is a list of string if\n","                ``self.nesting_field.sequential`` is ``False``, a list of list of string\n","                otherwise.\n","        Returns:\n","            list: The padded minibatch. or (padded, sentence_lens, word_lengths)\n","        \"\"\"\n","        minibatch = list(minibatch)\n","        if not self.nesting_field.sequential:\n","            return super(NestedField, self).pad(minibatch)\n","\n","        # Save values of attributes to be monkeypatched\n","        old_pad_token = self.pad_token\n","        old_init_token = self.init_token\n","        old_eos_token = self.eos_token\n","        old_fix_len = self.nesting_field.fix_length\n","        # Monkeypatch the attributes\n","        if self.nesting_field.fix_length is None:\n","            max_len = max(len(xs) for ex in minibatch for xs in ex)\n","            fix_len = max_len + 2 - (self.nesting_field.init_token,\n","                                     self.nesting_field.eos_token).count(None)\n","            self.nesting_field.fix_length = fix_len\n","        self.pad_token = [self.pad_token] * self.nesting_field.fix_length\n","        if self.init_token is not None:\n","            # self.init_token = self.nesting_field.pad([[self.init_token]])[0]\n","            self.init_token = [self.init_token]\n","        if self.eos_token is not None:\n","            # self.eos_token = self.nesting_field.pad([[self.eos_token]])[0]\n","            self.eos_token = [self.eos_token]\n","        # Do padding\n","        old_include_lengths = self.include_lengths\n","        self.include_lengths = True\n","        self.nesting_field.include_lengths = True\n","        padded, sentence_lengths = super(NestedField, self).pad(minibatch)\n","        padded_with_lengths = [self.nesting_field.pad(ex) for ex in padded]\n","        word_lengths = []\n","        final_padded = []\n","        max_sen_len = len(padded[0])\n","        for (pad, lens), sentence_len in zip(padded_with_lengths, sentence_lengths):\n","            if sentence_len == max_sen_len:\n","                lens = lens\n","                pad = pad\n","            elif self.pad_first:\n","                lens[:(max_sen_len - sentence_len)] = (\n","                    [0] * (max_sen_len - sentence_len))\n","                pad[:(max_sen_len - sentence_len)] = (\n","                    [self.pad_token] * (max_sen_len - sentence_len))\n","            else:\n","                lens[-(max_sen_len - sentence_len):] = (\n","                    [0] * (max_sen_len - sentence_len))\n","                pad[-(max_sen_len - sentence_len):] = (\n","                    [self.pad_token] * (max_sen_len - sentence_len))\n","            word_lengths.append(lens)\n","            final_padded.append(pad)\n","        padded = final_padded\n","\n","        # Restore monkeypatched attributes\n","        self.nesting_field.fix_length = old_fix_len\n","        self.pad_token = old_pad_token\n","        self.init_token = old_init_token\n","        self.eos_token = old_eos_token\n","        self.include_lengths = old_include_lengths\n","        if self.include_lengths:\n","            return padded, sentence_lengths, word_lengths\n","        return padded\n","\n","    def build_vocab(self, *args, **kwargs):\n","        \"\"\"Construct the Vocab object for nesting field and combine it with this field's vocab.\n","        Arguments:\n","            Positional arguments: Dataset objects or other iterable data\n","                sources from which to construct the Vocab object that\n","                represents the set of possible values for the nesting field. If\n","                a Dataset object is provided, all columns corresponding\n","                to this field are used; individual columns can also be\n","                provided directly.\n","            Remaining keyword arguments: Passed to the constructor of Vocab.\n","        \"\"\"\n","        sources = []\n","        for arg in args:\n","            if isinstance(arg, Dataset):\n","                sources.extend(\n","                    [getattr(arg, name) for name, field in arg.fields.items()\n","                     if field is self]\n","                )\n","            else:\n","                sources.append(arg)\n","\n","        flattened = []\n","        for source in sources:\n","            flattened.extend(source)\n","        old_vectors = None\n","        old_unk_init = None\n","        old_vectors_cache = None\n","        if \"vectors\" in kwargs.keys():\n","            old_vectors = kwargs[\"vectors\"]\n","            kwargs[\"vectors\"] = None\n","        if \"unk_init\" in kwargs.keys():\n","            old_unk_init = kwargs[\"unk_init\"]\n","            kwargs[\"unk_init\"] = None\n","        if \"vectors_cache\" in kwargs.keys():\n","            old_vectors_cache = kwargs[\"vectors_cache\"]\n","            kwargs[\"vectors_cache\"] = None\n","        # just build vocab and does not load vector\n","        self.nesting_field.build_vocab(*flattened, **kwargs)\n","        super(NestedField, self).build_vocab()\n","        self.vocab.extend(self.nesting_field.vocab)\n","        self.vocab.freqs = self.nesting_field.vocab.freqs.copy()\n","        if old_vectors is not None:\n","            self.vocab.load_vectors(old_vectors,\n","                                    unk_init=old_unk_init, cache=old_vectors_cache)\n","\n","        self.nesting_field.vocab = self.vocab\n","\n","    def numericalize(self, arrs, device=None):\n","        \"\"\"Convert a padded minibatch into a variable tensor.\n","        Each item in the minibatch will be numericalized independently and the resulting\n","        tensors will be stacked at the first dimension.\n","        Arguments:\n","            arr (List[List[str]]): List of tokenized and padded examples.\n","            device (str or torch.device): A string or instance of `torch.device`\n","                specifying which device the Variables are going to be created on.\n","                If left as default, the tensors will be created on cpu. Default: None.\n","        \"\"\"\n","        numericalized = []\n","        self.nesting_field.include_lengths = False\n","        if self.include_lengths:\n","            arrs, sentence_lengths, word_lengths = arrs\n","\n","        for arr in arrs:\n","            numericalized_ex = self.nesting_field.numericalize(\n","                arr, device=device)\n","            numericalized.append(numericalized_ex)\n","        padded_batch = torch.stack(numericalized)\n","\n","        self.nesting_field.include_lengths = True\n","        if self.include_lengths:\n","            sentence_lengths = \\\n","                torch.tensor(sentence_lengths, dtype=self.dtype, device=device)\n","            word_lengths = torch.tensor(word_lengths, dtype=self.dtype, device=device)\n","            return (padded_batch, sentence_lengths, word_lengths)\n","        return padded_batch\n","\n","\n","class LabelField(Field):\n","    \"\"\"A Label field.\n","    A label field is a shallow wrapper around a standard field designed to hold labels\n","    for a classification task. Its only use is to set the unk_token and sequential to\n","    `None` by default.\n","    \"\"\"\n","\n","    def __init__(self, **kwargs):\n","        # whichever value is set for sequential, unk_token, and is_target\n","        # will be overwritten\n","        kwargs['sequential'] = False\n","        kwargs['unk_token'] = None\n","        kwargs['is_target'] = True\n","\n","        super(LabelField, self).__init__(**kwargs)"],"execution_count":122,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-122-e99d0c52ac92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m                  ' ']\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0m_patterns_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_patterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_replacements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-122-e99d0c52ac92>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m                  ' ']\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0m_patterns_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_patterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_replacements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 're' is not defined"]}]},{"cell_type":"code","metadata":{"id":"rz8AqPnI3kNO","colab_type":"code","outputId":"99e69c60-b953-462b-a3c4-200eac26af51","executionInfo":{"status":"ok","timestamp":1580253925055,"user_tz":300,"elapsed":1833,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fxKQyqa438P0","colab_type":"code","outputId":"c6ee6eb6-49a2-48d0-b7c8-a556c5d286c7","executionInfo":{"status":"ok","timestamp":1580250949986,"user_tz":300,"elapsed":1416,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd drive/\"My Drive\"/NN4NLP/topicclass"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/NN4NLP/topicclass\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X-nQhfuF4KOa","colab_type":"code","outputId":"f43758ae-c1ae-48bd-a64b-abac9982b941","executionInfo":{"status":"ok","timestamp":1580250959006,"user_tz":300,"elapsed":4706,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":398}},"source":["! ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":["bestMobile_v2_NetModel.pt\t    Model2.pt\n","Classifier.ipynb\t\t    Model_3.png\n","ConvNetClassificationTesting_2.txt  Model_3.pt\n","ConvNetClassificationTesting_3.txt  Model_4.png\n","ConvNetClassificationTesting_4.txt  Model_4.pt\n","ConvNetClassificationTesting.txt    topicclass_test.csv\n","crawl-300d-2M-subword.bin\t    topicclass_test.txt\n","crawl-300d-2M-subword.vec\t    topicclass_train.csv\n","crawl-300d-2M-subword.zip\t    topicclass_train.txt\n","crawl-300d-2M.vec\t\t    topicclass_valid.csv\n","crawl-300d-2M.vec.zip\t\t    topicclass_valid_fixed.csv\n","int2Class.csv\t\t\t    topicclass_valid_fixed.gsheet\n","MobileNetModel_11.pt\t\t    topicclass_valid_fixed.txt\n","MobileNetModel_2.pt\t\t    topicclass_valid.txt\n","MobileNetModel_3.pt\t\t    Training1_NetPlot.png\n","MobileNetModel_4.pt\t\t    try1_NetModel.pt\n","MobileNetModel_5.pt\t\t    txtToCsv.ipynb\n","MobileNetModel_6.pt\t\t    VerboseConvNetClassificationTesting_2.txt\n","MobileNetModel_7.pt\t\t    VerboseConvNetClassificationTesting_3.txt\n","MobileNetModel_8.pt\t\t    VerboseConvNetClassificationTesting_4.txt\n","MobileNetModel_9.pt\t\t    VerboseConvNetClassificationTesting.txt\n","Model2.png\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MZkErq3QwtBH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":208},"outputId":"bdc46969-5ceb-4675-e9d5-722d0dceac02","executionInfo":{"status":"ok","timestamp":1580242136822,"user_tz":300,"elapsed":138715,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["# ! wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\""],"execution_count":10,"outputs":[{"output_type":"stream","text":["--2020-01-28 20:06:39--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:16a6, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1523785255 (1.4G) [application/zip]\n","Saving to: ‘crawl-300d-2M.vec.zip’\n","\n","crawl-300d-2M.vec.z 100%[===================>]   1.42G  10.9MB/s    in 2m 15s  \n","\n","2020-01-28 20:08:55 (10.8 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S6_S8UjXw9Wl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":165},"outputId":"39a407cc-f9cf-4bf3-cb70-3ed50d45749f"},"source":["#! wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-01-28 20:08:58--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:16a6, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5828358084 (5.4G) [application/zip]\n","Saving to: ‘crawl-300d-2M-subword.zip’\n","\n","        crawl-300d-  43%[=======>            ]   2.35G  11.0MB/s    eta 4m 50s "],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G24B1SFmxfAO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"cc8896aa-6416-488d-9563-ba62a2e7573d","executionInfo":{"status":"ok","timestamp":1580244580898,"user_tz":300,"elapsed":474543,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["# ! unzip crawl-300d-2M-subword.zip"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Archive:  crawl-300d-2M-subword.zip\n","  inflating: crawl-300d-2M-subword.vec  \n","  inflating: crawl-300d-2M-subword.bin  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zeEqXBSl45ke","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"160b6575-3440-4759-d6e5-607b26598f5f","executionInfo":{"status":"ok","timestamp":1580244829962,"user_tz":300,"elapsed":709597,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["# ! unzip crawl-300d-2M.vec.zip"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Archive:  crawl-300d-2M.vec.zip\n","  inflating: crawl-300d-2M.vec       \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XOHGyxdH9t_C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":210},"outputId":"98e11ad6-17c1-4673-99cb-0ac7096f7073","executionInfo":{"status":"ok","timestamp":1580250973129,"user_tz":300,"elapsed":4024,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["! head crawl-300d-2M-subword.vec"],"execution_count":5,"outputs":[{"output_type":"stream","text":["2000000 300\n",", 0.0062 0.0100 -0.0889 -0.0077 -0.0346 0.1686 -0.2244 -0.0291 0.1788 -0.0420 -0.0665 -0.0318 0.0031 0.0181 0.0817 -0.0242 -0.0363 0.0136 -0.1048 0.1852 -0.2318 -0.0051 0.0142 0.0433 -0.1126 0.0485 0.2424 -0.0079 0.0084 0.0020 -0.0750 0.1956 -0.0006 -0.0072 0.0058 0.0838 0.0749 0.0052 -0.0187 0.0232 -0.2201 0.2223 0.0061 -0.0153 0.0597 -0.0435 -0.0126 0.0348 -0.0570 0.1014 0.0160 0.0199 -0.0191 -0.0244 0.0086 -0.0140 0.2431 0.0473 0.3021 0.0271 -0.0188 -0.1661 0.2401 -0.0112 0.0094 -0.0186 0.0336 0.1088 -0.0332 -0.0254 0.0080 -0.0147 0.0356 -0.0297 -0.0071 0.0235 0.0281 0.0070 -0.1506 -0.0146 0.0897 0.0135 -0.6365 -0.0044 0.0186 0.0358 -0.0067 -0.0019 -0.1331 0.0577 0.0584 0.0468 0.2098 0.1217 -0.0117 -0.0108 -0.0051 0.0586 0.0146 -0.0092 -0.0544 0.0042 -0.0261 0.0173 0.1141 -0.0106 -0.0185 -0.0177 0.0950 0.0064 0.0380 0.0207 0.0432 -0.0232 -0.0452 0.0251 0.0621 -0.0350 -0.0825 -1.0386 -0.0340 -0.0258 0.0665 0.0138 0.0044 -0.0744 0.0256 -0.0792 0.0148 0.0835 -0.0092 -0.0262 0.1214 -0.0910 0.0447 -0.0429 -0.0217 0.0353 0.4365 0.0431 0.0121 -0.0716 -0.2567 -0.2556 0.0736 -0.0299 0.0024 0.0319 -0.0078 -0.0028 0.0727 -0.0118 -0.0192 -0.3859 -0.0118 -0.0197 0.3215 -0.2824 -0.1764 0.0205 0.0044 -0.0468 -0.0745 -0.0904 -0.2175 -0.1994 -0.0042 -0.0284 0.0020 -0.0240 0.0188 0.0183 0.0670 -0.0507 -0.0080 -0.0293 -0.2371 0.0198 0.0074 0.1119 0.0802 0.0058 0.0259 0.0316 0.0429 -0.0542 0.0259 -0.0615 0.0072 -0.0210 0.0056 -0.0054 -0.2971 -0.0197 -0.0341 0.0507 -0.0432 -0.0238 0.0117 -0.0320 0.1202 0.0111 -0.0411 -0.0427 0.0018 -0.0216 -0.0275 0.0035 0.0267 0.0728 -0.9520 -0.0140 0.1004 0.0345 -0.0281 -0.0106 0.0475 -0.0320 0.0558 -0.0024 -0.0124 0.0118 0.3504 0.0385 -0.1535 0.1071 -0.0120 -0.1646 -0.0070 -0.0178 0.0418 -0.0880 0.1653 0.0781 -0.0265 0.0081 -0.0831 0.0110 0.0068 -0.0187 -0.1557 0.0175 0.0110 -0.2980 0.0060 0.1306 -0.0480 0.1233 -0.0611 0.0451 -0.0081 0.0032 -0.0064 0.0259 -0.0397 -0.0165 0.0052 0.1211 -0.0095 -0.0067 0.0015 -0.0062 -0.0172 0.0386 0.0773 -0.0330 -0.0327 0.0412 0.0483 -0.0205 0.0199 -0.0237 -0.0445 -0.0153 0.0057 -0.0353 -0.1076 -0.0380 -0.0438 -0.0198 -0.2527 0.0614 -0.0425 0.1633 0.0318 -0.0247 0.0075 -0.0675 -0.0247 -0.0162 -0.0026 0.0372 0.3244 0.0017 -0.0066 -0.1241 -0.1273 -0.0774 -0.0524 0.0223\n","the 0.0120 -0.0268 0.1121 0.0277 0.0046 0.0338 0.1396 0.0112 -0.0631 0.0630 0.0315 0.0135 0.0028 0.0145 -0.0233 -0.0001 -0.0100 0.0161 -0.0915 0.2161 0.1492 -0.0080 0.0027 0.0014 -0.0194 -0.0071 -0.1985 -0.0052 -0.0087 0.0071 -0.0058 0.0374 0.0041 -0.0014 -0.0344 0.0084 -0.0064 -0.0255 -0.0164 0.0063 -0.0770 -0.2474 -0.0152 -0.0103 0.0446 0.0823 0.0314 -0.0163 0.0748 0.0186 0.0002 -0.0091 -0.0039 0.0110 -0.1585 -0.0065 0.1579 0.0313 0.2417 -0.0011 0.0023 0.0633 -0.0794 -0.0042 -0.0245 -0.0084 0.0172 -0.1142 -0.0102 0.0266 0.0107 -0.0213 0.0241 0.0269 -0.0057 -0.0005 -0.0187 -0.0189 0.0416 0.0194 -0.0106 -0.0279 -0.1751 0.0110 0.0023 0.0173 -0.0258 0.0249 0.0833 -0.0102 -0.0005 0.0171 0.0427 0.1354 0.0042 0.2789 -0.0311 0.0131 0.0069 0.0220 -0.0192 -0.0181 -0.0155 -0.0164 0.0419 -0.0036 -0.0147 0.0027 0.1047 -0.0110 -0.0571 -0.0017 0.0085 0.0854 0.0037 -0.0058 -0.0498 -0.0358 -0.0206 -0.3800 0.0183 0.0221 -0.0042 -0.0080 -0.0125 0.0002 0.0162 0.0362 -0.0182 -0.2271 0.0054 0.0021 -0.0553 -0.0546 -0.0022 0.0256 0.0047 0.0025 -0.0487 -0.0447 0.0142 -0.0144 0.2139 0.0057 -0.0786 0.0012 0.0218 -0.0062 -0.0154 -0.0151 -0.0075 -0.1063 -0.0479 0.0042 0.0235 -0.0068 -0.3213 -0.0563 0.0981 0.0032 0.0313 0.1816 -0.0491 -0.0083 0.0201 -0.0884 0.0027 -0.0065 -0.0030 0.0288 -0.0086 0.0172 -0.0105 -0.0233 0.0396 -0.0039 -0.0310 -0.0061 -0.0316 -0.0121 -0.0372 0.0164 0.0086 -0.0514 -0.1892 -0.0155 0.0107 0.0206 -0.0124 0.0032 0.0389 -0.0106 0.1121 0.0001 -0.0176 0.0038 0.0183 -0.0293 0.0150 -0.0245 -0.2179 0.0037 -0.0056 0.0066 0.0323 0.0633 0.0006 0.0153 -0.0106 -0.0334 -0.2179 -0.0117 0.0763 -0.0493 -0.0015 -0.0118 -0.0012 0.0020 -0.0416 0.0085 -0.0051 -0.0067 -0.0350 -0.0103 0.1299 0.0276 0.0383 0.2825 0.0157 -0.0359 -0.0775 -0.0430 -0.0554 -0.0097 -0.0044 -0.0648 -0.0013 -0.0346 -0.0308 0.0115 -0.0472 0.0131 -0.0136 -0.0010 -0.0069 0.0328 0.0233 -0.0677 -0.0048 -0.0133 0.2177 -0.0219 0.0118 0.0438 -0.0364 -0.0759 -0.0199 -0.0149 0.0075 0.0085 0.0255 -0.0065 0.0103 -0.0201 -0.0186 -0.0040 0.0107 0.0032 -0.0464 0.0227 0.0249 0.0114 -0.0169 0.0058 0.0145 -0.0255 -0.0306 0.0052 -0.0318 0.0210 -0.0202 0.0057 0.0264 0.0272 -0.0053 -0.0075 -0.0118 -0.1180 -0.0132 -0.0320 -0.0516 0.0033 -0.3371 0.0092 -0.0273 0.0900 0.0581 0.0093 -0.0337 -0.0091\n",". 0.0288 0.0394 0.1120 -0.0048 -0.0672 0.4486 -0.2465 -0.0427 -0.1035 -0.1428 -0.0109 -0.0402 -0.0180 -0.0060 0.0026 -0.0509 -0.0192 0.0461 -0.2065 0.0299 -0.0739 0.0700 0.0197 0.0019 -0.2418 -0.0228 0.0621 0.0049 0.0059 0.0357 -0.0301 -0.0198 -0.0190 0.0139 0.0365 0.0058 0.0326 0.0001 -0.0362 -0.0275 0.0625 0.3852 0.0160 0.0238 0.0388 -0.0858 0.0056 0.0203 -0.3400 0.0761 0.0538 0.0116 -0.0830 0.0476 0.3350 -0.0159 0.0415 0.0245 0.1795 -0.0067 0.0148 -0.0485 0.1169 0.0006 0.0124 -0.0045 -0.0281 -0.1320 -0.0120 -0.0874 -0.0118 -0.1048 -0.0109 -0.0636 0.0283 -0.0787 0.0139 -0.0161 -0.1569 0.0251 -0.0101 -0.0005 -0.6885 0.0006 0.0664 0.0173 -0.0387 0.0314 0.1147 0.0582 0.0388 0.0297 -0.2521 -0.1749 0.0345 0.0949 -0.0258 0.0056 -0.0111 0.0651 -0.0264 0.0113 0.0092 -0.0471 0.0561 -0.0050 0.0237 0.0033 0.2595 0.0042 -0.0765 0.0606 0.0308 0.0179 -0.0545 -0.0154 -0.1431 -0.0196 0.0745 -1.3656 -0.0038 -0.0298 -0.0159 0.0383 0.0070 0.0246 0.0634 -0.1009 0.0030 -0.0148 -0.0671 0.0023 0.0128 0.1138 0.0141 0.0284 -0.0085 0.0073 0.3970 -0.0006 0.0362 -0.0240 -0.1470 0.1237 -0.1126 0.0447 -0.0227 0.0007 0.0090 -0.0006 0.0074 -0.1510 -0.0731 -0.4609 0.0343 -0.0043 0.0252 0.0717 -0.2338 0.0163 -0.0141 -0.0868 -0.0288 -0.0780 -0.1060 0.0043 0.0132 0.0230 0.0368 0.0987 -0.0027 -0.0330 0.0062 -0.0206 -0.0003 0.0214 -0.6601 0.0460 -0.0406 0.0473 -0.0782 -0.0007 0.0675 0.0737 0.2838 -0.0536 -0.0053 0.0613 0.0711 0.0087 -0.0577 0.0280 -0.2265 -0.0114 -0.0198 0.0349 -0.1619 -0.0155 -0.0069 -0.0096 0.1171 0.0110 -0.0251 -0.0035 0.0186 0.0805 0.0422 0.0087 0.0163 0.0339 -0.9722 0.0314 -0.0444 0.0090 -0.0268 -0.0126 0.0966 -0.0304 0.1935 0.0185 0.0059 -0.0171 0.4042 -0.0382 -0.2051 0.0601 -0.0239 -0.1492 0.0383 -0.0132 -0.0459 -0.0024 0.0224 0.0264 -0.0005 -0.0879 -0.0217 -0.0495 0.0017 0.0111 0.0983 -0.0071 -0.0077 -0.0913 0.0189 -0.1923 -0.0033 0.0566 -0.0575 0.0174 -0.2580 -0.0249 0.0029 -0.0656 -0.0463 -0.0556 -0.0171 0.0273 0.0236 0.0053 0.0327 -0.0423 -0.0260 0.0034 0.0696 0.0135 -0.0357 0.0200 -0.0030 0.0302 0.0797 -0.0122 0.0274 -0.0059 -0.0210 -0.0365 -0.0997 -0.0083 -0.0371 -0.0288 0.1852 0.0530 0.0082 0.0216 0.0342 -0.0473 -0.0999 0.1885 -0.0064 -0.0039 -0.0359 -0.0331 0.4200 -0.0031 0.0080 0.1060 -0.1843 -0.0730 -0.0081 0.0261\n","</s> 0.0162 0.1468 0.0575 -0.0139 0.1862 0.1156 -0.0487 -0.0156 -0.0401 0.0778 0.0810 -0.0569 0.0147 -0.0832 -0.0961 0.0425 -0.0501 -0.0627 0.0606 -0.0638 0.0994 -0.0535 -0.0894 0.0126 0.0037 -0.0895 0.7074 -0.0660 -0.0638 0.0445 0.0690 -0.0905 0.0192 0.0915 -0.0307 0.0108 -0.0307 0.0215 -0.0881 -0.0056 -0.0333 -0.0619 -0.0339 -0.0189 -0.0437 -0.1650 0.0647 -0.0146 0.3799 0.1051 0.0142 -0.0489 -0.0137 0.0450 0.0227 0.0649 0.1014 -0.0169 -0.0405 0.0290 0.0585 -0.0596 0.1282 0.1068 -0.0173 0.0002 0.0243 0.0202 -0.0478 0.0496 -0.1134 -0.0888 0.0041 0.0183 -0.0472 -0.1012 -0.0844 0.0601 -0.0067 -0.0137 0.1621 0.0007 -0.0389 -0.0186 0.1251 -0.0516 0.0494 -0.0363 0.0764 0.0056 0.1011 0.0281 0.1226 -0.0657 0.0423 -0.7069 0.0894 0.0356 -0.0396 -0.1473 -0.0018 -0.0919 0.0966 -0.0058 0.0237 0.0505 0.0253 0.0601 1.1685 -0.0649 -0.0152 0.0434 0.0777 0.0246 -0.0736 -0.0253 0.1272 0.0599 -0.1527 -0.1279 0.0735 0.1311 -0.1011 -0.0128 -0.1043 0.0058 0.0854 0.0775 0.0237 0.1224 -0.0458 -0.0852 0.0227 -0.1710 0.0350 0.0311 -0.0165 -0.1567 0.0276 -0.0893 0.1118 0.0671 -0.2094 0.5966 0.0330 -0.0283 -0.0222 0.0688 0.1014 0.0085 -0.1809 0.0442 -0.0604 0.5213 0.0540 -0.0375 0.0872 -0.0375 -0.0351 -0.1669 -0.0445 0.0561 0.0713 0.1596 0.0815 0.1478 0.0736 -0.1098 -0.0671 -0.1125 0.0032 0.1210 -0.0367 -0.0019 0.0339 -0.1279 0.3334 0.0084 -0.0465 -0.1153 0.0449 0.0099 0.0472 -0.0729 -0.0151 0.0298 0.0338 -0.0634 -0.0777 -0.0011 0.1181 0.0358 -0.0169 -0.0560 -0.0209 -0.0290 0.1155 -0.0407 -0.1141 0.0563 0.1901 -0.0438 0.0283 0.0934 -0.0869 -0.0249 0.0341 0.0251 -0.1012 0.0046 -1.2195 0.0225 -0.1285 -0.1114 0.0213 0.0933 -0.0443 0.0895 0.1202 -0.0048 0.0134 -0.0187 0.0980 -0.0037 -0.0826 -0.0162 0.1038 -0.2475 0.0274 0.0513 0.0894 0.0040 -0.0769 -0.0685 -0.0749 -0.0251 0.1315 0.1295 -0.0826 -0.0632 -0.2258 0.0098 0.0348 0.2401 0.0200 -0.0450 0.1187 0.0227 -0.0107 -0.0327 0.1259 -0.0661 -0.1059 -0.0362 0.0315 -0.0759 -0.0568 0.0245 0.0326 0.0609 0.0456 0.0414 0.0246 -0.0629 -0.0397 -0.0807 0.0425 0.1883 -0.0970 -0.1038 -0.0735 -0.0628 0.2208 0.0642 0.0532 0.0986 0.0923 0.1108 0.0913 0.0018 0.3027 0.0082 -0.0085 -0.0538 0.0008 0.0042 0.0470 0.2200 -0.0784 -0.0921 0.0113 -0.0595 0.0028 0.0784 -0.0728 -0.1603 -0.0785 0.0180 0.0371 -0.0371\n","and 0.0201 -0.0432 0.1847 0.0137 -0.0281 -0.0811 0.1122 -0.0257 0.1010 0.1573 0.0184 -0.0040 0.0096 -0.0079 -0.0073 -0.0291 -0.0203 0.0084 0.0563 0.0537 -0.0764 -0.0323 -0.0028 0.0126 -0.0350 -0.0000 -0.0253 0.0122 0.0155 0.0131 -0.0127 0.0446 0.0074 -0.0018 -0.0148 -0.0153 0.0128 -0.0096 -0.0065 0.0259 -0.0892 -0.0972 0.0038 0.0053 0.0373 0.0622 0.0368 -0.0030 0.0450 -0.0163 -0.0041 -0.0255 -0.0091 -0.0141 -0.1210 0.0161 0.0052 0.0091 0.0076 -0.0153 0.0026 -0.0059 0.0690 -0.0154 -0.0366 -0.0006 0.0154 0.0401 -0.0159 0.0338 -0.0102 0.0154 0.0033 -0.0104 -0.0106 0.0111 0.0077 0.0023 0.0374 -0.0079 0.0161 -0.0154 -0.2483 0.0203 0.0245 0.0093 -0.0383 0.0065 -0.0502 -0.0114 -0.0907 0.0060 -0.0474 0.0604 0.0085 0.3431 -0.0083 -0.0138 0.0232 -0.0207 -0.0392 -0.0219 -0.0294 -0.0068 0.0730 0.0044 -0.0117 -0.0225 0.1164 -0.0203 -0.0051 0.0179 0.0298 0.0082 0.0049 0.0026 0.0099 -0.0163 -0.0542 -0.4523 0.0127 0.0001 0.0138 -0.0013 0.0119 0.0000 0.0222 0.0035 -0.0379 0.0027 -0.0079 -0.0261 0.0243 0.1043 0.0182 -0.0011 -0.0397 0.0138 0.0277 -0.0519 0.0097 -0.0159 0.0315 -0.0186 0.0819 0.0109 0.0026 0.0006 0.0039 -0.0294 -0.0022 -0.0424 -0.0385 0.0700 0.0223 -0.0147 0.0786 -0.0293 0.1129 0.0342 0.0342 -0.0340 -0.0375 -0.0442 -0.0990 0.1419 0.0229 0.0192 0.0244 0.0134 0.0132 0.0058 0.0280 -0.0106 0.0208 0.0202 -0.0635 0.0006 -0.0393 0.0221 0.0190 0.0041 0.0189 0.0116 -0.0941 -0.0208 0.0049 0.0676 0.0104 0.0046 -0.0020 0.0015 -0.0934 -0.0166 -0.0189 0.0228 0.0255 -0.0164 -0.0077 0.0127 -0.1993 0.0130 -0.0181 0.0318 0.0219 0.0946 -0.0059 0.0137 -0.0132 -0.0004 -0.2210 -0.0162 0.0275 -0.0483 0.0142 0.0315 0.0221 -0.0047 -0.0709 0.0026 -0.0288 0.0122 0.0749 -0.0041 0.1207 0.0764 0.0265 0.0713 0.0166 -0.0100 -0.0169 -0.0436 0.0361 -0.0195 0.0037 0.0103 -0.0242 -0.0110 -0.0082 0.0112 -0.0260 -0.0130 0.0026 0.0024 -0.0085 0.0506 -0.0070 0.0421 -0.0180 0.0528 -0.0035 -0.0267 -0.0088 0.0366 -0.0303 -0.0272 0.0005 0.0233 0.0243 -0.0010 -0.0297 -0.0587 -0.0204 -0.0000 -0.0127 -0.0092 -0.0183 -0.0027 0.0130 -0.0075 -0.0313 0.0252 -0.0010 -0.0242 -0.0126 -0.0273 -0.0003 -0.0217 -0.0133 0.0057 -0.0130 0.0235 -0.0099 0.0270 -0.0149 -0.0309 -0.0167 -0.1007 -0.0018 0.0103 -0.0174 -0.0266 0.0240 0.0243 -0.0056 0.0360 0.0277 -0.0226 -0.0262 -0.0138\n","to 0.0249 -0.2444 0.1888 0.0472 -0.0583 -0.0991 0.4287 -0.0103 -0.0245 0.0925 -0.0067 0.0265 -0.0018 0.0119 -0.0147 -0.0216 -0.0301 0.0344 0.0076 0.0390 0.1735 -0.1788 -0.0193 -0.0077 0.0920 0.0033 0.0032 -0.0311 -0.0143 0.0500 0.0137 -0.1493 -0.0064 0.0120 -0.0026 -0.0177 -0.0414 -0.0465 0.0045 0.0224 -0.0375 -0.1727 -0.0133 0.0083 -0.0148 0.1744 -0.0345 -0.0062 0.0422 0.0430 -0.0124 -0.0426 -0.0105 0.0049 -0.2343 -0.0236 -0.2321 0.0081 -0.2200 -0.0181 0.0007 0.0258 -0.1491 -0.0126 -0.0181 0.0216 0.0022 -0.0562 -0.0115 -0.0308 -0.0186 -0.0002 0.0323 0.0077 -0.0010 0.0025 0.0295 -0.0034 0.0137 0.0148 0.0113 -0.0311 -0.2123 0.0108 0.0114 0.0091 -0.0229 0.0275 -0.0107 -0.0266 -0.0623 0.0065 -0.1540 -0.5197 0.0125 0.5347 -0.0467 0.0117 -0.0001 -0.0259 0.0335 -0.0123 0.0049 -0.0238 -0.0014 -0.0212 0.0031 -0.0142 0.0091 -0.0061 -0.0176 0.0255 0.0109 -0.0014 -0.0024 -0.0188 0.1241 -0.0383 0.0580 -0.5668 0.0349 0.0169 -0.0050 -0.0144 -0.0119 -0.0324 0.0240 -0.0745 -0.0205 0.2414 0.0142 -0.0177 0.0116 -0.1870 -0.0240 -0.0014 0.0269 0.0035 0.0405 -0.0289 0.0255 -0.0007 -0.0366 0.1008 0.0467 -0.0044 0.0076 0.1185 0.0026 -0.0057 0.0325 -0.0544 -0.0415 -0.0758 0.0390 -0.0221 -0.2381 0.1008 0.3198 0.0019 -0.0239 -0.0996 -0.0178 -0.0020 -0.2055 -0.0022 0.0354 -0.0640 0.0065 0.0218 -0.0277 0.0365 0.0193 0.0156 0.0093 -0.0126 0.0277 0.0054 0.0129 0.0044 -0.0053 0.0184 0.0168 -0.0953 -0.5280 0.0062 0.0210 0.2777 -0.0183 0.0107 0.0109 0.0086 -0.0772 -0.0028 -0.0213 0.0237 0.4247 -0.0624 -0.0022 -0.0170 -0.3042 -0.0080 -0.0231 0.0211 -0.0489 -0.0622 0.0160 0.0382 -0.0150 -0.0020 -0.3096 -0.0321 -0.0052 -0.0052 -0.0158 0.0171 0.0100 -0.0101 -0.1192 -0.0032 -0.0116 0.0032 -0.0322 0.0005 0.0136 0.0461 0.0518 0.5226 0.0092 -0.0234 -0.0622 -0.0056 -0.0618 -0.0407 0.0120 -0.0302 -0.0180 -0.0006 -0.0238 0.0017 0.1796 -0.0065 -0.0328 0.0111 -0.0015 0.3047 0.0276 -0.0571 0.0138 -0.0945 0.1479 -0.0281 -0.0042 -0.0249 -0.0705 -0.1057 0.0240 -0.0137 0.0140 -0.0082 0.0210 0.0267 -0.0013 0.0619 -0.0367 -0.0129 0.0250 0.0171 0.0233 -0.0032 -0.0456 0.0302 -0.0576 0.0098 -0.0084 0.0044 0.0261 -0.0175 -0.0083 0.0470 0.2899 0.0269 0.0139 0.0176 0.0046 -0.0354 0.0293 -0.2339 -0.0027 -0.0332 -0.0153 -0.0453 0.0897 0.0071 0.0060 0.0748 -0.2137 -0.0200 -0.0518 -0.0097\n","of 0.0242 -0.0544 0.2472 0.0734 0.0470 -0.0402 0.3732 0.0172 0.0103 -0.0350 -0.0169 0.0041 0.0075 0.0094 0.0112 -0.0386 -0.0097 0.0418 -0.3384 0.1741 -0.0432 0.1047 -0.0165 0.0151 -0.1455 -0.0302 -0.4927 -0.0173 0.0063 0.0339 -0.0074 -0.0821 -0.0086 -0.0205 -0.0559 -0.0392 0.0447 -0.0397 -0.0366 0.0316 -0.0290 -0.2286 -0.0135 -0.0053 -0.0413 0.0771 0.1022 -0.0141 0.0969 0.0094 0.0224 -0.0064 -0.0089 0.0166 -0.0072 0.0020 -0.1634 0.0397 -0.2165 -0.0225 0.0290 0.2649 -0.0251 -0.0204 0.0163 -0.0245 0.0129 0.1187 -0.0293 0.0108 0.0004 0.0043 0.0229 -0.0190 -0.0149 0.0213 0.0052 -0.0295 -0.1479 -0.0017 -0.1046 -0.0475 -0.4714 0.0244 0.0122 0.0169 -0.0244 0.0141 0.0691 -0.0118 0.0565 -0.0006 -0.0330 0.4819 -0.0144 0.2907 -0.0174 0.0259 0.0077 -0.0212 -0.0192 -0.0527 -0.0079 0.0012 0.1660 0.0051 -0.0006 0.0132 0.2655 -0.0124 -0.1027 0.0276 -0.0399 -0.0144 0.0258 0.0010 0.1865 -0.0242 -0.0515 -0.4319 0.0060 0.0122 0.0035 0.0160 -0.0268 -0.0422 0.0295 0.1221 -0.0203 -0.0103 0.0012 -0.0072 0.0348 0.2713 -0.0166 0.0099 -0.0228 0.0155 0.0187 -0.0247 0.0227 0.0158 -0.0332 0.0515 -0.0158 0.0065 0.0005 0.0367 0.0027 0.0074 0.0073 0.0544 -0.0338 0.0523 0.0353 -0.0107 -0.1327 -0.2635 -0.0312 0.0172 0.0105 0.2305 -0.0641 0.0077 0.1343 0.1377 0.0494 0.0197 -0.0538 0.0569 -0.0123 0.0225 -0.0024 0.0057 0.0566 -0.0145 -0.2345 0.0172 -0.0391 -0.0262 -0.0448 0.0131 0.0055 -0.0140 -0.3506 -0.0695 0.0052 0.0181 -0.0282 -0.0176 0.0108 0.0149 0.2030 0.0174 -0.0281 0.0094 -0.2268 -0.0335 -0.0135 -0.0313 -0.2400 0.0200 0.0168 0.0145 0.0368 0.1607 -0.0200 0.0172 -0.0425 0.0295 -0.3807 -0.0228 0.1399 -0.0165 0.0087 0.0047 0.0057 -0.0318 0.1111 -0.0052 0.0274 -0.0345 0.0267 -0.0076 0.1745 0.0776 0.0837 0.4347 0.0115 -0.0217 -0.0380 0.0908 -0.0728 -0.0591 0.0002 0.0182 0.0278 0.0180 -0.0298 0.0119 -0.0465 0.0203 -0.0469 -0.0826 -0.0156 0.2175 -0.0164 -0.0680 -0.0167 -0.1013 0.4681 -0.0217 0.0332 0.0300 -0.0612 -0.1199 -0.0108 -0.0021 0.0013 0.0089 0.0409 -0.0139 -0.0096 -0.0050 -0.0141 0.0318 0.0215 0.0492 -0.0560 0.0458 0.0063 0.0291 0.0490 -0.0085 0.0023 0.0189 0.0740 -0.0746 -0.0455 0.0561 0.1294 0.0024 0.0103 -0.0286 0.0021 -0.0131 0.0136 -0.1210 -0.0215 -0.0717 -0.0440 -0.0434 -0.4107 -0.0206 0.0026 0.1683 -0.2046 -0.0211 -0.1027 -0.0031\n","a -0.0018 0.0737 -0.0340 0.0286 -0.0184 0.1961 0.0436 0.0338 0.1920 -0.1108 -0.0214 -0.0048 -0.0205 0.0209 0.0270 0.0107 0.0052 0.0463 -0.1647 -0.3277 0.0553 0.2018 -0.0290 0.0115 -0.0522 0.0308 0.0186 -0.0361 0.0191 0.0428 -0.0263 -0.1071 -0.0032 0.0164 -0.0166 -0.0395 0.0185 0.0359 0.0033 0.0639 0.0060 -0.0047 0.0273 0.0494 -0.1604 -0.1142 -0.1126 -0.0271 -0.3286 0.0412 0.0280 0.0361 0.0567 -0.0226 -0.3177 0.0162 0.0388 0.0133 -0.2722 0.0654 0.0090 -0.2485 -0.4060 0.0326 0.0219 0.0173 -0.0193 0.0720 0.0107 0.1184 -0.0230 -0.0918 -0.0596 -0.0487 0.0074 -0.0107 -0.0037 -0.0409 -0.0270 -0.0202 -0.1024 0.0035 -0.8927 0.0358 0.0207 0.0444 0.0148 0.0163 0.0256 0.0275 0.2436 -0.0012 0.4729 0.0461 -0.0540 0.0840 -0.0181 0.0408 0.0189 -0.0198 -0.0862 -0.0010 0.0514 0.0356 0.0647 0.0513 0.0856 0.0010 0.2948 0.0417 -0.0531 -0.0137 0.0095 -0.1155 -0.0108 -0.0091 -0.2122 -0.0281 0.0205 -1.5194 0.0482 0.0121 0.0098 -0.0473 0.0575 -0.0079 0.0272 -0.0206 0.0050 0.0981 -0.0241 -0.0005 0.3134 -0.2368 0.0154 0.0209 -0.0538 0.0009 0.6372 0.0982 0.0168 -0.0027 -0.1764 -0.0054 -0.0453 -0.0278 0.0159 -0.1758 0.0508 0.0291 -0.0299 -0.0599 -0.0545 -0.5764 0.0094 -0.0019 0.1555 -0.2172 -0.2440 0.0338 -0.0207 -0.4889 -0.0028 -0.0510 -0.1095 -0.7024 0.0483 -0.0129 0.0066 -0.0349 0.0268 0.0253 -0.0422 -0.0087 0.0201 -0.0273 -0.3874 0.0247 -0.0807 0.0576 -0.0642 -0.0163 -0.0278 -0.0009 -0.2094 -0.0782 -0.0488 0.1522 -0.0047 0.0334 -0.0030 0.0159 0.2031 0.0101 -0.0115 0.0667 0.3909 -0.0368 -0.0090 0.0221 0.6696 0.0161 0.0372 -0.0223 0.0479 -0.0101 0.0473 -0.0026 -0.0246 -0.0154 -0.9964 0.0467 -0.3619 0.1059 -0.0105 -0.0057 0.0779 -0.0131 -0.1168 -0.0095 -0.0124 -0.0291 0.5275 -0.0480 -0.2137 0.0244 0.0316 0.1013 0.0150 -0.0109 -0.0145 0.1018 -0.0824 -0.0779 0.0211 0.0245 -0.0367 0.0425 -0.0265 0.0420 -0.0025 0.0146 0.0123 -0.1169 0.0464 -0.3384 0.0473 0.0494 -0.0501 -0.0464 0.6496 0.0051 -0.0280 -0.0679 0.0470 -0.0499 -0.0340 0.0200 -0.0389 -0.0181 0.0873 -0.0726 0.1165 0.1120 0.0777 0.0229 0.0136 0.0159 -0.0233 0.0982 0.0400 0.0214 -0.0952 0.0158 0.0063 0.0193 -0.1193 -0.1017 -0.0122 -0.0252 0.1115 0.0253 0.0113 -0.0771 0.0315 -0.0339 0.1122 0.1340 0.0074 -0.0018 -0.0794 -0.0243 -0.1983 -0.0494 0.0618 0.1807 -0.0605 -0.0198 0.1114 -0.0081\n","in 0.0325 -0.2989 0.1075 0.0213 0.0288 0.0303 -0.1370 -0.0008 -0.0855 -0.0125 -0.0110 -0.0194 0.0214 0.0316 -0.0132 0.0724 -0.0171 0.0312 0.0636 0.4581 0.2972 -0.0786 -0.0083 0.0169 -0.1064 -0.0020 -0.2648 -0.0105 -0.0134 0.0442 0.0008 -0.0597 0.0247 -0.0258 0.0684 -0.0059 0.0147 -0.0223 -0.0391 -0.0026 0.2116 -0.1352 -0.0245 -0.0011 -0.0252 0.1169 -0.0328 -0.0368 0.0707 0.0376 0.0254 -0.0255 -0.0438 -0.0450 -0.0270 -0.0007 0.1370 0.0067 0.2859 0.0269 0.0067 0.0829 0.1142 -0.0212 -0.0230 -0.0733 0.0090 0.1385 0.0017 0.0237 -0.0170 0.0121 0.0352 -0.0383 -0.0014 0.0280 0.0199 -0.0259 -0.0033 0.0340 -0.0749 -0.0035 -0.3969 0.0042 -0.0026 0.0172 -0.0323 0.0012 -0.0514 0.0086 0.0113 0.0125 -0.0317 0.0155 -0.0176 0.4369 -0.0308 0.0072 0.0617 0.0379 -0.0584 -0.0299 0.0023 0.0089 0.1327 -0.0079 0.0205 -0.0080 0.1903 -0.0235 0.0071 0.0057 -0.0183 -0.0586 0.0139 0.0255 -0.1196 0.0234 0.0373 -0.5569 -0.0084 0.0274 -0.0007 -0.0078 -0.0131 -0.0251 0.0222 -0.0642 -0.0342 -0.1275 -0.0050 0.0073 -0.0771 0.1575 -0.0011 0.0225 0.0345 0.0018 0.1063 -0.1109 0.0034 0.0114 0.0899 -0.0880 -0.1454 -0.0012 -0.0076 0.0712 0.0057 -0.0348 0.0706 0.0202 -0.0352 0.0301 0.0303 0.0054 0.2526 -0.2913 0.1618 -0.0246 0.0101 -0.1428 -0.0730 -0.0401 0.0674 0.0308 0.0527 -0.0346 -0.0254 0.0191 -0.0298 0.0368 -0.0063 0.0088 -0.0054 0.0018 -0.1062 -0.0082 0.0617 -0.0458 -0.0418 0.0087 0.0082 0.0782 -0.6552 -0.0585 0.0008 -0.0621 -0.0144 -0.0110 0.0460 0.0164 0.0554 -0.0354 -0.0421 -0.0121 0.0856 -0.0095 0.0087 -0.0063 -0.2516 0.0194 -0.0263 0.0221 0.0837 0.1044 -0.0246 0.0095 0.0029 0.0171 -0.2929 -0.0567 -0.0311 -0.0441 -0.0176 0.0065 -0.0280 -0.0664 -0.1789 -0.0055 0.0141 -0.0112 -0.1093 -0.0315 0.0389 -0.0000 0.0642 0.5521 0.0119 -0.0236 0.0225 -0.0364 0.0737 -0.0635 -0.0055 0.0008 -0.0239 0.0037 -0.0305 -0.0103 0.1779 0.0222 -0.0004 0.0787 0.0079 -0.0081 0.0191 -0.1327 -0.0521 -0.1740 0.1743 -0.0648 -0.0003 0.0264 0.0138 -0.1032 -0.0086 -0.0053 0.0048 0.0235 0.0568 -0.0156 0.0804 -0.0364 -0.0299 0.0400 0.0030 0.0090 -0.0728 -0.0024 0.0044 0.0068 0.2027 0.0062 0.0026 -0.0012 -0.1252 0.0412 0.0090 0.0341 -0.0723 -0.0013 0.0048 -0.0604 -0.0110 -0.0105 0.0734 -0.2298 -0.0155 -0.0594 -0.0810 -0.0125 -0.2516 -0.0038 0.0037 0.1451 -0.1723 0.0298 -0.0140 -0.0225\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cquJQfjE3W9F","colab_type":"code","colab":{}},"source":["int2Label = \\\n","{0: 'Miscellaneous',\n"," 1: 'Video games',\n"," 2: 'Language and literature',\n"," 3: 'Music',\n"," 4: 'Social sciences and society',\n"," 5: 'Sports and recreation',\n"," 6: 'Natural sciences',\n"," 7: 'Art and architecture',\n"," 8: 'History',\n"," 9: 'Warfare',\n"," 10: 'Engineering and technology',\n"," 11: 'Philosophy and religion',\n"," 12: 'Agriculture, food and drink',\n"," 13: 'Geography and places',\n"," 14: 'Mathematics',\n"," 15: 'Media and drama'}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sulAfm2L3W9J","colab_type":"code","colab":{}},"source":["np.random.seed(11747)\n","LOWER = False\n","LEARNING_RATE = 3e-4\n","N_CLASS = len(int2Label.keys())\n","MODELNO = 4\n","LOG_FILE = \"ConvNetClassificationTesting_\" + str(MODELNO) + \".txt\"\n","VERBOSE_LOG_FILE = \"VerboseConvNetClassificationTesting_\" + str(MODELNO) + \".txt\"\n","STEP_SIZE = 5\n","GAMMA = 1.0\n","NUMBER_EPOCHS = 10\n","EMBEDDING_DIM = 300"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtjvDxEI3W9M","colab_type":"code","colab":{}},"source":["f = open(LOG_FILE,\"w+\")\n","v = open(VERBOSE_LOG_FILE, \"w+\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1MtnSnwqqFUn","colab_type":"code","colab":{}},"source":["def _split_tokenizer(x):\n","    return x.split()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVOqYVZQ3W9P","colab_type":"code","colab":{}},"source":["tokenizer = get_tokenizer(\"spacy\")\n","\n","TEXT = Field(sequential=True, tokenize=tokenizer, lower=LOWER, batch_first=True)\n","\n","LABEL = Field(sequential=False, use_vocab=False, batch_first=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-YDPjyO_3W9T","colab_type":"code","colab":{}},"source":["train, val, test = TabularDataset.splits(\".\", \n","                                            train = \"topicclass_train.csv\", \n","                                            validation = \"topicclass_valid_fixed.csv\", \n","                                            test = \"topicclass_test.csv\", \n","                                            format = \"csv\", \n","                                            skip_header = True,\n","                                            fields = [('label', LABEL), ('text', TEXT)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sC-QO3I5JowB","colab_type":"code","colab":{}},"source":["TEXT.build_vocab(train, val, test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PHvrNYVzN_uG","colab_type":"code","colab":{}},"source":["embedding_mtx = torch.zeros((max(TEXT.vocab.stoi.values()), EMBEDDING_DIM))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXbIf-r-N7Fh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["e9f0478fddcc4c48a1d17c40b1aa2a5e","d0847889d153422ab3c615eabbb2e44d","9f8202e8f79141208304bb6224ca5a83","d29e532824e44b7cbd583eecb3b8b1c3","6b1515b7012e4b9889544bc4e455f20a","9ac567cecb1346b188f5fc41b4120858","6de4cc2b40a649f19b31c7b9466d06b8","6b9d8c4a28694ff7be68aeec1490d718"]},"outputId":"103aea93-3e10-49d9-cd60-6524782b2e71","executionInfo":{"status":"ok","timestamp":1580255822798,"user_tz":300,"elapsed":245227,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["defined_vocab = set()\n","num_lines = 2000000\n","\n","with open(\"crawl-300d-2M.vec\") as vec_file: \n","  for i, line in enumerate(tqdm_notebook(vec_file, total=num_lines)): \n","    if i == 0: \n","      pass\n","    else: \n","      word, vector = line.split(\" \", 1)\n","      if word in TEXT.vocab.stoi: \n","        embedding_mtx[TEXT.vocab.stoi.get(word)] =  torch.from_numpy(np.fromstring(vector, sep = \" \"))\n","        defined_vocab.add(word)\n","\n"],"execution_count":90,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9f0478fddcc4c48a1d17c40b1aa2a5e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=2000000), HTML(value='')))"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ltn7TTN73W9W","colab_type":"code","colab":{}},"source":["#TEXT.build_vocab(train, val, test, vectors = \"fasttext.en.300d\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvnabwCr3W9Z","colab_type":"code","colab":{}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Kz3uwhbUJoZ","colab_type":"code","colab":{}},"source":["#device = \"cpu\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EiGPnMhhhu3C","colab_type":"code","colab":{}},"source":["TEXT.vocab.unk_index = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"797P9UtpizSn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"c7fa275d-5545-4f98-e403-dace089ec572","executionInfo":{"status":"ok","timestamp":1580255882919,"user_tz":300,"elapsed":1557,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["TEXT.numericalize([\"the cat in the hat\"])"],"execution_count":93,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1283,   659,  4774, 23765,  1346,     8,  1283, 23765,  5060,  5907,\n","         23765,  1283,   659,  4774, 23765,   659,     8,  1283]])"]},"metadata":{"tags":[]},"execution_count":93}]},{"cell_type":"code","metadata":{"id":"v12eMl72jRSq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"06b99c9f-42cb-45a0-ae4b-f62ab0de95f3","executionInfo":{"status":"ok","timestamp":1580257138346,"user_tz":300,"elapsed":1475,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["TEXT.numericalize([['the', 'cat', 'in', 'the', 'hat'], ['the', 'cat', 'in', 'the', 'hat']]).numpy().tolist()"],"execution_count":128,"outputs":[{"output_type":"stream","text":["arr is [[2, 10793, 7, 2, 11192], [2, 10793, 7, 2, 11192]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[[2, 10793, 7, 2, 11192], [2, 10793, 7, 2, 11192]]"]},"metadata":{"tags":[]},"execution_count":128}]},{"cell_type":"code","metadata":{"id":"O-c6ahU8jJGp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"28d8028d-1802-43a2-be2e-894de1632987","executionInfo":{"status":"ok","timestamp":1580255886072,"user_tz":300,"elapsed":422,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["[TEXT.vocab.itos[i] for i in TEXT.numericalize([['the', 'cat', 'in', 'the', 'hat'], ['the', 'cat', 'in', 'the', 'hat']]).numpy().tolist()[0]  ]"],"execution_count":95,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the', 'cat', 'in', 'the', 'hat']"]},"metadata":{"tags":[]},"execution_count":95}]},{"cell_type":"code","metadata":{"id":"3iAfx5AD3W9c","colab_type":"code","outputId":"85e54d6a-2b21-46d1-cc1e-d7de151ccf0b","executionInfo":{"status":"ok","timestamp":1580251265922,"user_tz":300,"elapsed":534,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print(\"device is: {}\".format(device))\n","f.write(\"device is: {}\\n\".format(device))\n","v.write(\"device is: {}\\n\".format(device))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["device is: cpu\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"jICQYWlF3W9g","colab_type":"code","colab":{}},"source":["train_loader, val_loader = Iterator.splits(\n","                                    (train, val), \n","                                    batch_sizes = (64, 64), \n","                                    shuffle = True, \n","                                    sort_key = lambda x: len(x.text), \n","                                    device = device, \n","                                    )\n","\n","test_loader = Iterator(test, batch_size=64, device = device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xp7XLHgcl2NL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":384},"outputId":"272abb7e-968f-4d55-e4f8-528a8d5d7df7","executionInfo":{"status":"error","timestamp":1580257139899,"user_tz":300,"elapsed":3013,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["for i, batch in enumerate(train_loader): \n","  print(batch)\n","  break"],"execution_count":130,"outputs":[{"output_type":"stream","text":["arr is [7, 4, 4, 4, 15, 15, 3, 3, 15, 15, 8, 8, 1, 13, 13, 15, 10, 7, 13, 8, 15, 3, 8, 9, 6, 15, 10, 15, 15, 6, 5, 9, 9, 5, 3, 9, 3, 11, 3, 15, 10, 3, 9, 6, 9, 8, 3, 9, 8, 3, 4, 5, 2, 5, 5, 6, 4, 1, 6, 2, 5, 10, 13, 13]\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-130-c95622480558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-120-643dfd4f7554>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \"\"\"\n\u001b[1;32m    330\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-120-643dfd4f7554>\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"arr is {arr}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"]}]},{"cell_type":"code","metadata":{"id":"GByxUna5rU6Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":166},"outputId":"c682a945-9ba9-42d0-b6dc-fed079d87295","executionInfo":{"status":"error","timestamp":1580257434800,"user_tz":300,"elapsed":1695,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["torch.tensor([7, 4, 4, 4, 15, 15, 3, 3, 15, 15, 8, 8, 1, 13, 13, 15, 10, 7, 13, 8, 15, 3, 8, 9, 6, 15, 10, 15, 15, 6, 5, 9, 9, 5, 3, 9, 3, 11, 3, 15, 10, 3, 9, 6, 9, 8, 3, 9, 8, 3, 4, 5, 2, 5, 5, 6, 4, 1, 6, 2, 5, 10, 13, 13], dtype = torch.long, device = device)"],"execution_count":134,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-134-ab927a71cb58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,...\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"]}]},{"cell_type":"code","metadata":{"id":"1u5z2GiZmB-a","colab_type":"code","colab":{}},"source":["! head "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9GcQlvBx3W9j","colab_type":"code","colab":{}},"source":["dataloaders = {'train': train_loader, 'val': val_loader}\n","dataset_sizes = {'train': len(train), 'val': len(val)}\n","\n","Metric = namedtuple('Metric', ['loss', 'train_error', 'val_error'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNGvffDT3W9m","colab_type":"code","colab":{}},"source":["class ConvClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, channels_first = 64, channels_second = 64, kernel_first = 2, kernel_second = 2): \n","        super().__init__()\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","        \n","        #self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first, dilation = 1, kernel_size = kernel_first, padding = kernel_first - 1)\n","        #self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # self.conv2 = nn.Conv1d(in_channels=channels_first, out_channels=channels_second, dilation = 1, kernel_size = kernel_second, padding = kernel_second - 1)\n","        # self.bn2 = nn.BatchNorm1d(channels_second)\n","    \n","        self.relu = nn.ReLU()\n","        \n","        self.fc = nn.Linear(channels_first, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            static_embeddings = self.embedding(texts)\n","        embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","        conv1_out = self.conv1(embeddings.transpose(1,2))\n","        #conv1_out = self.bn1(conv1_out)\n","        conv1_out = self.relu(conv1_out)\n","        \n","        # conv2_out = self.conv2(conv1_out)\n","        # conv2_out = self.bn2(conv2_out)\n","        # conv2_out = self.relu(conv2_out) \n","        \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        pool = nn.MaxPool1d(kernel_size = conv1_out.shape[2])\n","#         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","        pooled_out = pool(conv1_out).squeeze(2)\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","        \n","        out = self.fc(pooled_out)\n","        \n","        return out\n","        \n","        \n","        \n","        \n","        \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OyoVNg0WK_Pe","colab_type":"code","colab":{}},"source":["def init_weights(m):\n","    if type(m) == nn.Conv1d or type(m) == nn.Linear:\n","        nn.init.xavier_normal_(m.weight.data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlEnuEjy3W9p","colab_type":"code","colab":{}},"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","\n","    since = time.time()\n","    \n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        f.write('Epoch {}/{}\\n'.format(epoch+1, num_epochs))\n","        v.write('Epoch {}/{}\\n'.format(epoch+1, num_epochs))\n","        f.write('-' *10)\n","        v.write('-' * 10)\n","\n","        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            f.write(\"\\nstarting epoch {} for {} phase\\n\".format(epoch+1, phase))\n","            v.write(\"\\nstarting epoch {} for {} phase\\n\".format(epoch+1, phase))\n","            print(\"starting epoch {} for {} phase\".format(epoch+1, phase))\n","\n","            for i, data in enumerate(tqdm_notebook(dataloaders[phase])):\n","                #pdb.set_trace()\n","                inputs = data.text\n","                labels = data.label\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","                if ((i%300) == 0):\n","                    #print(\"inside\")\n","                    v.write(\"inputs size: {}\\n\".format(inputs.size(0)))\n","                    v.write(\"epoch {}, batch {},  loss : {}\\n\".format(epoch+1, i, loss.item()))\n","                    v.write(\"percent correct: {}\\n\".format((torch.sum(preds == labels.data)/inputs.size(0))))\n","            \n","            \n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            if phase == 'train':\n","\n","                train_loss = epoch_loss\n","                train_error = 1 - epoch_acc\n","                scheduler.step()\n","\n","            elif phase == 'val': \n","\n","                val_error = 1 - epoch_acc\n","                metrics.append(Metric(loss=train_loss, train_error=train_error,val_error=val_error))\n","            \n","            f.write('{} Loss: {:.4f} Acc: {:.4f}\\n'.format(phase, epoch_loss, epoch_acc))\n","            v.write('{} Loss: {:.4f} Acc: {:.4f}\\n'.format(phase, epoch_loss, epoch_acc))\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                PATH = 'MobileNetModel_' + str(epoch+2) + '.pt'\n","                torch.save(model, PATH)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    f.write('Training complete in {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n","    v.write('Training complete in {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n","\n","\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    f.write('Best val Acc: {:4f}'.format(best_acc))\n","    v.write('Best val Acc: {:4f}'.format(best_acc))\n","\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQSVyWye3W9s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":381},"outputId":"3c450437-8750-41a4-bd14-44cbde61b13d","executionInfo":{"status":"error","timestamp":1580255555820,"user_tz":300,"elapsed":1984,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["model = ConvClassifier(embedding_mtx, N_CLASS, channels_first = 16, channels_second = 64, kernel_first = 3, kernel_second = 2)\n","model = model.to(device)\n","model.apply(init_weights)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n"],"execution_count":83,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-83-bdff441dd950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_CLASS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_second\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_second\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"]}]},{"cell_type":"code","metadata":{"id":"ogDkGH3K3W9w","colab_type":"code","outputId":"ac376351-ae03-442d-c6ea-491935f058c0","executionInfo":{"status":"error","timestamp":1580255555821,"user_tz":300,"elapsed":1257,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":465,"referenced_widgets":["e2a3e79a7ef04081aa6326195ab7b57c","e79e20e83fb147f58de6a3b4395751d1","853cfe128c95493780f8cd9e89ca9c74","1318283686394b75981d77f991018544","5f60ad28b974487a8cfdf33b83cabf03","63e9fa5186154cb29928ddba8466884a","9e0b2d05d1a14b71b2017f7e9246eea9","63c1da1c433f4387afb1a11ae1a9ce5c"]}},"source":["metrics = []\n","model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=NUMBER_EPOCHS)\n","\n","f.close()\n","v.close()\n","torch.save(model, \"Model_\" + str(MODELNO) + \".pt\")\n","\n","def training_plot(metrics):\n","    plt.figure(1)\n","    plt.plot([m.val_error for m in metrics], 'b')\n","    plt.plot([m.train_error for m in metrics], 'r')\n","    plt.title('Train Error (red), Val Error (blue)')\n","    plt.savefig('Model_' + str(MODELNO) + '.png')\n","\n","training_plot(metrics)\n"],"execution_count":84,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","----------\n","starting epoch 1 for train phase\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2a3e79a7ef04081aa6326195ab7b57c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=3968), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-84-fb073a051788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUMBER_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-82-b2903dbb0c3f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"starting epoch {} for {} phase\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[1;32m    200\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"]}]},{"cell_type":"code","metadata":{"id":"hrsl8-epllwk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":294},"outputId":"1ae0cb8a-7115-43bd-ff07-2fa967688152","executionInfo":{"status":"ok","timestamp":1580253964175,"user_tz":300,"elapsed":4737,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["! nvidia-smi"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Tue Jan 28 23:26:00 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   77C    P0    32W /  70W |    971MiB / 15079MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o7i6eMQUedGd","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}