{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"Copy of hw1p2_full_dataset_testing_v3.ipynb","provenance":[{"file_id":"15_PhOmWqwfHyRiPXPdlrarWdNp-1Bm5M","timestamp":1569504656905},{"file_id":"1gW2K-Mj_Fht9nrNBWTKX1-AWzKYFTT5G","timestamp":1569470305104},{"file_id":"1d1z054lEUY0fhFBoPSHzudx1Sgye-i9K","timestamp":1569467647024},{"file_id":"1G9fhfMI7yopjX-BssS6jGw2rAdUfDnR_","timestamp":1569463670397},{"file_id":"1HKHFDHWgPfnEaM5eMxk09l90wbFq8ili","timestamp":1569459481074},{"file_id":"1ByVA4VPrKExk1n1duRcDy1FNvxVQ32E8","timestamp":1569444624068},{"file_id":"1EwBnDmH_l5HxoZP0xOWNXHuHIX6ph8Ds","timestamp":1569435170029}],"collapsed_sections":["x71SZjggBQs6","GfmZ5jn6BQs_","5NQB7-hBBQtK","-HysMAnmBQtP","Dnq4DoDtBQtQ","WCEaDfP_BQtS","XJ8fpqNBBQtY"],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9FPTpQUvBQrj","colab_type":"text"},"source":["# Neural Network Optimization and Tuning\n","\n","You've learned how to build computational graphs in PyTorch and compute gradients. The final piece to training a network is applying the gradients to update the network parameters. In this tutorial you will learn how to implement a number of optimization techniques in PyTorch along with other tuning methods. "]},{"cell_type":"code","metadata":{"id":"IiW3SBB-NKGr","colab_type":"code","colab":{}},"source":["import torch\n","import random\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms, datasets, models\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.autograd import Variable\n","from collections import namedtuple\n","from IPython.display import Image\n","import multiprocessing\n","import pdb \n","%matplotlib inline\n","np.random.seed(2019)\n","\n","np_load_old = np.load\n","\n","#modify the default parameters of np.load\n","np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQNp9p_Eqre3","colab_type":"code","colab":{}},"source":["PRE_PADDING = 12\n","POST_PADDING = 12"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDo1jL7QqtfU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKVBLBiFqszs","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2PDLtwcQkAd","colab_type":"code","colab":{}},"source":["# import imp\n","# imp.reload(np)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"60kqp7KibC9e","colab_type":"code","colab":{}},"source":["# import importlib\n","# importlib.reload(np)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7ZoB-hyBr_k","colab_type":"code","outputId":"b83e6271-897c-4265-9d25-a2168ff443bd","executionInfo":{"status":"ok","timestamp":1569504355917,"user_tz":240,"elapsed":22532,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KlUKkUiPj4XM","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"H6I_yAV4EoTc","colab_type":"code","colab":{}},"source":["# !mkdir -p ~/.kaggle\n","# !cp kaggle.json ~/.kaggle/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"euHX9Po-Ctcl","colab_type":"code","colab":{}},"source":["# ! kaggle competitions download 11785-hw1-fall2019"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"furVdIRsDJeR","colab_type":"code","outputId":"f2affd10-3104-405f-ab83-005092a63a72","executionInfo":{"status":"ok","timestamp":1569504369350,"user_tz":240,"elapsed":308,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["% cd drive/My\\ Drive/11-785"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/11-785\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l4nlwnW9J8FC","colab_type":"code","outputId":"c336b9dc-005f-4719-89fe-65c7c4d440e2","executionInfo":{"status":"ok","timestamp":1569504370691,"user_tz":240,"elapsed":1281,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":49}},"source":["! ls 11-785hw1p2-f19/"],"execution_count":9,"outputs":[{"output_type":"stream","text":[" dev_labels.npy   test.npy\t   train_labels.npy\n"," dev.npy\t 'train (1).npy'   train.npy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5hWIJgiEIP2t","colab_type":"code","colab":{}},"source":["#! tar -xvf 11-785hw1p2-f19.tar.gz"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pV5Him4DoKDp","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"cd5ZODnwBQro","colab_type":"code","colab":{}},"source":["def pad_constant_central_pattern(x, kval_pre, kval_post):\n","    \"\"\"\n","    Takes one 2-dimensional array with the constant value of padding. \n","    Pads the instances with the given constant value while\n","    maintaining the array at the center of the padding.\n","\n","    Parameters: \n","    x (numpy.ndarray): 2-d numpy array.\n","    cval (numpy.int64): scalar quantity.\n","    \n","    Returns: \n","    numpy.ndarray: 3-dimensional int numpy array, (n, m, k).\n","    \"\"\"\n","    spectrogram = x\n","    \n","    # Input function dimension specification\n","\n","    \n","    assert(x.ndim == 2)\n","\n","    dim1 = x.shape[0] +  kval_pre + kval_post   # n\n","    dim2 = x.shape[1]\n","\n","    result = np.ones((dim1,dim2))\n","\n","    \n","    pad_above = kval_pre\n","    pad_below = kval_post\n","        \n","    pad_left = 0\n","    pad_right = 0\n","    \n","    n_add = ((pad_above, pad_below), (pad_left, pad_right))\n","\n","    result = np.pad(x , pad_width=n_add , mode = 'edge') \n","    \n","\n","    # Assert output function dimension specification\n","    assert(result.shape[0] == dim1)\n","    assert(result.shape[1] == dim2)\n","    \n","    return result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qaH1ieKPBQrq","colab_type":"code","outputId":"c9087797-a28b-421d-c5f1-0b2d6b98f284","executionInfo":{"status":"ok","timestamp":1569504371331,"user_tz":240,"elapsed":424,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"imDAa7XKMz2g","colab_type":"code","outputId":"2a0c588b-5532-49e0-d381-7af8236a9890","executionInfo":{"status":"ok","timestamp":1569504435430,"user_tz":240,"elapsed":64234,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":49}},"source":["X = np.load(\"11-785hw1p2-f19/train.npy\")\n","Y = np.load(\"11-785hw1p2-f19/train_labels.npy\")\n","print(X.shape)\n","print(Y.shape)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["(24500,)\n","(24500,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m4VCSEFVNP48","colab_type":"code","outputId":"71b6913e-30f4-4f7c-fe63-91dbbd291f67","executionInfo":{"status":"ok","timestamp":1569504435438,"user_tz":240,"elapsed":63910,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":82}},"source":["train_data_length = int(X.shape[0] * 0.95)\n","X_train = X[0:train_data_length]\n","X_val = X[train_data_length:]\n","Y_train = Y[0:train_data_length]\n","Y_val = Y[train_data_length:]\n","print(X_train.shape)\n","print(Y_train.shape)\n","print(X_val.shape)\n","print(Y_val.shape)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["(23275,)\n","(23275,)\n","(1225,)\n","(1225,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HLlCT_MVMUo9","colab_type":"code","colab":{}},"source":["from torch.utils import data\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","class UtteranceDataset(data.Dataset):\n","    \"\"\"\n","\n","    Arguments:\n","        \n","    \"\"\"\n","\n","    def __init__(self, utterance, labels, transform=None, pre_padding = 0, post_padding = 0, frame_dim = 40):\n","        \n","        self.utterance = utterance\n","        self.labels = labels.tolist()\n","        \n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.pre_padding = pre_padding\n","        self.post_padding = post_padding\n","        self.frame_dim = frame_dim\n","\n","    def __getitem__(self, index):\n","        \n","        #utterance = self.X_train[index]\n","        #Y = self.Y_train[index]\n","        \n","        slice_size = self.pre_padding + self.post_padding + 1\n","        dim1 = self.utterance.shape[0]\n","        dim2 = self.frame_dim * slice_size\n","        \n","        utterance = pad_constant_central_pattern(self.utterance, self.pre_padding, self.post_padding)\n","        \n","        i = random.randint(0, dim1 - 1)\n","        #x = torch.from_numpy(utterance[i]).type(torch.FloatTensor)\n","        \n","        x = torch.from_numpy(utterance[i: i + slice_size].flatten()).type(torch.FloatTensor)#.to(self.device)\n","        y = self.labels[i]\n","        #x = torch.from_numpy(np.random.randn(40*slice_size)).type(torch.FloatTensor)\n","        \n","        assert x.shape[0] == dim2\n","        \n","        return x, y\n","\n","    def __len__(self):\n","        return self.utterance.shape[0]\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZCiX-bNbags","colab_type":"code","outputId":"dc6c74f1-c6fc-4832-fba6-66c37d5cb464","executionInfo":{"status":"ok","timestamp":1569504437137,"user_tz":240,"elapsed":64958,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":49}},"source":["datasets = [UtteranceDataset(x, y, pre_padding = PRE_PADDING, post_padding = PRE_PADDING) for x, y in zip(X_train, Y_train)]\n","train_data = data.ConcatDataset(datasets)\n","print(len(datasets[0]))\n","len(train_data)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["477\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["14651349"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"dzf0RAemmyFb","colab_type":"code","colab":{}},"source":["train_data_size = len(train_data)\n","weights = []\n","#one = 0\n","for dataset in datasets:\n","  utterance_length = len(dataset)\n","  weight_array = np.empty(utterance_length)\n","  utterance_weight = 1 / train_data_size\n","  \n","  #one += utterance_weight\n","  \n","  weight_array.fill(utterance_weight)\n","  weights.append(weight_array)\n","\n","weights_array = np.concatenate(np.array(weights))\n","\n","#sum(weights_array)  \n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5hupL4jDBQrs","colab_type":"code","colab":{}},"source":["from torch.utils import data\n","#from sklearn.preprocessing import MultiLabelBinarizer\n","\n","class SpectrogramTrainDataset(data.Dataset):\n","    \"\"\"\n","\n","    Arguments:\n","        \n","    \"\"\"\n","\n","    def __init__(self, path, X_train_name, Y_train_name, transform=None, pre_padding = 0, post_padding = 0, frame_dim = 40):\n","        \n","        self.X_train = np.load(path + X_train_name)\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        #self.X_test = np.load(path + X_test_name)\n","        \n","        self.Y_train = np.load(path + Y_train_name)\n","        #self.mlb = MultiLabelBinarizer(); \n","        #self.Y_train = self.mlb.fit_transform(np.load(path + Y_train_name))\n","        self.pre_padding = pre_padding\n","        self.post_padding = post_padding\n","        self.frame_dim = frame_dim\n","\n","    def __getitem__(self, index):\n","        \n","        utterance = self.X_train[index]\n","        Y = self.Y_train[index]\n","        \n","        slice_size = self.pre_padding + self.post_padding + 1\n","        dim1 = utterance.shape[0]\n","        dim2 = self.frame_dim * slice_size\n","        \n","        utterance = pad_constant_central_pattern(utterance, self.pre_padding, self.post_padding)\n","        \n","        i = random.randint(0, dim1 - 1)\n","        #x = torch.from_numpy(utterance[i]).type(torch.FloatTensor)\n","        y = Y[i]\n","        \n","        x = torch.from_numpy(utterance[i: i + slice_size].flatten()).type(torch.FloatTensor)#.to(self.device)\n","        #x = torch.from_numpy(np.random.randn(40*slice_size)).type(torch.FloatTensor)\n","        \n","        assert x.shape[0] == dim2\n","        \n","        return x, y\n","\n","    def __len__(self):\n","        return self.X_train.shape[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"24Qo0cliBQru","colab_type":"code","colab":{}},"source":["class SpectrogramTestDataset(data.Dataset):\n","    \"\"\"\n","\n","    Arguments:\n","        \n","    \"\"\"\n","\n","    def __init__(self, path, X_test_name, transform=None, pre_padding = 5, post_padding = 5, frame_dim = 40):\n","        \n","        \n","        self.pre_padding = pre_padding\n","        self.post_padding = post_padding\n","        self.frame_dim = frame_dim\n","        \n","        slice_size = self.pre_padding + self.post_padding + 1\n","        \n","        test = np.load(path + X_test_name)\n","        dim1 = sum([t.shape[0] for t in test])\n","        dim2 = self.frame_dim * slice_size\n","\n","        self.X_test = torch.empty(dim1, dim2).to(device)\n","\n","        X_test_index = 0\n","\n","        for utterance in test:\n","\n","            slice_size = self.pre_padding + self.post_padding + 1\n","            nframes = utterance.shape[0]\n","\n","            utterance = pad_constant_central_pattern(utterance, self.pre_padding, self.post_padding)\n","\n","            for i in range(nframes):\n","                self.X_test[X_test_index] = torch.from_numpy(utterance[i: i + slice_size].flatten()).type(torch.FloatTensor).to(device)\n","                X_test_index += 1\n","        \n","\n","    def __getitem__(self, index):\n","        \n","        x = self.X_test[index]\n","        \n","        return x\n","\n","    def __len__(self):\n","        return self.X_test.shape[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6WVrdp8ORgEg","colab_type":"code","colab":{}},"source":["class SpectrogramValDataset(data.Dataset):\n","    \"\"\"\n","\n","    Arguments:\n","        \n","    \"\"\"\n","\n","    def __init__(self, X_val, Y_val, transform=None, pre_padding = 5, post_padding = 5, frame_dim = 40):\n","        \n","        self.Y = np.concatenate(Y_val)\n","        \n","        self.pre_padding = pre_padding\n","        self.post_padding = post_padding\n","        self.frame_dim = frame_dim\n","        \n","        slice_size = self.pre_padding + self.post_padding + 1\n","        \n","        test = X_val\n","        dim1 = sum([t.shape[0] for t in test])\n","        dim2 = self.frame_dim * slice_size\n","\n","        self.X_test = torch.empty(dim1, dim2).to(device)\n","\n","        X_test_index = 0\n","\n","        for utterance in test:\n","\n","            slice_size = self.pre_padding + self.post_padding + 1\n","            nframes = utterance.shape[0]\n","\n","            utterance = pad_constant_central_pattern(utterance, self.pre_padding, self.post_padding)\n","\n","            for i in range(nframes):\n","                self.X_test[X_test_index] = torch.from_numpy(utterance[i: i + slice_size].flatten()).type(torch.FloatTensor)#.to(device)\n","                X_test_index += 1\n","        \n","\n","    def __getitem__(self, index):\n","        \n","        x = self.X_test[index]\n","        y = self.Y[index]\n","        \n","        return x, y\n","\n","    def __len__(self):\n","        return self.X_test.shape[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zkegZU9PR61n","colab_type":"code","colab":{}},"source":["val_dataset = SpectrogramValDataset(X_val, Y_val, pre_padding = PRE_PADDING, post_padding = PRE_PADDING)\n","\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1000)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yAAbB_v1fjvn","colab_type":"code","outputId":"58594206-c206-4df1-bd03-97bd59b4701e","executionInfo":{"status":"ok","timestamp":1569504481130,"user_tz":240,"elapsed":107054,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["val_size = len(val_dataset)\n","val_size"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["737364"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"0TTblcIu6wrl","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"55rPXOiNBQr1","colab_type":"code","colab":{}},"source":["#train_data = SpectrogramTrainDataset('11-785hw1p2-f19/', \"train.npy\", \"train_labels.npy\", pre_padding=5, post_padding=5)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdp1y-K3BQr4","colab_type":"code","colab":{}},"source":["nworkers = multiprocessing.cpu_count()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hEO9nnndBQr7","colab_type":"code","colab":{}},"source":["# n_frames = 0\n","# for utterance in train_data.X_train:\n","#     n_frames+=utterance.shape[0]\n","# print(n_frames)\n","# print(n_frames/len(train_data.X_train))\n","\n","\n","# weights = []\n","# for utterance in train_data.X_train: \n","#     weight = utterance.shape[0] / n_frames\n","#     weights.append(weight)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-CidatiBQr9","colab_type":"code","colab":{}},"source":["SpectrogramLoader = data.DataLoader(\n","    train_data, \n","    batch_size=1000,      # Batch size\n","    shuffle=True,      # Shuffles the dataset at every epoch\n","    pin_memory=False,   # Copy data to CUDA pinned memory\n","                       # so that they can be transferred to the GPU very fast\n","    num_workers=nworkers      # Number of worker processes for loading data.\n","                       # If zero, use the current process (blocks until data are loaded)\n","                       # Otherwise fork/spawn new processes (asynchronous load)\n","                       # Spawning new processes can be problematic on Windows, see:\n","                       # https://pytorch.org/docs/stable/notes/windows.html#usage-multiprocessing\n",")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jC6-vqBtBQsE","colab_type":"code","colab":{}},"source":["train_size = train_data.__len__()\n","\n","#val_size, train_size = int(0.05 * train_size), int(0.95 * train_size) # 80 / 20 train-val split\n","\n","#train_weights = weights_array.tolist()[val_size: val_size+train_size] \n","#val_weights = weights_array.tolist()[0:val_size]\n","train_sampler = torch.utils.data.sampler.RandomSampler(train_data, replacement = False)\n","#val_sampler = torch.utils.data.sampler.RandomSampler(, replacement = False)\n","#train_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_weights, len(train_weights), replacement = False)\n","#val_sampler = torch.utils.data.sampler.WeightedRandomSampler(val_weights, len(val_weights), replacement = False)\n","\n","batch_size = 1000\n","\n","# Add dataset to dataloader that handles batching\n","train_loader = torch.utils.data.DataLoader(train_data, \n","                                           batch_size=batch_size,\n","                                           #shuffle = True, \n","                                           #pin_memory = True, \n","                                           num_workers = nworkers, \n","                                           sampler=train_sampler)\n","\n","# val_loader = torch.utils.data.DataLoader(train_data, \n","#                                            batch_size=batch_size, \n","#                                            sampler=val_sampler)\n","\n","# Setup metric class\n","Metric = namedtuple('Metric', ['loss', 'train_error', 'val_error'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hJkcGqZJLne-","colab_type":"code","outputId":"b2d2b67a-41d6-4e88-eb64-9d00880b19ff","executionInfo":{"status":"ok","timestamp":1569504481727,"user_tz":240,"elapsed":105300,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":214}},"source":["class PhonemeModel(nn.Module):\n","    def __init__(self):\n","        super(PhonemeModel, self).__init__()\n","        self.fc1 = nn.Linear((PRE_PADDING + POST_PADDING + 1) * 40, 1500)\n","        self.fc2 = nn.Linear(1500, 3000)\n","        self.fc3 = nn.Linear(3000, 4500)\n","        self.fc4 = nn.Linear(4500, 3000)\n","        self.fc5 = nn.Linear(3000, 1500)\n","        self.fc6 = nn.Linear(1500, 138)\n","        \n","        self.bn1 = nn.BatchNorm1d(1500)\n","        self.bn2 = nn.BatchNorm1d(3000)\n","        self.bn3 = nn.BatchNorm1d(4500)\n","        \n","        self.dropout = nn.Dropout(p=0.2)\n","    \n","    def forward(self, x):\n","        \n","        x = self.bn1(F.relu(self.fc1(x)))\n","        x = self.bn2(F.relu(self.fc2(x)))\n","        x = self.dropout(self.bn3(F.relu(self.fc3(x))))\n","        x = self.bn2(F.relu(self.fc4(x)))\n","        x = self.bn1(F.relu(self.fc5(x)))\n","        x = F.log_softmax(self.fc6(x))\n","        return x\n"," \n","print(PhonemeModel())"],"execution_count":26,"outputs":[{"output_type":"stream","text":["PhonemeModel(\n","  (fc1): Linear(in_features=1000, out_features=1500, bias=True)\n","  (fc2): Linear(in_features=1500, out_features=3000, bias=True)\n","  (fc3): Linear(in_features=3000, out_features=4500, bias=True)\n","  (fc4): Linear(in_features=4500, out_features=3000, bias=True)\n","  (fc5): Linear(in_features=3000, out_features=1500, bias=True)\n","  (fc6): Linear(in_features=1500, out_features=138, bias=True)\n","  (bn1): BatchNorm1d(1500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (bn2): BatchNorm1d(3000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (bn3): BatchNorm1d(4500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (dropout): Dropout(p=0.2)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CHcv51AyJSuy","colab_type":"code","colab":{}},"source":["# import pdb"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JyRkJbuutRls","colab_type":"code","colab":{}},"source":["from tqdm import tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X47lq_CtBQsP","colab_type":"code","colab":{}},"source":["def inference(model, loader, n_members):\n","    correct = 0\n","    for data, label in loader:\n","        X = Variable(data.view(-1, (PRE_PADDING+ POST_PADDING + 1) * 40)).to(device) # replace with input_size\n","        Y = Variable(label).to(device)\n","        out = model(X).to(device)\n","        pred = out.data.max(1, keepdim=True)[1]\n","        predicted = pred.eq(Y.data.view_as(pred))\n","        correct += predicted.sum()\n","    return correct.item() / n_members\n","\n","\n","class Trainer():\n","    \"\"\" \n","    A simple training cradle\n","    \"\"\"\n","    \n","    def __init__(self, model, optimizer, load_path=None):\n","        self.model = model\n","        if load_path is not None:\n","            self.model = torch.load(load_path)\n","        self.optimizer = optimizer\n","            \n","    def save_model(self, path):\n","        torch.save(self.model.state_dict(), path)\n","\n","    def run(self, epochs):\n","        print(\"Start Training...\")\n","        self.metrics = []\n","        for e in range(epochs):\n","            #print(\"step\")\n","            scheduler.step()\n","            #print(\"init\")\n","            epoch_loss = 0\n","            correct = 0\n","            #print(\"loader\")\n","            #pdb.set_trace()\n","            for batch_idx, (data, label) in enumerate(tqdm(train_loader)):\n","                #print(\"zero grad\")\n","                self.optimizer.zero_grad()\n","                #print(\"x\")\n","                X = Variable(data.view(-1, (PRE_PADDING+ POST_PADDING + 1) * 40)).to(device) # replace with input_size\n","                #print(\"y\")\n","                Y = Variable(label).to(device)\n","                #pdb.set_trace()\n","                #print(\"forward\")\n","                out = self.model(X).to(device)\n","                pred = out.data.max(1, keepdim=True)[1]\n","                predicted = pred.eq(Y.data.view_as(pred))\n","                correct += predicted.sum()\n","#                 print(correct)\n","#                 print(correct.item())\n","#                 print(train_size)\n","#                 print(type(correct))\n","#                 print(type(correct.item()))\n","#                 print(type(train_size))\n","#                 print(correct.cpu() / train_size)\n","                loss = F.nll_loss(out, Y)\n","                \n","#                 print(\"X shape is: {}\".format(X.shape))\n","#                 print(\"Y shape is: {}\".format(Y.shape))\n","#                 print(\"Out shape is: {}\".format(out.shape))\n","#                 print(\"Pred shape is: {}\".format(pred.shape))\n","                #print(\"Pred is: {}\".format(pred))\n","                #print(\"Pred[0] is: {}\".format(pred[0]))\n","                #print(\"Loss is: {}\".format(loss))\n","                #print(\"backward\")\n","                loss.backward()\n","                self.optimizer.step()\n","                epoch_loss += loss.item()\n","            total_loss = epoch_loss/train_size\n","            train_error = 1.0 - (correct.cpu().item()/train_size)\n","            #print(\"correct / train_size is: {}\".format(correct / train_size))\n","            val_error = 1.0 - inference(self.model, val_loader, val_size)\n","            print(\"\\n\\nepoch: {0}, loss: {1:.8f}\".format(e+1, total_loss))\n","            print(\"train_error is: {}\".format(train_error))\n","            print(\"val error is: {}\".format(val_error))            \n","            self.metrics.append(Metric(loss=total_loss, \n","                                  train_error=train_error,\n","                                  val_error=val_error))\n","         "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8CLu1Sa9BQsR","colab_type":"code","colab":{}},"source":["# def init_randn(m):\n","#     if type(m) == nn.Linear:\n","#         m.weight.data.normal_(0,1)\n","\n","# def init_he_normal(m):\n","#   if type(m) == nn.Linear:\n","#     nn.init.kaiming_normal_(m.weight)\n","\n","# def init_xavier_uniform(m):\n","#   if type(m) == nn.Linear:\n","#     nn.init.xavier_uniform_(m.weight)\n","\n","def init_he_uniform(m):\n","  if type(m) == nn.Linear:\n","    nn.init.kaiming_uniform_(m.weight)\n","\n","# We first initialize a Fashion Object and initialize the parameters \"normally\".\n","model = PhonemeModel().to(device)\n","model.apply(init_he_uniform)\n","\n","AdamOptimizer = torch.optim.Adam(model.parameters(), lr=0.0009)\n","scheduler = torch.optim.lr_scheduler.StepLR(AdamOptimizer, step_size=4, gamma=0.90)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hQBzeJCaBQsT","colab_type":"code","outputId":"6e675c95-df91-4bf4-ce58-fd2a6c90a034","executionInfo":{"status":"error","timestamp":1569467630735,"user_tz":240,"elapsed":43942,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["np.random.seed(2019)\n","\n","modelTrainer = Trainer(model, AdamOptimizer)\n","modelTrainer.run(5)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/14652 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Start Training...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  6%|▋         | 942/14652 [02:54<42:52,  5.33it/s]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"jitcjUY-BQsV","colab_type":"code","colab":{}},"source":["def training_plot(metrics):\n","    plt.figure(1)\n","    plt.plot([m.loss for m in metrics], 'b')\n","    plt.title('Training Loss')\n","    plt.show()\n","\n","training_plot(modelTrainer.metrics)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KTYtgukfBQsY","colab_type":"code","colab":{}},"source":["def training_plot(metrics):\n","    plt.figure(1)\n","    plt.plot([m.val_error for m in metrics], 'b')\n","    plt.plot([m.train_error for m in metrics], 'r')\n","    plt.title('Val Error (blue) Train Error (red)')\n","    plt.show()\n","\n","training_plot(modelTrainer.metrics)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0l-ah0gMDb3q","colab_type":"code","colab":{}},"source":["modelTrainer.metrics[4]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMMHPjraBQsc","colab_type":"code","colab":{}},"source":["test_data = SpectrogramTestDataset('11-785hw1p2-f19/', \"test.npy\", pre_padding=PRE_PADDING, post_padding=POST_PADDING)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4BdHo_l9XoCh","colab_type":"code","colab":{}},"source":["test_loader = torch.utils.data.DataLoader(test_data, \n","                                           batch_size=5000)\n","\n","pred = []\n","for data in test_loader:\n","    X = Variable(data.view(-1, (PRE_PADDING+ POST_PADDING + 1) * 40))\n","    out = modelTrainer.model(X)\n","    pred.append(out.data.max(1, keepdim=True)[1].cpu().numpy())\n","    #pred.append(out.data.max(1, keepdim=True)[1])\n","pred = np.concatenate(np.array(pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B76aKjCZBQsl","colab_type":"code","colab":{}},"source":["import pandas as pd\n","df = pd.DataFrame(pred, columns=[\"label\"])\n","df.to_csv('preds_model_30_9_25.csv', index_label = \"id\", index=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Jn_nN0LBQsm","colab_type":"code","colab":{}},"source":["torch.save(modelTrainer.model, \"model_30_9_25.pt\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z5WbobL5BQsy","colab_type":"text"},"source":["# How to adjust Learning Rate\n","\n","`torch.optim.lr_scheduler` provides several methods to adjust the learning rate based on the number of epochs. Learning rate scheduling should be applied after optimizer’s update. See https://pytorch.org/docs/stable/optim.html for details."]},{"cell_type":"code","metadata":{"id":"oLd6aANqBQsz","colab_type":"code","colab":{}},"source":["def inference(model, loader, n_members):\n","    correct = 0\n","    for data, label in loader:\n","        X = Variable(data.view(-1, 440))\n","        Y = Variable(label)\n","        out = model(X)\n","        pred = out.data.max(1, keepdim=True)[1]\n","        predicted = pred.eq(Y.data.view_as(pred))\n","        correct += predicted.sum()\n","    return correct.numpy() / n_members\n","\n","class Trainer():\n","    \"\"\" \n","    A simple training cradle\n","    \"\"\"\n","    \n","    def __init__(self, model, optimizer, load_path=None):\n","        self.model = model\n","        if load_path is not None:\n","            self.model = torch.load(load_path)\n","        self.optimizer = optimizer\n","            \n","    def save_model(self, path):\n","        torch.save(self.model.state_dict(), path)\n","\n","    def run(self, epochs):\n","        print(\"Start Training...\")\n","        self.metrics = []\n","        for e in range(n_epochs):\n","            scheduler.step()\n","            epoch_loss = 0\n","            correct = 0\n","            for batch_idx, (data, label) in enumerate(train_loader):\n","                self.optimizer.zero_grad()\n","                X = Variable(data.view(-1, 440))\n","                Y = Variable(label)\n","                out = self.model(X)\n","                pred = out.data.max(1, keepdim=True)[1]\n","                predicted = pred.eq(Y.data.view_as(pred))\n","                correct += predicted.sum()\n","                loss = F.nll_loss(out, Y)\n","                loss.backward()\n","                self.optimizer.step()\n","                epoch_loss += loss.item()\n","            total_loss = epoch_loss/train_size\n","            train_error = 1.0 - correct/train_size\n","            val_error = 1.0 - inference(self.model, val_loader, val_size)\n","            print(\"epoch: {0}, loss: {1:.8f}\".format(e+1, total_loss))\n","            self.metrics.append(Metric(loss=total_loss, \n","                                  train_error=train_error,\n","                                  val_error=val_error))\n","         "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BNrxceIsBQs1","colab_type":"code","colab":{}},"source":["### LET'S TRAIN ###\n","\n","# A function to apply \"normal\" distribution on the parameters\n","def init_randn(m):\n","    if type(m) == nn.Linear:\n","        m.weight.data.normal_(0,1)\n","\n","# We first initialize a Fashion Object and initialize the parameters \"normally\".\n","normalmodel = PhonemeModel()\n","normalmodel.apply(init_randn)\n","\n","n_epochs = 100\n","'''\n","print(\"SGD OPTIMIZER\")\n","SGDOptimizer = torch.optim.SGD(normalmodel.parameters(), lr=0.01)\n","scheduler = torch.optim.lr_scheduler.StepLR(SGDOptimizer, step_size=0.01, gamma=0.1)\n","sgd_trainer = Trainer(normalmodel, SGDOptimizer)\n","sgd_trainer.run(n_epochs)\n","sgd_trainer.save_model('./sgd_model.pt')\n","print('')\n","'''\n","\n","print(\"ADAM OPTIMIZER\")\n","normalmodel = PhonemeModel()\n","normalmodel.apply(init_randn)\n","AdamOptimizer = torch.optim.Adam(normalmodel.parameters(), lr=0.001)\n","scheduler = torch.optim.lr_scheduler.StepLR(AdamOptimizer, step_size=4, gamma=0.1)\n","adam_trainer = Trainer(normalmodel, AdamOptimizer)\n","adam_trainer.run(n_epochs)\n","adam_trainer.save_model('./adam_model.pt')\n","print('')\n","\n","\n","print(\"RMSPROP OPTIMIZER\")\n","normalmodel = PhonemeModel()\n","normalmodel.apply(init_randn)\n","RMSPropOptimizer = torch.optim.RMSprop(normalmodel.parameters(), lr=0.001)\n","scheduler = torch.optim.lr_scheduler.StepLR(RMSPropOptimizer, step_size=4, gamma=0.1)\n","rms_trainer = Trainer(normalmodel, RMSPropOptimizer)\n","rms_trainer.run(n_epochs)\n","rms_trainer.save_model('./rmsprop_model.pt')\n","print('')\n","\n","\n","### TEST ###\n","model = PhonemeModel()\n","model.load_state_dict(torch.load('./sgd_model.pt'))\n","test_acc = inference(model, test_loader, test_size)\n","print(\"Test accuracy of model optimizer with SGD: {0:.2f}\".format(test_acc * 100))\n","\n","model = PhonemeModel()\n","model.load_state_dict(torch.load('./adam_model.pt'))\n","test_acc = inference(model, test_loader, test_size)\n","print(\"Test accuracy of model optimizer with Adam: {0:.2f}\".format(test_acc * 100))\n","\n","model = PhonemeModel()\n","model.load_state_dict(torch.load('./rmsprop_model.pt'))\n","test_acc = inference(model, test_loader, test_size)\n","print(\"Test accuracy of model optimizer with RMSProp: {0:.2f}\".format(test_acc * 100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvWfR1brBQs2","colab_type":"code","colab":{}},"source":["model = PhonemeModel()\n","model.load_state_dict(torch.load('./rmsprop_model.pt'))\n","\n","#value = torch.from_numpy(x_train[example_index])\n","\n","# then put it on the GPU, make it float and insert a fake batch dimension\n","arrays = [array for array in train_data.X_test]\n","test_value = Variable(torch.from_numpy(np.concatenate(arrays)))\n","test_value = test_value.float()\n","test_value = test_value.unsqueeze(0)\n","\n","# pass it through the model\n","prediction = model(test_value)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPilYg1PBQs4","colab_type":"code","colab":{}},"source":["### VISUALIZATION ###\n","def training_plot(metrics):\n","    plt.figure(1)\n","    plt.plot([m.loss for m in metrics], 'b')\n","    plt.title('Training Loss')\n","    plt.show()\n","\n","training_plot(sgd_trainer.metrics)\n","training_plot(adam_trainer.metrics)\n","training_plot(rms_trainer.metrics)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x71SZjggBQs6","colab_type":"text"},"source":["## Parameter Initialization\n","\n","While training a network, the initial value of the weights plays a significant role. In the extreme case, an oracle could just set the weights directly to values that minimize the objective function, and in practical cases a good initialization can bring us to a more favorable starting position in the parameter space. \n","\n","This raises the question of how to choose these weights. \n","\n","- What happens if all the weights are set to zero? The gradients become zero, and the network finds itself without a direction. \n","- What if all of them are set to the same non-zero value? Although the gradients are no longer zero, each neuron has the same weight and follows the same gradient. Such neurons will continue to have the same value, since they're identical. \n","\n","So any initialization scheme must break this symmetry somehow, and randomly initializing the weights is a first step in that direction.\n","\n","Let's begin with creating a weight initialization function that samples from **N(0,1)**. A clean way of initializing the weights is to access the network parameters by traversing all modules inside the network, and then applying the desired initialization. This method also allows us to encapsulate all the initializations into a single function."]},{"cell_type":"code","metadata":{"id":"7vpJDdaxBQs6","colab_type":"code","colab":{}},"source":["def init_randn(m):\n","    if type(m) == nn.Linear:\n","        m.weight.data.normal_(0,1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7gkQYLlFBQs9","colab_type":"text"},"source":["Now let's use this scheme to initialize the network.\n","\n","Note that *apply(fn)* applies the function *fn* recursively to every submodule (as returned by .children()) as well as self. Also, since it is applied to itself as well, you must take care to select the appropriate type of module *m* and apply the initialization to it."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"LeN-ciLcBQs9","colab_type":"code","colab":{}},"source":["normalmodel = FashionModel()\n","normalmodel.apply(init_randn)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GfmZ5jn6BQs_","colab_type":"text"},"source":["## Custom initializations\n","\n","We could also choose a different way to initialize the weights, where you explicitly copy some values into the weights."]},{"cell_type":"code","metadata":{"id":"nVvDt_7nBQs_","colab_type":"code","colab":{}},"source":["def init_custom(m):\n","    if type(m) == nn.Linear:\n","        rw = torch.randn(m.weight.data.size())\n","        m.weight.data.copy_(rw)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZgmLiZUABQtB","colab_type":"text"},"source":["Now let's use this initialization scheme to implement Xavier initialization. \n","\n","Xavier initialization is a way of initializing the weights such that the variance of the inputs is the same as the variance of the outputs. At each layer, the fan_in and fan_out (i.e. input connections and output connections) might be different. To calculate the variance, you will multiply each weight with the inputs. Evidently, if the number of inputs is less, they will need to be multiplied with higher weights so that they can sum up to the product of a larger number of outputs with smaller weights. This is the intuition behind Xavier initialization.\n"]},{"cell_type":"code","metadata":{"id":"myIhwBl6BQtC","colab_type":"code","colab":{}},"source":["def init_xavier(m):\n","    if type(m) == nn.Linear:\n","        fan_in = m.weight.size()[1]\n","        fan_out = m.weight.size()[0]\n","        std = np.sqrt(2.0 / (fan_in + fan_out))\n","        m.weight.data.normal_(0,std)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z_7hraAfBQtF","colab_type":"code","colab":{}},"source":["xaviermodel = FashionModel()\n","xaviermodel.apply(init_xavier)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9Ijf_Z8BQtH","colab_type":"code","colab":{}},"source":["### LET'S TRAIN ###\n","n_epochs = 3\n","\n","print(\"NORMAL INIT WEIGHTS\")\n","AdamOptimizer = torch.optim.Adam(normalmodel.parameters(), lr=0.001)\n","normal_trainer = Trainer(normalmodel, AdamOptimizer)\n","normal_trainer.run(n_epochs)\n","normal_trainer.save_model('./normal_model.pt')\n","print('')\n","\n","\n","print(\"XAVIER INIT WEIGHTS\")\n","AdamOptimizer = torch.optim.Adam(xaviermodel.parameters(), lr=0.001)\n","xavier_trainer = Trainer(xaviermodel, AdamOptimizer)\n","xavier_trainer.run(n_epochs)\n","xavier_trainer.save_model('./xavier_model.pt')\n","print('')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DzI4ZQ2VBQtJ","colab_type":"code","colab":{}},"source":["### VISUALIZATION ###\n","def training_plot(metrics):\n","    plt.figure(1)\n","    plt.plot([m.loss for m in metrics], 'b')\n","    plt.title('Training Loss')\n","    plt.show()\n","\n","training_plot(normal_trainer.metrics)\n","training_plot(xavier_trainer.metrics)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5NQB7-hBBQtK","colab_type":"text"},"source":["## Using pretrained weights\n","\n","In the previous section we saw that initializations can start the training from a good spot. In addition to these schemes, you might also need to have specific methods to initialize the weights in different layers. For example, you might want to use a pretrained model like Alexnet to give your network a head start for visual recognition tasks. Let's load the pretrained Alexnet model and see how it works."]},{"cell_type":"code","metadata":{"id":"dC8hqhqSBQtL","colab_type":"code","colab":{}},"source":["alexnet_model = models.alexnet(pretrained=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-HysMAnmBQtP","colab_type":"text"},"source":["## Adding Momentum\n","\n","We can make use of the `self.state` data-structure to maintain a copy of an accumulated gradient that we also decay at each step. Once again we use inplace operations to avoid unneccesary buffer allocation. Recall a standard update with momentum given decay rate $\\mu$.\n","\n","$$ \\begin{align}\n","\\textbf{V'} &= \\mu \\textbf{V} - \\eta \\nabla L(\\textbf{W})\\\\\n","\\textbf{W'} &= \\textbf{W} + \\textbf{V'}\\\\\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"Dnq4DoDtBQtQ","colab_type":"text"},"source":["## Batch Normalization\n","\n","Batch normalization is a relatively simple but significant improvement in training neural networks. In machine learning, *covariate shift* is a phenomenon in which the covariate distribution is non-stationary over the course of training. This is a common phenomenon in online learning. When training a neural network on a fixed dataset, there is no covariate shift (excluding sample noise from minibatch approximation), but the distribution of individual node and layer activity shifts as the network parameters are updated. As an abstraction, we can consider each node's activity to be a covariate of the following nodes in the network. Thus we can think of the non-stationarity of node (and layer) activations as a sort of *internal covariate shift*. \n","\n","Why is internal covariate shift a problem? Each subsequent layer has to account for a shifting distribution of its inputs. For saturating non-linearities the problem becomes even more dire, as the shift in activity will more likely place the unit output in the saturated region of the non-linearity.\n","\n"]},{"cell_type":"code","metadata":{"id":"zrCPLxv8BQtQ","colab_type":"code","colab":{}},"source":["class BatchNorm(nn.Module):\n","\n","    def __init__(self, num_features):\n","        super(BatchNorm, self).__init__()\n","        self.num_features = num_features\n","        self.affine = affine\n","        self.weight = Parameter(torch.Tensor(num_features))\n","        self.bias = Parameter(torch.Tensor(num_features))\n","        self.register_buffer('running_mean', torch.zeros(num_features))\n","        self.register_buffer('running_var', torch.ones(num_features))\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        self.running_mean.zero_()\n","        self.running_var.fill_(1)\n","        self.weight.data.uniform_()\n","        self.bias.data.zero_()\n","\n","    def forward(self, x):\n","        pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WCEaDfP_BQtS","colab_type":"text"},"source":["## Overfitting\n","Deep neural networks contain multiple non-linear hidden layers and this makes them very\n","expressive models that can learn very complicated relationships between their inputs and\n","outputs. With limited training data, however, many of these complicated relationships\n","will be the result of sampling noise, so they will exist in the training set but not in real\n","test data even if it is drawn from the same distribution. This leads to overfitting and many\n","methods have been developed for reducing it.\n","\n","## Dropout\n","Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks. The term \"dropout\" refers to dropping out units in a neural network.\n","\n","## Regularization (weight_decay)\n","\n","Weight decay specifies regularization in the neural network.\n","During training, a regularization term is added to the network's loss to compute the backpropagation gradient. The weight decay value determines how dominant this regularization term will be in the gradient computation.\n","\n","As a rule of thumb, the more training examples you have, the weaker this term should be. The more parameters you have the higher this term should be.\n"]},{"cell_type":"code","metadata":{"id":"yrYv37OLBQtU","colab_type":"code","colab":{}},"source":["class FashionModel_Tricks(nn.Module):\n","    \n","    def __init__(self):\n","        super(FashionModel_Tricks, self).__init__()\n","        self.fc1 = nn.Linear(784, 64)\n","        self.bnorm1 = nn.BatchNorm1d(64)\n","        self.dp1 = nn.Dropout(p=0.2)\n","        self.fc2 = nn.Linear(64, 32)\n","        self.bnorm2 = nn.BatchNorm1d(32)\n","        self.dp2 = nn.Dropout(p=0.1)\n","        self.fc3 = nn.Linear(32, 10)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = self.dp1(self.bnorm1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.dp2(self.bnorm2(x))\n","        x = F.log_softmax(self.fc3(x))\n","        return x\n","print(FashionModel_Tricks())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mLMDXNmVBQtW","colab_type":"code","colab":{}},"source":["### TRAIN MODELS WITH BATCHNORM AND DROPOUT ###\n","n_epochs = 10\n","\n","model = FashionModel_Tricks()\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9, weight_decay = 0.001)\n","btrainer = Trainer(model, optimizer)\n","btrainer.run(n_epochs)\n","btrainer.save_model('./dropout-batchnorm_optimized_model.pt')\n","\n","training_plot(btrainer.metrics)\n","\n","print('')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XJ8fpqNBBQtY","colab_type":"text"},"source":["## Gradient Clipping\n","\n","During experimentation, once the gradient value grows extremely large, it causes an overflow (i.e. NaN) which is easily detectable at runtime or in a less extreme situation, the Model starts overshooting past our Minima; this issue is called the Gradient Explosion Problem.\n","\n","Gradient clipping will ‘clip’ the gradients or cap them to a Threshold value to prevent the gradients from getting too large."]},{"cell_type":"code","metadata":{"id":"1Gsd_XUeBQtZ","colab_type":"code","colab":{}},"source":["#Gradient Clipping \n","# `clip_grad_norm` helps prevent the exploding gradient problem. To be used before optimizer.step()during training\n","torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vYDkkKmVBQta","colab_type":"text"},"source":["## Annealing Learning Rate\n","In training deep networks, it is usually helpful to anneal the learning rate over time. Good intuition to have in mind is that with a high learning rate, the system contains too much kinetic energy and the parameter vector bounces around chaotically, unable to settle down into deeper, but narrower parts of the loss function. Knowing when to decay the learning rate can be tricky: Decay it slowly and you’ll be wasting computation bouncing around chaotically with little improvement for a long time. But decay it too aggressively and the system will cool too quickly, unable to reach the best position it can. One way of doing it is using step decay. Step decay schedule drops the learning rate by a factor every few epochs. The mathematical form of step decay is:\n","\n","$$\\eta = \\eta_0 * drop^{\\floor ( \\frac{epoch}{epochs\\_drop})}$$"]},{"cell_type":"code","metadata":{"id":"KmI0kI30BQta","colab_type":"code","colab":{}},"source":["def step_decay(epoch):\n","    initial_lrate = 0.1\n","    drop = 0.5\n","    epochs_drop = 10.0\n","    lrate = initial_lrate * math.pow(drop,  \n","           math.floor((1+epoch)/epochs_drop))\n","    return lrate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"968UMvFABQtc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7UJtS2ZBUtn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}