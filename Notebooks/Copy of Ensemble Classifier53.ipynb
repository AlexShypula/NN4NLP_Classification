{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Copy of Ensemble Classifier53.ipynb","provenance":[{"file_id":"1TMP9nUMXGV-_vv1dw-2NI8_xImAhhKQu","timestamp":1581312419123},{"file_id":"10jm5EOsWZhZbZxu5R66cVbVyy63vqY2C","timestamp":1580708133931},{"file_id":"1kXcaja-5l-ZJIDVlqeNbZzIw2hhmvJpI","timestamp":1580704834858},{"file_id":"19rjk07FZiwdR2vcUu4XotoojR9h4jW4N","timestamp":1580703764562},{"file_id":"1G3srQDfp_pUg8O-t0nB8VCrN_RbBnXAV","timestamp":1580614625857},{"file_id":"1ofx6a6dasWAhK3T0TH9JZMWCQ_2rfzDj","timestamp":1580273350581},{"file_id":"1ED5e4ix1fNMHWEL5dgGJ_O8K6bfTAcuk","timestamp":1580267148468},{"file_id":"1huuuSmGDlDTPMlbFHAX27vof7qF4BeLN","timestamp":1580266605400},{"file_id":"1TZiMiTWcGHWRCTSKigbYgqtz3FCRmC2W","timestamp":1580263420924},{"file_id":"1pAUwp3i142c7G1uJmPOCiACW6Si0ezor","timestamp":1580262006654},{"file_id":"1ufq2JpnXay_JsRo4vhhIGSUvjRw5er7O","timestamp":1580232037354},{"file_id":"1uIeQCgKK4IzV6nCuridHnIOjGxlEtQ8f","timestamp":1579903800063},{"file_id":"1pMsQwNuwLvpMdHY_Vj9c467qvQCMAyJi","timestamp":1579901507622}]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"354d7490533e4699ad0e5e78b8bb136c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_29ef72741ffc45a08bcbf7f060dba35f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e0a1c3f3cd74497f9b46c9b47c4cc063","IPY_MODEL_5a6ee4d4514a4cbf9ec323cf071c833f"]}},"29ef72741ffc45a08bcbf7f060dba35f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0a1c3f3cd74497f9b46c9b47c4cc063":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_aa0e47d4ab79416cb00bfb14c1f4048f","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":11,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":11,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_21c3c8999af6461883497b1efd8a8c01"}},"5a6ee4d4514a4cbf9ec323cf071c833f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b8b91c685b454534ad9a57cd6417dcfb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 11/11 [00:01&lt;00:00,  7.79it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_15ab1d8a74e74336abc7cac1f75cfd93"}},"aa0e47d4ab79416cb00bfb14c1f4048f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"21c3c8999af6461883497b1efd8a8c01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b8b91c685b454534ad9a57cd6417dcfb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"15ab1d8a74e74336abc7cac1f75cfd93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d15c149ffa1348cf9308e643ea88e47e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1265eae7fc8047ea985378f6fd21872d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a7c1539b1d2147308be1deb5552d5239","IPY_MODEL_0ef650cf94f04412a57adb2867421307"]}},"1265eae7fc8047ea985378f6fd21872d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a7c1539b1d2147308be1deb5552d5239":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7640076764c2455e87cf796c06e19ed2","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":11,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":11,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6ae5cec64f5d44098b8fdc12733ae7e2"}},"0ef650cf94f04412a57adb2867421307":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_460f40db2bb64030bdb5d860ca3e266c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 11/11 [00:00&lt;00:00, 20.71it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fc5eb4f88c774a5aa3a2e8facac7901e"}},"7640076764c2455e87cf796c06e19ed2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6ae5cec64f5d44098b8fdc12733ae7e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"460f40db2bb64030bdb5d860ca3e266c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fc5eb4f88c774a5aa3a2e8facac7901e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"SsKE9VwfucfE","colab_type":"code","outputId":"ca6c03ec-9074-4dc2-aac8-275f346d16a2","executionInfo":{"status":"ok","timestamp":1581313338693,"user_tz":300,"elapsed":3413,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["! pip install pytorch-nlp"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.17.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.28.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wb2bLnjL3W88","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import torchtext\n","\n","from torchtext.data import TabularDataset, Field, Iterator\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import Vocab\n","from torch import optim \n","from torch.optim import lr_scheduler\n","import matplotlib.pyplot as plt\n","from torch.nn import functional as F\n","from torchnlp.nn import Attention\n","\n","import copy\n","import time\n","from collections import namedtuple\n","from tqdm import tqdm, tqdm_notebook"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rz8AqPnI3kNO","colab_type":"code","outputId":"8d5e753a-a12a-43ce-a0ad-ff21830b8bfb","executionInfo":{"status":"ok","timestamp":1581314329337,"user_tz":300,"elapsed":415,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fxKQyqa438P0","colab_type":"code","outputId":"1ff4f8c0-ab0a-42e9-dc5c-ebff1439b478","executionInfo":{"status":"ok","timestamp":1581314330551,"user_tz":300,"elapsed":503,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd drive/\"My Drive\"/NN4NLP/topicclass"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/NN4NLP/topicclass\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X-nQhfuF4KOa","colab_type":"code","outputId":"db7be53b-536d-440c-cbb9-baede3ad2412","executionInfo":{"status":"ok","timestamp":1581312752034,"user_tz":300,"elapsed":5543,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["! ls"],"execution_count":6,"outputs":[{"output_type":"stream","text":["bestMobile_v2_NetModel.pt\n","Classifier.ipynb\n","ConvNetClassificationTesting_10.txt\n","ConvNetClassificationTesting_11.txt\n","ConvNetClassificationTesting_12.txt\n","ConvNetClassificationTesting_13.txt\n","ConvNetClassificationTesting_2.txt\n","ConvNetClassificationTesting_3.txt\n","ConvNetClassificationTesting_4.txt\n","ConvNetClassificationTesting_5.txt\n","ConvNetClassificationTesting_6.txt\n","ConvNetClassificationTesting_7.txt\n","ConvNetClassificationTesting_8.txt\n","ConvNetClassificationTesting_9.txt\n","ConvNetClassificationTesting_real_10.txt\n","ConvNetClassificationTesting_real_11.txt\n","ConvNetClassificationTesting_real_12_redo.txt\n","ConvNetClassificationTesting_real_12.txt\n","ConvNetClassificationTesting_real_13_redo.txt\n","ConvNetClassificationTesting_real_13.txt\n","ConvNetClassificationTesting_real_14_redo.txt\n","ConvNetClassificationTesting_real_14.txt\n","ConvNetClassificationTesting_real_15_redo.txt\n","ConvNetClassificationTesting_real_15.txt\n","ConvNetClassificationTesting_real_16_redo.txt\n","ConvNetClassificationTesting_real_16.txt\n","ConvNetClassificationTesting_real_17_redo.txt\n","ConvNetClassificationTesting_real_17.txt\n","ConvNetClassificationTesting_real_18.txt\n","ConvNetClassificationTesting_real_19.txt\n","ConvNetClassificationTesting_real_1.txt\n","ConvNetClassificationTesting_real_20.txt\n","ConvNetClassificationTesting_real_21.txt\n","ConvNetClassificationTesting_real_22.txt\n","ConvNetClassificationTesting_real_23.txt\n","ConvNetClassificationTesting_real_24.txt\n","ConvNetClassificationTesting_real_25.txt\n","ConvNetClassificationTesting_real_26.txt\n","ConvNetClassificationTesting_real_27_nonlinearity_drop.txt\n","ConvNetClassificationTesting_real_27_nonlinearity.txt\n","ConvNetClassificationTesting_real_27.txt\n","ConvNetClassificationTesting_real_28.txt\n","ConvNetClassificationTesting_real_29.txt\n","ConvNetClassificationTesting_real_2.txt\n","ConvNetClassificationTesting_real_2_with_nonstatic.gdoc\n","ConvNetClassificationTesting_real_2_with_nonstatic.txt\n","ConvNetClassificationTesting_real_31_drop_.5.txt\n","ConvNetClassificationTesting_real_31_drop.txt\n","ConvNetClassificationTesting_real_31.txt\n","ConvNetClassificationTesting_real_32.txt\n","ConvNetClassificationTesting_real_33.txt\n","ConvNetClassificationTesting_real_34.txt\n","ConvNetClassificationTesting_real_35.txt\n","ConvNetClassificationTesting_real_36.txt\n","ConvNetClassificationTesting_real_37.txt\n","ConvNetClassificationTesting_real_38.txt\n","ConvNetClassificationTesting_real_39.txt\n","ConvNetClassificationTesting_real_3.txt\n","ConvNetClassificationTesting_real_40.txt\n","ConvNetClassificationTesting_real_41.txt\n","ConvNetClassificationTesting_real_42.txt\n","ConvNetClassificationTesting_real_43.txt\n","ConvNetClassificationTesting_real_45.txt\n","ConvNetClassificationTesting_real_46.txt\n","ConvNetClassificationTesting_real_47.txt\n","ConvNetClassificationTesting_real_48.txt\n","ConvNetClassificationTesting_real_4_pt2.txt\n","ConvNetClassificationTesting_real_4.txt\n","ConvNetClassificationTesting_real_50.txt\n","ConvNetClassificationTesting_real_54.txt\n","ConvNetClassificationTesting_real_56.txt\n","ConvNetClassificationTesting_real_57.txt\n","ConvNetClassificationTesting_real_58.txt\n","ConvNetClassificationTesting_real_59.txt\n","ConvNetClassificationTesting_real_5.txt\n","ConvNetClassificationTesting_real_60.txt\n","ConvNetClassificationTesting_real_61.txt\n","ConvNetClassificationTesting_real_62.txt\n","ConvNetClassificationTesting_real_63.txt\n","ConvNetClassificationTesting_real_64.txt\n","ConvNetClassificationTesting_real_65.txt\n","ConvNetClassificationTesting_real_66.txt\n","ConvNetClassificationTesting_real_67.txt\n","ConvNetClassificationTesting_real_68.txt\n","ConvNetClassificationTesting_real_69_cont.txt\n","ConvNetClassificationTesting_real_69.txt\n","ConvNetClassificationTesting_real_6_pt2.txt\n","ConvNetClassificationTesting_real_6.txt\n","ConvNetClassificationTesting_real_70_cont.txt\n","ConvNetClassificationTesting_real_70.txt\n","ConvNetClassificationTesting_real_7.txt\n","ConvNetClassificationTesting_real_9.txt\n","ConvNetClassificationTesting.txt\n","crawl-300d-2M-subword.bin\n","crawl-300d-2M-subword.vec\n","crawl-300d-2M-subword.zip\n","crawl-300d-2M.vec\n","crawl-300d-2M.vec.zip\n","embedding_mtx.pt\n","int2Class.csv\n","MobileNetModel_10.pt\n","MobileNetModel_11.pt\n","MobileNetModel_18.pt\n","MobileNetModel_2.pt\n","MobileNetModel_3.pt\n","MobileNetModel_4.pt\n","MobileNetModel_5.pt\n","MobileNetModel_6.pt\n","MobileNetModel_7.pt\n","MobileNetModel_8.pt\n","MobileNetModel_9.pt\n","Model_10.png\n","Model_10.pt\n","Model_11.png\n","Model_11.pt\n","Model_12.png\n","Model_12.pt\n","Model2.png\n","Model2.pt\n","Model_3.png\n","Model_3.pt\n","Model_4.png\n","Model_4.pt\n","Model_5.png\n","Model_5.pt\n","Model_6.png\n","Model_6.pt\n","Model_7.png\n","Model_7.pt\n","Model_8.png\n","Model_8.pt\n","Model_9.png\n","Model_9.pt\n","Model_real_10.png\n","Model_real_10.pt\n","Model_real_11.png\n","Model_real_11.pt\n","Model_real_12.png\n","Model_real_12.pt\n","Model_real_12_redo.png\n","Model_real_12_redo.pt\n","Model_real_13.png\n","Model_real_13.pt\n","Model_real_13_redo.png\n","Model_real_13_redo.pt\n","Model_real_14.png\n","Model_real_14.pt\n","Model_real_14_redo.png\n","Model_real_14_redo.pt\n","Model_real_15.png\n","Model_real_15.pt\n","Model_real_15_redo.png\n","Model_real_15_redo.pt\n","Model_real_16.png\n","Model_real_16.pt\n","Model_real_16_redo.png\n","Model_real_16_redo.pt\n","Model_real_17.png\n","Model_real_17.pt\n","Model_real_17_redo.png\n","Model_real_17_redo.pt\n","Model_real_18.png\n","Model_real_18.pt\n","Model_real_19.png\n","Model_real_19.pt\n","Model_real_1.png\n","Model_real_1.pt\n","Model_real_20.png\n","Model_real_20.pt\n","Model_real_21.png\n","Model_real_21.pt\n","Model_real_22.png\n","Model_real_22.pt\n","Model_real_23.png\n","Model_real_23.pt\n","Model_real_24.png\n","Model_real_24.pt\n","Model_real_25.png\n","Model_real_25.pt\n","Model_real_26.png\n","Model_real_26.pt\n","Model_real_27_nonlinearity_drop.png\n","Model_real_27_nonlinearity_drop.pt\n","Model_real_27_nonlinearity.png\n","Model_real_27_nonlinearity.pt\n","Model_real_27.png\n","Model_real_27.pt\n","Model_real_28.png\n","Model_real_28.pt\n","Model_real_29.png\n","Model_real_29.pt\n","Model_real_2.png\n","Model_real_2.pt\n","Model_real_2_with_nonstatic.png\n","Model_real_2_with_nonstatic.pt\n","Model_real_31_drop_.5.png\n","Model_real_31_drop_.5.pt\n","Model_real_31_drop.png\n","Model_real_31_drop.pt\n","Model_real_31.png\n","Model_real_31.pt\n","Model_real_32.png\n","Model_real_32.pt\n","Model_real_33.png\n","Model_real_33.pt\n","Model_real_34.png\n","Model_real_34.pt\n","Model_real_35.png\n","Model_real_35.pt\n","Model_real_36.png\n","Model_real_36.pt\n","Model_real_37.png\n","Model_real_37.pt\n","Model_real_38.png\n","Model_real_38.pt\n","Model_real_39.png\n","Model_real_39.pt\n","Model_real_3.png\n","Model_real_3.pt\n","Model_real_40.png\n","Model_real_40.pt\n","Model_real_41.png\n","Model_real_41.pt\n","Model_real_42.png\n","Model_real_42.pt\n","Model_real_43.png\n","Model_real_43.pt\n","Model_real_45.png\n","Model_real_45.pt\n","Model_real_46.png\n","Model_real_46.pt\n","Model_real_47.png\n","Model_real_47.pt\n","Model_real_48.png\n","Model_real_48.pt\n","Model_real_4.png\n","Model_real_4.pt\n","Model_real_4_pt2.png\n","Model_real_4_pt2.pt\n","Model_real_54.png\n","Model_real_54.pt\n","Model_real_56.png\n","Model_real_56.pt\n","Model_real_58.png\n","Model_real_58.pt\n","Model_real_59.png\n","Model_real_59.pt\n","Model_real_5.png\n","Model_real_5.pt\n","Model_real_60.png\n","Model_real_60.pt\n","Model_real_61.png\n","Model_real_61.pt\n","Model_real_62.png\n","Model_real_62.pt\n","Model_real_63.png\n","Model_real_63.pt\n","Model_real_64.png\n","Model_real_64.pt\n","Model_real_65.png\n","Model_real_65.pt\n","Model_real_66.png\n","Model_real_66.pt\n","Model_real_67.png\n","Model_real_67.pt\n","Model_real_68.png\n","Model_real_68.pt\n","Model_real_69_cont.png\n","Model_real_69_cont.pt\n","Model_real_69.png\n","Model_real_69.pt\n","Model_real_6.png\n","Model_real_6.pt\n","Model_real_6_pt2.png\n","Model_real_6_pt2.pt\n","Model_real_70.png\n","Model_real_70.pt\n","Model_real_7.png\n","Model_real_7.pt\n","Model_real_9.png\n","Model_real_9.pt\n","topicclass_test.csv\n","topicclass_test.txt\n","topicclass_train.csv\n","topicclass_train.txt\n","topicclass_valid.csv\n","topicclass_valid_fixed.csv\n","topicclass_valid_fixed.gsheet\n","topicclass_valid_fixed.txt\n","topicclass_valid.txt\n","Training1_NetPlot.png\n","try1_NetModel.pt\n","txtToCsv.ipynb\n","VerboseConvNetClassificationTesting_10.txt\n","VerboseConvNetClassificationTesting_11.txt\n","VerboseConvNetClassificationTesting_12.txt\n","VerboseConvNetClassificationTesting_13.txt\n","VerboseConvNetClassificationTesting_2.txt\n","VerboseConvNetClassificationTesting_3.txt\n","VerboseConvNetClassificationTesting_4.txt\n","VerboseConvNetClassificationTesting_5.txt\n","VerboseConvNetClassificationTesting_6.txt\n","VerboseConvNetClassificationTesting_7.txt\n","VerboseConvNetClassificationTesting_8.txt\n","VerboseConvNetClassificationTesting_9.txt\n","VerboseConvNetClassificationTesting_real_10.txt\n","VerboseConvNetClassificationTesting_real_11.txt\n","VerboseConvNetClassificationTesting_real_12_redo.txt\n","VerboseConvNetClassificationTesting_real_12.txt\n","VerboseConvNetClassificationTesting_real_13_redo.txt\n","VerboseConvNetClassificationTesting_real_13.txt\n","VerboseConvNetClassificationTesting_real_14_redo.txt\n","VerboseConvNetClassificationTesting_real_14.txt\n","VerboseConvNetClassificationTesting_real_15_redo.txt\n","VerboseConvNetClassificationTesting_real_15.txt\n","VerboseConvNetClassificationTesting_real_16_redo.txt\n","VerboseConvNetClassificationTesting_real_16.txt\n","VerboseConvNetClassificationTesting_real_17_redo.txt\n","VerboseConvNetClassificationTesting_real_17.txt\n","VerboseConvNetClassificationTesting_real_18.txt\n","VerboseConvNetClassificationTesting_real_19.txt\n","VerboseConvNetClassificationTesting_real_1.txt\n","VerboseConvNetClassificationTesting_real_20.txt\n","VerboseConvNetClassificationTesting_real_21.txt\n","VerboseConvNetClassificationTesting_real_22.txt\n","VerboseConvNetClassificationTesting_real_23.txt\n","VerboseConvNetClassificationTesting_real_24.txt\n","VerboseConvNetClassificationTesting_real_25.txt\n","VerboseConvNetClassificationTesting_real_26.txt\n","VerboseConvNetClassificationTesting_real_27_nonlinearity_drop.txt\n","VerboseConvNetClassificationTesting_real_27_nonlinearity.txt\n","VerboseConvNetClassificationTesting_real_27.txt\n","VerboseConvNetClassificationTesting_real_28.txt\n","VerboseConvNetClassificationTesting_real_29.txt\n","VerboseConvNetClassificationTesting_real_2.txt\n","VerboseConvNetClassificationTesting_real_2_with_nonstatic.txt\n","VerboseConvNetClassificationTesting_real_31_drop_.5.txt\n","VerboseConvNetClassificationTesting_real_31_drop.txt\n","VerboseConvNetClassificationTesting_real_31.txt\n","VerboseConvNetClassificationTesting_real_32.txt\n","VerboseConvNetClassificationTesting_real_33.txt\n","VerboseConvNetClassificationTesting_real_34.txt\n","VerboseConvNetClassificationTesting_real_35.txt\n","VerboseConvNetClassificationTesting_real_36.txt\n","VerboseConvNetClassificationTesting_real_37.txt\n","VerboseConvNetClassificationTesting_real_38.txt\n","VerboseConvNetClassificationTesting_real_39.txt\n","VerboseConvNetClassificationTesting_real_3.txt\n","VerboseConvNetClassificationTesting_real_40.txt\n","VerboseConvNetClassificationTesting_real_41.txt\n","VerboseConvNetClassificationTesting_real_42.txt\n","VerboseConvNetClassificationTesting_real_43.txt\n","VerboseConvNetClassificationTesting_real_45.txt\n","VerboseConvNetClassificationTesting_real_46.txt\n","VerboseConvNetClassificationTesting_real_47.txt\n","VerboseConvNetClassificationTesting_real_48.txt\n","VerboseConvNetClassificationTesting_real_4_pt2.txt\n","VerboseConvNetClassificationTesting_real_4.txt\n","VerboseConvNetClassificationTesting_real_50.txt\n","VerboseConvNetClassificationTesting_real_54.txt\n","VerboseConvNetClassificationTesting_real_56.txt\n","VerboseConvNetClassificationTesting_real_57.txt\n","VerboseConvNetClassificationTesting_real_58.txt\n","VerboseConvNetClassificationTesting_real_59.txt\n","VerboseConvNetClassificationTesting_real_5.txt\n","VerboseConvNetClassificationTesting_real_60.txt\n","VerboseConvNetClassificationTesting_real_61.txt\n","VerboseConvNetClassificationTesting_real_62.txt\n","VerboseConvNetClassificationTesting_real_63.txt\n","VerboseConvNetClassificationTesting_real_64.txt\n","VerboseConvNetClassificationTesting_real_65.txt\n","VerboseConvNetClassificationTesting_real_66.txt\n","VerboseConvNetClassificationTesting_real_67.txt\n","VerboseConvNetClassificationTesting_real_68.txt\n","VerboseConvNetClassificationTesting_real_69_cont.txt\n","VerboseConvNetClassificationTesting_real_69.txt\n","VerboseConvNetClassificationTesting_real_6_pt2.txt\n","VerboseConvNetClassificationTesting_real_6.txt\n","VerboseConvNetClassificationTesting_real_70_cont.txt\n","VerboseConvNetClassificationTesting_real_70.txt\n","VerboseConvNetClassificationTesting_real_7.txt\n","VerboseConvNetClassificationTesting_real_9.txt\n","VerboseConvNetClassificationTesting.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MZkErq3QwtBH","colab_type":"code","colab":{}},"source":["# ! wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S6_S8UjXw9Wl","colab_type":"code","colab":{}},"source":["#! wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G24B1SFmxfAO","colab_type":"code","colab":{}},"source":["# ! unzip crawl-300d-2M-subword.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zeEqXBSl45ke","colab_type":"code","colab":{}},"source":["# ! unzip crawl-300d-2M.vec.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cquJQfjE3W9F","colab_type":"code","colab":{}},"source":["int2Label = \\\n","{0: 'Miscellaneous',\n"," 1: 'Video games',\n"," 2: 'Language and literature',\n"," 3: 'Music',\n"," 4: 'Social sciences and society',\n"," 5: 'Sports and recreation',\n"," 6: 'Natural sciences',\n"," 7: 'Art and architecture',\n"," 8: 'History',\n"," 9: 'Warfare',\n"," 10: 'Engineering and technology',\n"," 11: 'Philosophy and religion',\n"," 12: 'Agriculture, food and drink',\n"," 13: 'Geography and places',\n"," 14: 'Mathematics',\n"," 15: 'Media and drama'}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sulAfm2L3W9J","colab_type":"code","colab":{}},"source":["np.random.seed(11747)\n","LOWER = False\n","LEARNING_RATE = 3e-4\n","N_CLASS = len(int2Label.keys())\n","MODELNO = \"ensemble_out\"\n","LOG_FILE = \"ConvNetClassificationTesting_\" + str(MODELNO) + \".txt\"\n","VERBOSE_LOG_FILE = \"VerboseConvNetClassificationTesting_\" + str(MODELNO) + \".txt\"\n","STEP_SIZE = 5\n","GAMMA = 1.0\n","NUMBER_EPOCHS = 10\n","EMBEDDING_DIM = 300"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtjvDxEI3W9M","colab_type":"code","colab":{}},"source":["f = open(LOG_FILE,\"w+\")\n","v = open(VERBOSE_LOG_FILE, \"w+\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVOqYVZQ3W9P","colab_type":"code","colab":{}},"source":["tokenizer = get_tokenizer(\"spacy\")\n","\n","TEXT = Field(sequential=True, tokenize=tokenizer, lower=LOWER, batch_first=True )\n","\n","LABEL = Field(sequential=False, use_vocab=False, batch_first=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-YDPjyO_3W9T","colab_type":"code","colab":{}},"source":["train, val, test = TabularDataset.splits(\".\", \n","                                            train = \"topicclass_train.csv\", \n","                                            validation = \"topicclass_valid_fixed.csv\", \n","                                            test = \"topicclass_test.csv\", \n","                                            format = \"csv\", \n","                                            skip_header = True,\n","                                            fields = [('label', LABEL), ('text', TEXT)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sC-QO3I5JowB","colab_type":"code","colab":{}},"source":["TEXT.build_vocab(train, val, test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PHvrNYVzN_uG","colab_type":"code","colab":{}},"source":["embedding_mtx = torch.load(\"embedding_mtx.pt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXbIf-r-N7Fh","colab_type":"code","colab":{}},"source":["# defined_vocab = set()\n","# num_lines = 2000000\n","\n","# with open(\"crawl-300d-2M.vec\") as vec_file: \n","#   for i, line in enumerate(tqdm_notebook(vec_file, total=num_lines)): \n","#     if i == 0: \n","#       pass\n","#     else: \n","#       word, vector = line.split(\" \", 1)\n","#       if word in TEXT.vocab.stoi: \n","#         embedding_mtx[TEXT.vocab.stoi.get(word)] =  torch.from_numpy(np.fromstring(vector, sep = \" \"))\n","#         defined_vocab.add(word)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvnabwCr3W9Z","colab_type":"code","outputId":"125adcb2-8140-489e-f051-c473c4fc0294","executionInfo":{"status":"ok","timestamp":1581314471440,"user_tz":300,"elapsed":130039,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"3iAfx5AD3W9c","colab_type":"code","outputId":"64b43153-44d3-4436-fee1-ba90a2c58a95","executionInfo":{"status":"ok","timestamp":1581314471441,"user_tz":300,"elapsed":128702,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print(\"device is: {}\".format(device))\n","f.write(\"device is: {}\\n\".format(device))\n","v.write(\"device is: {}\\n\".format(device))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["device is: cuda:0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["18"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"jICQYWlF3W9g","colab_type":"code","colab":{}},"source":["train_loader = Iterator.splits(   train, \n","                                  batch_size = 64, \n","                                  shuffle = True, \n","                                  sort_key = lambda x: len(x.text), \n","                                  device = device, \n","                                  )\n","\n","val_loader = Iterator(val, shuffle= False, batch_size=64, device = device )\n","test_loader = Iterator(test, shuffle= False, batch_size=64, device = device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9GcQlvBx3W9j","colab_type":"code","colab":{}},"source":["dataloaders = {'train': train_loader, 'val': val_loader}\n","dataset_sizes = {'train': len(train), 'val': len(val)}\n","\n","Metric = namedtuple('Metric', ['loss', 'train_error', 'val_error'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DColD6xm7rG","colab_type":"code","colab":{}},"source":["class ConvClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, channels_first = 64, channels_second = 64, kernels = (2,3,4,5), dropout = 0.1): \n","        super().__init__()\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","        \n","        # self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        self.bn4 = nn.BatchNorm1d(channels_first)\n","    \n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(channels_first, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","        conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2,)))\n","\n","        conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out), dim = 1)\n","        conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        pool = nn.MaxPool1d(kernel_size = conv_out.shape[2])\n","#         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","        pooled_out = pool(conv_out).squeeze(2)\n","        #print(f\"embeddings shape {embeddings.shape} and conv out is {conv_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","        pooled_out = self.dropout(pooled_out)\n","        out = self.fc(pooled_out)\n","        \n","        return out\n","        \n","        \n","        \n","class LSTMClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, hidden_size = 256, num_layers = 2, dropout = 0.1): \n","        super().__init__()\n","        self.n_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","\n","        self.rnn = nn.LSTM(input_size = self.embedding_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True, bidirectional = True)\n","        \n","        # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # self.bn4 = nn.BatchNorm1d(channels_first)\n","        \n","    \n","        self.relu = nn.ReLU()\n","        self.tanh = nn.Tanh()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_size*2, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2)))\n","        # conv5_out = self.conv5(F.pad(embeddings.transpose(1,2), (3,2)))\n","        # conv6_out = self.conv6(F.pad(embeddings.transpose(1,2), (3,3)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out, conv5_out, conv6_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        outputs, hiddens = self.rnn(embeddings)\n","        # hiddens[0] -> (num_layers * num_directions, batch, hidden_size)\n","        h_n, _ = hiddens\n","        #print(h_n.shape)\n","        #print(h_n.transpose(0,1).shape)\n","        \n","        # num_layers * num_directions, batch, hidden_size\n","        #hidden_out = h_n.view(embeddings.shape[0], self.n_layers, 2, self.hidden_size)[:, self.n_layers-1].view(embeddings.shape[0], 2*self.hidden_size)\n","        \n","        hidden_out = h_n.reshape(self.n_layers, 2, -1, self.hidden_size)\n","        #print(f\"hidden_out shape right after reshape is {hidden_out.shape}\")\n","        hidden_out = hidden_out[self.n_layers-1]\n","        #print(f\"hidden_out after indexing is {hidden_out.shape}\")\n","        hidden_out = hidden_out.transpose(0,1).reshape(-1, 2*self.hidden_size)\n","        \n","        #hidden_out = h_n.transpose(0,1).reshape(-1, self.hidden_size * 2)\n","        #print(f\"hidden_out shape is {hidden_out.shape}, h_n is {h_n.shape}\")\n","\n","#         pool = nn.MaxPool1d(kernel_size = conv_out.shape[2])\n","# #         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","#         pooled_out = pool(conv_out).squeeze(2)\n","        #print(f\"embeddings shape {embeddings.shape} and conv out is {conv_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","\n","        hidden_out = self.dropout(hidden_out)\n","\n","        out = self.fc(hidden_out)\n","        \n","        return out\n","        \n","        \n","        \n","        \n","        \n","class AttnClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, channels_first = 64, channels_second = 64, kernels = (2,3,4,5), dropout = 0.1): \n","        super().__init__()\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","        \n","        # self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # self.bn4 = nn.BatchNorm1d(channels_first)\n","\n","        self.self_attention1 = Attention(self.embedding_size)\n","        self.self_attention2 = Attention(self.embedding_size)\n","    \n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(self.embedding_size, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","\n","        attn_out = self.self_attention1(embeddings, embeddings)[0]\n","        attn_out = self.self_attention2(attn_out, attn_out)[0] + attn_out\n","\n","\n","\n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2,)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        pool = nn.MaxPool1d(kernel_size = attn_out.shape[1])\n","#         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","        pooled_out = pool(attn_out.transpose(1,2)).squeeze(2)\n","        # print(f\"embeddings shape {embeddings.shape} and attn out is {attn_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","        pooled_out = self.dropout(pooled_out)\n","        out = self.fc(pooled_out)\n","        \n","        return out\n","        \n","        \n","        \n","        \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruh-qXaLmDkq","colab_type":"code","outputId":"b107932e-af1e-4cfc-8ae5-a4d3f951b815","executionInfo":{"status":"ok","timestamp":1581314540578,"user_tz":300,"elapsed":14859,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":245}},"source":["model1 = torch.load(\"Model_real_22.pt\", map_location=torch.device('cpu'))\n","\n","model2 = torch.load(\"Model_real_29.pt\", map_location=torch.device('cpu'))\n","\n","model3 = torch.load(\"Model_real_31.pt\", map_location=torch.device('cpu'))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"XXSaDPB0nG0X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":297},"outputId":"ea55bdfe-77cd-4089-b723-456251f15d81","executionInfo":{"status":"ok","timestamp":1581313320574,"user_tz":300,"elapsed":128513,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["# pip install torch==1.2.0"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Collecting torch==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n","\u001b[K     |████████████████████████████████| 748.9MB 22kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.2.0) (1.17.5)\n","\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 1.2.0 which is incompatible.\u001b[0m\n","Installing collected packages: torch\n","  Found existing installation: torch 1.4.0\n","    Uninstalling torch-1.4.0:\n","      Successfully uninstalled torch-1.4.0\n","Successfully installed torch-1.2.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"7GkCs6Swp_Jv","colab_type":"code","outputId":"bad8f9a9-7761-4f28-dda3-628be1526e8d","executionInfo":{"status":"ok","timestamp":1581314553179,"user_tz":300,"elapsed":20863,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":329}},"source":["\n","model1.to(device)\n","model2.to(device)\n","model3.to(device)\n","model1.eval()\n","model2.eval()\n","model3.eval()"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AttnClassifier(\n","  (embedding): Embedding(137968, 300)\n","  (self_attention1): Attention(\n","    (linear_in): Linear(in_features=300, out_features=300, bias=False)\n","    (linear_out): Linear(in_features=600, out_features=300, bias=False)\n","    (softmax): Softmax(dim=-1)\n","    (tanh): Tanh()\n","  )\n","  (self_attention2): Attention(\n","    (linear_in): Linear(in_features=300, out_features=300, bias=False)\n","    (linear_out): Linear(in_features=600, out_features=300, bias=False)\n","    (softmax): Softmax(dim=-1)\n","    (tanh): Tanh()\n","  )\n","  (relu): ReLU()\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=300, out_features=16, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"ckhg0MPFm1Vx","colab_type":"code","outputId":"4a3a7c9c-2ffa-4ef1-a187-c72a33a1a680","executionInfo":{"status":"ok","timestamp":1581314584935,"user_tz":300,"elapsed":30427,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":748}},"source":["class ConvClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, channels_first = 128, channels_second = 128, kernel_first = 2, kernel_second = 2): \n","        super().__init__()\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","        \n","        #self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first, dilation = 1, kernel_size = kernel_first, padding = kernel_first - 1)\n","        self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        self.conv2 = nn.Conv1d(in_channels=channels_first, out_channels=channels_second, dilation = 1, kernel_size = kernel_second, padding = kernel_second - 1)\n","        self.bn2 = nn.BatchNorm1d(channels_second)\n","    \n","        self.relu = nn.ReLU()\n","\n","        self.dropout = nn.Dropout(0.2)\n","        \n","        self.fc = nn.Linear(channels_first, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            static_embeddings = self.embedding(texts)\n","        embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","        conv1_out = self.conv1(embeddings.transpose(1,2))\n","        conv1_out = self.bn1(conv1_out)\n","        conv1_out = self.relu(conv1_out)\n","        \n","        # conv2_out = self.conv2(conv1_out)\n","        # conv2_out = self.bn2(conv2_out)\n","        # conv2_out = self.relu(conv2_out) \n","        \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        pool = nn.MaxPool1d(kernel_size = conv1_out.shape[2])\n","#         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","        pooled_out = pool(conv1_out).squeeze(2)\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","\n","        #pooled_out = self.dropout(pooled_out)\n","        \n","        out = self.fc(pooled_out)\n","        \n","        return out\n","        \n","model4 = torch.load(\"Model_real_2.pt\", map_location=torch.device('cpu'))\n","model4.to(device)       \n","model4.eval()\n","\n","class ConvClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, channels_first = 64, channels_second = 64, kernels = (2,3,4,5,6,7), dropout = 0.1): \n","        super().__init__()\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","        \n","        # self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        self.bn4 = nn.BatchNorm1d(channels_first)\n","\n","        self.conv5 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[4])\n","        self.bn4 = nn.BatchNorm1d(channels_first)\n","\n","        self.conv6 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[5])\n","        self.bn4 = nn.BatchNorm1d(channels_first)\n","    \n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(channels_first, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","        conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2)))\n","        conv5_out = self.conv5(F.pad(embeddings.transpose(1,2), (3,2)))\n","        conv6_out = self.conv6(F.pad(embeddings.transpose(1,2), (3,3)))\n","\n","        conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out, conv5_out, conv6_out), dim = 1)\n","        conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        pool = nn.MaxPool1d(kernel_size = conv_out.shape[2])\n","#         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","        pooled_out = pool(conv_out).squeeze(2)\n","        #print(f\"embeddings shape {embeddings.shape} and conv out is {conv_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","        pooled_out = self.dropout(pooled_out)\n","        out = self.fc(pooled_out)\n","        \n","        return out\n","        \n","        \n","model5 = torch.load(\"Model_real_24.pt\", map_location=torch.device('cpu'))\n","model5.to(device)       \n","model5.eval()\n","        \n","\n","\n","class LSTMClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, hidden_size = 256, num_layers = 2, dropout = 0.1): \n","        super().__init__()\n","        self.n_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","\n","        self.rnn = nn.LSTM(input_size = self.embedding_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True, bidirectional = True)\n","        \n","        # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # self.bn4 = nn.BatchNorm1d(channels_first)\n","        \n","    \n","        self.relu = nn.ReLU()\n","        self.tanh = nn.Tanh()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_size*2, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2)))\n","        # conv5_out = self.conv5(F.pad(embeddings.transpose(1,2), (3,2)))\n","        # conv6_out = self.conv6(F.pad(embeddings.transpose(1,2), (3,3)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out, conv5_out, conv6_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        outputs, hiddens = self.rnn(embeddings)\n","        # hiddens[0] -> (num_layers * num_directions, batch, hidden_size)\n","        h_n, _ = hiddens\n","        #print(h_n.shape)\n","        #print(h_n.transpose(0,1).shape)\n","        \n","        # num_layers * num_directions, batch, hidden_size\n","        #hidden_out = h_n.view(embeddings.shape[0], self.n_layers, 2, self.hidden_size)[:, self.n_layers-1].view(embeddings.shape[0], 2*self.hidden_size)\n","        \n","        hidden_out = h_n.reshape(self.n_layers, 2, -1, self.hidden_size)\n","        #print(f\"hidden_out shape right after reshape is {hidden_out.shape}\")\n","        hidden_out = hidden_out[self.n_layers-1]\n","        #print(f\"hidden_out after indexing is {hidden_out.shape}\")\n","        hidden_out = hidden_out.transpose(0,1).reshape(-1, 2*self.hidden_size)\n","        \n","        #hidden_out = h_n.transpose(0,1).reshape(-1, self.hidden_size * 2)\n","        #print(f\"hidden_out shape is {hidden_out.shape}, h_n is {h_n.shape}\")\n","\n","#         pool = nn.MaxPool1d(kernel_size = conv_out.shape[2])\n","# #         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","#         pooled_out = pool(conv_out).squeeze(2)\n","        #print(f\"embeddings shape {embeddings.shape} and conv out is {conv_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","\n","        hidden_out = self.dropout(hidden_out)\n","\n","        out = self.fc(hidden_out)\n","        \n","        return out\n","        \n","model6 = torch.load(\"Model_real_28.pt\", map_location=torch.device('cpu'))\n","model6.to(device)       \n","model6.eval()      \n","\n","class LSTMClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, hidden_size = 256, num_layers = 2, dropout = 0.1): \n","        super().__init__()\n","        self.n_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","\n","        self.rnn = nn.LSTM(input_size = self.embedding_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True, bidirectional = True)\n","        \n","        # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # self.bn4 = nn.BatchNorm1d(channels_first)\n","        \n","    \n","        self.relu = nn.ReLU()\n","        self.tanh = nn.Tanh()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_size*2, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2)))\n","        # conv5_out = self.conv5(F.pad(embeddings.transpose(1,2), (3,2)))\n","        # conv6_out = self.conv6(F.pad(embeddings.transpose(1,2), (3,3)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out, conv5_out, conv6_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        outputs, hiddens = self.rnn(embeddings)\n","        # hiddens[0] -> (num_layers * num_directions, batch, hidden_size)\n","        h_n, _ = hiddens\n","        #print(h_n.shape)\n","        #print(h_n.transpose(0,1).shape)\n","        \n","        # num_layers * num_directions, batch, hidden_size\n","        #hidden_out = h_n.view(embeddings.shape[0], self.n_layers, 2, self.hidden_size)[:, self.n_layers-1].view(embeddings.shape[0], 2*self.hidden_size)\n","        \n","        hidden_out = h_n.reshape(self.n_layers, 2, -1, self.hidden_size)\n","        #print(f\"hidden_out shape right after reshape is {hidden_out.shape}\")\n","        hidden_out = hidden_out[self.n_layers-1]\n","        #print(f\"hidden_out after indexing is {hidden_out.shape}\")\n","        hidden_out = hidden_out.transpose(0,1).reshape(-1, 2*self.hidden_size)\n","        \n","        #hidden_out = h_n.transpose(0,1).reshape(-1, self.hidden_size * 2)\n","        #print(f\"hidden_out shape is {hidden_out.shape}, h_n is {h_n.shape}\")\n","\n","#         pool = nn.MaxPool1d(kernel_size = conv_out.shape[2])\n","# #         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","#         pooled_out = pool(conv_out).squeeze(2)\n","        #print(f\"embeddings shape {embeddings.shape} and conv out is {conv_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","\n","        hidden_out = self.dropout(hidden_out)\n","\n","        out = self.fc(hidden_out)\n","        \n","        return out\n","        \n","model7 = torch.load(\"Model_real_29.pt\", map_location=torch.device('cpu'))\n","model7.to(device)       \n","model7.eval()      \n","                \n","\n","class AttnClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, channels_first = 64, channels_second = 64, kernels = (2,3,4,5), dropout = 0.1): \n","        super().__init__()\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","        \n","        # self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # self.bn4 = nn.BatchNorm1d(channels_first)\n","\n","        self.self_attention = Attention(self.embedding_size)\n","    \n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(self.embedding_size, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","\n","        attn_out, _ = self.self_attention(embeddings, embeddings)\n","\n","\n","\n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2,)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        pool = nn.MaxPool1d(kernel_size = attn_out.shape[1])\n","#         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","        pooled_out = pool(attn_out.transpose(1,2)).squeeze(2)\n","        # print(f\"embeddings shape {embeddings.shape} and attn out is {attn_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","        pooled_out = self.dropout(pooled_out)\n","        out = self.fc(pooled_out)\n","        \n","        return out\n","\n","        \n","model8 = torch.load(\"Model_real_32.pt\", map_location=torch.device('cpu'))\n","model8.to(device)       \n","model8.eval()                     \n","\n","class AttnClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, channels_first = 64, channels_second = 64, kernels = (2,3,4,5), dropout = 0.1): \n","        super().__init__()\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","        \n","        # self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # self.bn4 = nn.BatchNorm1d(channels_first)\n","\n","        self.self_attention1 = Attention(self.embedding_size)\n","        self.self_attention2 = Attention(self.embedding_size)\n","        self.self_attention_query = Attention(self.embedding_size)\n","\n","\n","        self.softmax = nn.Softmax(dim = 1)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(self.embedding_size, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","\n","        attn_out = self.self_attention1(embeddings, embeddings)[0]\n","        attn_out = self.self_attention2(attn_out, attn_out)[0] + attn_out\n","\n","        attn_query = self.self_attention_query(attn_out, attn_out)[0]\n","        \n","        raw_energy = torch.mul(attn_query, attn_out).sum(axis = 2)\n","        normalized_energy = self.softmax(raw_energy).reshape(embeddings.shape[0], 1, -1)\n","        \n","        attn_contexts = torch.bmm(normalized_energy, attn_out).reshape(embeddings.shape[0], self.embedding_size)\n","    \n","\n","\n","\n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2,)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        # pool = nn.MaxPool1d(kernel_size = attn_out.shape[1])\n","\n","#         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","        # pooled_out = pool(attn_out.transpose(1,2)).squeeze(2)\n","        # print(f\"embeddings shape {embeddings.shape} and attn out is {attn_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","        pooled_out = self.dropout(attn_contexts)\n","        out = self.fc(pooled_out)\n","        \n","        return out\n","\n","        \n","model9 = torch.load(\"Model_real_38.pt\", map_location=torch.device('cpu'))\n","model9.to(device)       \n","model9.eval()             \n","        "],"execution_count":18,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Softmax' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Tanh' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["AttnClassifier(\n","  (embedding): Embedding(137968, 300)\n","  (self_attention1): Attention(\n","    (linear_in): Linear(in_features=300, out_features=300, bias=False)\n","    (linear_out): Linear(in_features=600, out_features=300, bias=False)\n","    (softmax): Softmax(dim=-1)\n","    (tanh): Tanh()\n","  )\n","  (self_attention2): Attention(\n","    (linear_in): Linear(in_features=300, out_features=300, bias=False)\n","    (linear_out): Linear(in_features=600, out_features=300, bias=False)\n","    (softmax): Softmax(dim=-1)\n","    (tanh): Tanh()\n","  )\n","  (self_attention_query): Attention(\n","    (linear_in): Linear(in_features=300, out_features=300, bias=False)\n","    (linear_out): Linear(in_features=600, out_features=300, bias=False)\n","    (softmax): Softmax(dim=-1)\n","    (tanh): Tanh()\n","  )\n","  (softmax): Softmax(dim=1)\n","  (relu): ReLU()\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc): Linear(in_features=300, out_features=16, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"gFzqsPUMtwEw","colab_type":"code","colab":{}},"source":["device"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SRYgdJaspGPB","colab_type":"code","outputId":"4dcacc4a-cc69-4341-9d20-ef4e3dfbb0bd","executionInfo":{"status":"error","timestamp":1581312915361,"user_tz":300,"elapsed":656,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":166}},"source":["print(model5)"],"execution_count":26,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-82e3e97b0bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model5' is not defined"]}]},{"cell_type":"code","metadata":{"id":"LNGvffDT3W9m","colab_type":"code","colab":{}},"source":["class EnsembleClassifier(nn.Module): \n","    def __init__(self, models, n_class, dropout = 0.5): \n","        super().__init__()\n","        # self.vocab_size, self.embedding_size = embeddings.shape\n","        # self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        # self.embedding.weight.data.copy_(embeddings)\n","        \n","        # # self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        # # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # # self.bn4 = nn.BatchNorm1d(channels_first)\n","\n","        # self.self_attention1 = Attention(self.embedding_size)\n","        # self.self_attention2 = Attention(self.embedding_size)\n","        # self.self_attention_query = Attention(self.embedding_size)\n","\n","\n","        # self.softmax = nn.Softmax(dim = 1)\n","        # self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.tanh = nn.Tanh()\n","\n","        self.model1 = models[0]\n","        self.model2 = models[1]\n","        self.model3 = models[2]\n","        self.model4 = models[3]\n","        self.model5 = models[4]\n","        self.model6 = models[5]\n","        self.model7 = models[6]\n","        self.model8 = models[7]\n","        self.model9 = models[8]\n","\n","        \n","        self.fc = nn.Linear(n_class*3, n_class)\n","        self.softmax = nn.Softmax(dim = 1)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            # model1_out = 1/9 * self.softmax(self.model1(texts))\n","            # model2_out = 1/9 * self.softmax(self.model2(texts))\n","            # model3_out = 1/9 * self.softmax(self.model3(texts))\n","            # model4_out = 1/9 * self.softmax(self.model4(texts))\n","            # model5_out = 1/9 * self.softmax(self.model5(texts))\n","            # model6_out = 1/9 * self.softmax(self.model6(texts))\n","            # model7_out = 1/9 * self.softmax(self.model7(texts))\n","            # model8_out = 1/9 * self.softmax(self.model8(texts))\n","            # model9_out = 1/9 * self.softmax(self.model9(texts))\n","\n","            model1_out = 1/9 * torch.log((self.softmax(self.model1(texts))))\n","            model2_out = 1/9 * torch.log(self.softmax(self.model2(texts)))\n","            model3_out = 1/9 * torch.log(self.softmax(self.model3(texts)))\n","            model4_out = 1/9 * torch.log(self.softmax(self.model4(texts)))\n","            model5_out = 1/9 * torch.log(self.softmax(self.model5(texts)))\n","            model6_out = 1/9 * torch.log(self.softmax(self.model6(texts)))\n","            model7_out = 1/9 * torch.log(self.softmax(self.model7(texts)))\n","            model8_out = 1/9 * torch.log(self.softmax(self.model8(texts)))\n","            model9_out = 1/9 * torch.log(self.softmax(self.model9(texts)))\n","            \n","            out = model1_out+model2_out+model3_out+model4_out+model5_out+model6_out+model7_out+model8_out+model9_out\n","            out = self.softmax(out)\n","\n","\n","            # stacked_out = torch.stack([model1_out, model2_out, model3_out])\n","\n","            \n","            # out = torch.mean(stacked_out, dim=0)\n","            #stacked_out = self.dropout(self.tanh(stacked_out))\n","            \n","        #out = self.fc(stacked_out)\n","\n","        # attn_query = self.self_attention_query(attn_out, attn_out)[0]\n","\n","        \n","        # raw_energy = torch.mul(attn_query, attn_out).sum(2)\n","        # normalized_energy = self.softmax(raw_energy).reshape(embeddings.shape[0], 1, -1)\n","        \n","        # attn_contexts = torch.bmm(normalized_energy, attn_out).reshape(embeddings.shape[0], self.embedding_size)\n","    \n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2,)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","#         pool = nn.MaxPool1d(kernel_size = attn_out.shape[1])\n","\n","# #         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","#         pooled_out = pool(attn_out.transpose(1,2)).squeeze(2)\n","#         # print(f\"embeddings shape {embeddings.shape} and attn out is {attn_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","# #         print(f\"pooled out shape is {pooled_out.shape}\")\n","        \n","        return out\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OyoVNg0WK_Pe","colab_type":"code","colab":{}},"source":["def init_weights(m):\n","    if type(m) == nn.Conv1d or type(m) == nn.Linear:\n","        nn.init.xavier_normal_(m.weight.data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlEnuEjy3W9p","colab_type":"code","colab":{}},"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25, lambda_reg = .0001):\n","\n","    since = time.time()\n","    \n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        f.write('Epoch {}/{}\\n'.format(epoch+1, num_epochs))\n","        v.write('Epoch {}/{}\\n'.format(epoch+1, num_epochs))\n","        f.write('-' *10)\n","        v.write('-' * 10)\n","\n","        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0.0\n","\n","            # Iterate over data.\n","            f.write(\"\\nstarting epoch {} for {} phase\\n\".format(epoch+1, phase))\n","            v.write(\"\\nstarting epoch {} for {} phase\\n\".format(epoch+1, phase))\n","            print(\"starting epoch {} for {} phase\".format(epoch+1, phase))\n","\n","            for i, data in enumerate(tqdm_notebook(dataloaders[phase])):\n","                #pdb.set_trace()\n","                inputs = data.text.to(device)\n","                labels = data.label.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels) \n","                    # for i, param in enumerate(model.parameters()): \n","                    #   if i == 1: \n","                    #     loss += lambda_reg * torch.norm(param)\n","\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","                if ((i%300) == 0):\n","                    #print(\"inside\")\n","                    v.write(\"inputs size: {}\\n\".format(inputs.size(0)))\n","                    v.write(\"epoch {}, batch {},  loss : {}\\n\".format(epoch+1, i, loss.item()))\n","                    v.write(\"percent correct: {}\\n\".format((torch.sum(preds == labels.data)/inputs.size(0))))\n","            \n","            \n","            epoch_loss = running_loss / dataset_sizes[phase]\n","\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            if phase == 'train':\n","\n","                train_loss = epoch_loss\n","                train_error = 1 - epoch_acc\n","                scheduler.step()\n","\n","            elif phase == 'val': \n","\n","                val_error = 1 - epoch_acc\n","                metrics.append(Metric(loss=train_loss, train_error=train_error,val_error=val_error))\n","            \n","            f.write('{} Loss: {:.4f} Acc: {:.4f}\\n'.format(phase, epoch_loss, epoch_acc))\n","            v.write('{} Loss: {:.4f} Acc: {:.4f}\\n'.format(phase, epoch_loss, epoch_acc))\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                PATH = 'MobileNetModel_' + str(epoch+2) + '.pt'\n","                torch.save(model, PATH)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    f.write('Training complete in {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n","    v.write('Training complete in {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n","\n","\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    f.write('Best val Acc: {:4f}'.format(best_acc))\n","    v.write('Best val Acc: {:4f}'.format(best_acc))\n","\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u_85BRlFtFm5","colab_type":"code","colab":{}},"source":["# for named_param in model.named_parameters(): \n","#   print(named_param)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQSVyWye3W9s","colab_type":"code","colab":{}},"source":["model = EnsembleClassifier((model1, model2, model3, model4, model5, model6, model7, model8, model9), N_CLASS, dropout=0)\n","# model = EnsembleClassifier((model1, model2, model3), N_CLASS, dropout=0)\n","model = model.to(device)\n","#model.apply(init_weights)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ogDkGH3K3W9w","colab_type":"code","outputId":"bcf91ddf-7648-499e-870e-aa23cd404ffc","executionInfo":{"status":"error","timestamp":1580704402825,"user_tz":300,"elapsed":106,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":236}},"source":["metrics = []\n","model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=NUMBER_EPOCHS, lambda_reg = 0)\n","\n","f.close()\n","v.close()\n","torch.save(model, \"Model_\" + str(MODELNO) + \".pt\")\n","\n","def training_plot(metrics):\n","    plt.figure(1)\n","    plt.plot([m.val_error for m in metrics], 'b')\n","    plt.plot([m.train_error for m in metrics], 'r')\n","    plt.title('Train Error (red), Val Error (blue)')\n","    plt.savefig('Model_' + str(MODELNO) + '.png')\n"," \n","training_plot(metrics)\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-c8f8287fde7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUMBER_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"KN5bLsQF_J7m","colab_type":"code","outputId":"ddd15ba3-4823-4f5e-edce-aec256189ee1","executionInfo":{"status":"ok","timestamp":1581314608644,"user_tz":300,"elapsed":1741,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["354d7490533e4699ad0e5e78b8bb136c","29ef72741ffc45a08bcbf7f060dba35f","e0a1c3f3cd74497f9b46c9b47c4cc063","5a6ee4d4514a4cbf9ec323cf071c833f","aa0e47d4ab79416cb00bfb14c1f4048f","21c3c8999af6461883497b1efd8a8c01","b8b91c685b454534ad9a57cd6417dcfb","15ab1d8a74e74336abc7cac1f75cfd93"]}},"source":["running_loss = 0.0\n","running_corrects = 0\n","model.eval()\n","model.cuda()\n","pred_list = []\n","for i, data in enumerate(tqdm_notebook(val_loader)):\n","  #pdb.set_trace()\n","  inputs = data.text.to(device)\n","  labels = data.label.to(device)\n","\n","  # zero the parameter gradients\n","  optimizer.zero_grad()\n","\n","  # forward\n","  # track history if only in train\n","  \n","  with torch.no_grad():\n","      outputs = model(inputs)\n","      #print(outputs.shape)\n","\n","      _, preds = torch.max(outputs, 1)\n","      pred_list.extend(preds.cpu().tolist())\n","      loss = criterion(outputs, labels) \n","      # for i, param in enumerate(model.parameters()): \n","      #   if i == 1: \n","      #     loss += lambda_reg * torch.norm(param)\n","\n","\n","  # statistics\n","  running_loss += loss.item() * inputs.size(0)\n","  running_corrects += torch.sum(preds == labels.data)\n","\n","\n","epoch_loss = running_loss / len(val)\n","\n","epoch_acc = running_corrects.double() / len(val)\n","print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"354d7490533e4699ad0e5e78b8bb136c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Loss: 2.0697 Acc: 0.8849\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6tep2KcZpCSX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"07197332-f0aa-4c7a-cbf8-e0c343efa7ac","executionInfo":{"status":"ok","timestamp":1581314611257,"user_tz":300,"elapsed":423,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["val_preds = [int2Label.get(pred) for pred in pred_list]\n","val_preds[:10]"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Sports and recreation',\n"," 'Sports and recreation',\n"," 'Sports and recreation',\n"," 'Media and drama',\n"," 'Social sciences and society',\n"," 'Music',\n"," 'Music',\n"," 'Media and drama',\n"," 'Social sciences and society',\n"," 'History']"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"w5BQmF45p5we","colab_type":"code","colab":{}},"source":["import pandas as pd\n","valid = pd.read_csv(\"topicclass_valid_fixed.csv\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKWjlcMFq9_U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d943b8a1-d5c1-4d1a-d517-424dda342473","executionInfo":{"status":"ok","timestamp":1581314616567,"user_tz":300,"elapsed":517,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["pred_list[:10]"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[5, 5, 5, 15, 4, 3, 3, 15, 4, 8]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"Qh-h7EA_sq0i","colab_type":"code","colab":{}},"source":["answers = []\n","with open(\"topicclass_valid_fixed.txt\") as f: \n","  for line in f: \n","    answers.append(line.split(\" ||| \")[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"htFF-6Nds7An","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"bcc48ab7-3e40-4048-deb8-482e304c9fa8","executionInfo":{"status":"ok","timestamp":1581314722121,"user_tz":300,"elapsed":546,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["answers[:10]"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Sports and recreation',\n"," 'Sports and recreation',\n"," 'Sports and recreation',\n"," 'Media and drama',\n"," 'Music',\n"," 'Music',\n"," 'Music',\n"," 'Media and drama',\n"," 'Social sciences and society',\n"," 'History']"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"-r-KeqPgtBfh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"67b94c64-afa6-4eba-cb20-952b63fe7443","executionInfo":{"status":"ok","timestamp":1581314815881,"user_tz":300,"elapsed":440,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["total_number = len(answers)\n","total_correct = 0\n","for pair in zip(val_preds, answers): \n","  if pair[0] == pair[1]: \n","    total_correct+=1\n","total_correct / total_number"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8849144634525661"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"SW2HmKspuKhV","colab_type":"code","colab":{}},"source":["with open('dev_results.txt', 'w') as f:\n","    for pred in val_preds:\n","        f.write(\"%s\\n\" % pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_k3E5P5euRsx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"64b2875e-9877-4b0d-dc70-bb5eac076cc6","executionInfo":{"status":"ok","timestamp":1581315085948,"user_tz":300,"elapsed":2175,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["! head dev_results.txt"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Sports and recreation\n","Sports and recreation\n","Sports and recreation\n","Media and drama\n","Social sciences and society\n","Music\n","Music\n","Media and drama\n","Social sciences and society\n","History\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N1NKEd9CucIz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["d15c149ffa1348cf9308e643ea88e47e","1265eae7fc8047ea985378f6fd21872d","a7c1539b1d2147308be1deb5552d5239","0ef650cf94f04412a57adb2867421307","7640076764c2455e87cf796c06e19ed2","6ae5cec64f5d44098b8fdc12733ae7e2","460f40db2bb64030bdb5d860ca3e266c","fc5eb4f88c774a5aa3a2e8facac7901e"]},"outputId":"5bda4b6f-78ee-4da9-d0e3-b544212840cf","executionInfo":{"status":"ok","timestamp":1581315184310,"user_tz":300,"elapsed":1158,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["pred_list = []\n","for i, data in enumerate(tqdm_notebook(test_loader)):\n","  #pdb.set_trace()\n","  inputs = data.text.to(device)\n","  \n","  with torch.no_grad():\n","      outputs = model(inputs)\n","      #print(outputs.shape)\n","\n","      _, preds = torch.max(outputs, 1)\n","      pred_list.extend(preds.cpu().tolist())\n","      # for i, param in enumerate(model.parameters()): \n","      #   if i == 1: \n","      #     loss += lambda_reg * torch.norm(param)\n","\n","\n","  # statistics"],"execution_count":38,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d15c149ffa1348cf9308e643ea88e47e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qCIwb0P0ut2T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":277},"outputId":"7861ec9f-9cb7-45cc-b12c-048353ca261b","executionInfo":{"status":"ok","timestamp":1581315383449,"user_tz":300,"elapsed":539,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["test_preds = [int2Label.get(pred) for pred in pred_list]\n","test_preds[:15]"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Engineering and technology',\n"," 'Media and drama',\n"," 'Sports and recreation',\n"," 'Sports and recreation',\n"," 'History',\n"," 'History',\n"," 'Music',\n"," 'Music',\n"," 'Warfare',\n"," 'Geography and places',\n"," 'History',\n"," 'Media and drama',\n"," 'Philosophy and religion',\n"," 'Art and architecture',\n"," 'Media and drama']"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"vRcs42RZuz7a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":297},"outputId":"244b44ee-c790-4ded-f2c1-5f594bc98452","executionInfo":{"status":"ok","timestamp":1581315385427,"user_tz":300,"elapsed":2419,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["! head -n 15 topicclass_test.txt"],"execution_count":43,"outputs":[{"output_type":"stream","text":["UNK ||| NY 93 was moved onto NY 104 and Junction Road in Cambria in the 1940s , and altered to bypass Lockport to the south on a new highway and Robinson and Dysinger roads in 1991 .\n","UNK ||| It was also staged in Hartford , Connecticut in the United States in 1983 and starred John Cullum as Hitler .\n","UNK ||| In 2008 , Dodd was the Australian national Grade IV para @-@ equestrian champion .\n","UNK ||| He has headlined numerous pay @-@ per @-@ view events for both the WWE and UFC , including WrestleMania XIX , WrestleMania 31 , UFC 100 , and UFC 116 .\n","UNK ||| Nerva became Emperor at the age of sixty @-@ five , after a lifetime of imperial service under Nero and the rulers of the Flavian dynasty .\n","UNK ||| Maynilà had been Indianized since the sixth century CE and earlier .\n","UNK ||| The single peaked at number 46 on the US Billboard Hot 100 and has been certified gold by the Recording Industry Association of America ( RIAA ) for shipments of 500 @,@ 000 copies .\n","UNK ||| For Independiente , Arjona returns to his trademark sound after his stylistic departure for Poquita Ropa ( 2010 ) .\n","UNK ||| The next year its elements took part in the Battle of Kupres and Operation Tiger aimed at lifting the Siege of Dubrovnik .\n","UNK ||| Bintulu remained a fishing village until 1969 when oil and gas reserves were discovered off the coast .\n","UNK ||| Upon his death he was succeeded and deified by Trajan .\n","UNK ||| The film was released by Pathé on October 7 , 1917 .\n","UNK ||| In the early 1990s conflict between feminist and traditional members led to the latter leaving Beth Israel , and forming the Orthodox Congregation Ahavas Torah .\n","UNK ||| There is no existing Catalogue raisonné of Josepha Petrick 's artworks , to date , no fakes have been cited .\n","UNK ||| This gives the effect of a man trapped and suffocated by his surroundings , screaming into an airless void .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j4ywjwyQqKcP","colab_type":"code","colab":{}},"source":["with open('test_results.txt', 'w') as f:\n","    for pred in test_preds:\n","        f.write(\"%s\\n\" % pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRvNSDBMv33D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9af6fd91-5abc-4ccf-ed3c-7b0256390cd9","executionInfo":{"status":"ok","timestamp":1581315506261,"user_tz":300,"elapsed":2578,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["! pwd"],"execution_count":46,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/NN4NLP/topicclass\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ow8aCHEXv6FK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQCrYT2fyx8V","colab_type":"code","colab":{}},"source":["int2Label = \\\n","{0: 'Miscellaneous',\n"," 1: 'Video games',\n"," 2: 'Language and literature',\n"," 3: 'Music',\n"," 4: 'Social sciences and society',\n"," 5: 'Sports and recreation',\n"," 6: 'Natural sciences',\n"," 7: 'Art and architecture',\n"," 8: 'History',\n"," 9: 'Warfare',\n"," 10: 'Engineering and technology',\n"," 11: 'Philosophy and religion',\n"," 12: 'Agriculture, food and drink',\n"," 13: 'Geography and places',\n"," 14: 'Mathematics',\n"," 15: 'Media and drama'}"],"execution_count":0,"outputs":[]}]}