{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Ensemble Classifier52.ipynb","provenance":[{"file_id":"10jm5EOsWZhZbZxu5R66cVbVyy63vqY2C","timestamp":1580708133931},{"file_id":"1kXcaja-5l-ZJIDVlqeNbZzIw2hhmvJpI","timestamp":1580704834858},{"file_id":"19rjk07FZiwdR2vcUu4XotoojR9h4jW4N","timestamp":1580703764562},{"file_id":"1G3srQDfp_pUg8O-t0nB8VCrN_RbBnXAV","timestamp":1580614625857},{"file_id":"1ofx6a6dasWAhK3T0TH9JZMWCQ_2rfzDj","timestamp":1580273350581},{"file_id":"1ED5e4ix1fNMHWEL5dgGJ_O8K6bfTAcuk","timestamp":1580267148468},{"file_id":"1huuuSmGDlDTPMlbFHAX27vof7qF4BeLN","timestamp":1580266605400},{"file_id":"1TZiMiTWcGHWRCTSKigbYgqtz3FCRmC2W","timestamp":1580263420924},{"file_id":"1pAUwp3i142c7G1uJmPOCiACW6Si0ezor","timestamp":1580262006654},{"file_id":"1ufq2JpnXay_JsRo4vhhIGSUvjRw5er7O","timestamp":1580232037354},{"file_id":"1uIeQCgKK4IzV6nCuridHnIOjGxlEtQ8f","timestamp":1579903800063},{"file_id":"1pMsQwNuwLvpMdHY_Vj9c467qvQCMAyJi","timestamp":1579901507622}]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"865e31f304284ed4a3c24e34ab6e6de4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6b36a64f64b14738a2336802016da3b6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7a80dc295bef4611bba36c442578b738","IPY_MODEL_e0ef0cd4ef654d058036fa2f705902cb"]}},"6b36a64f64b14738a2336802016da3b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7a80dc295bef4611bba36c442578b738":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_53d408c7a4844e8e9808fcc96f9e299d","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":11,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":11,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_82d91769a2364fe8849bc7700e39bc7f"}},"e0ef0cd4ef654d058036fa2f705902cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4f6c9ecad8f443ca9d0653a44b98d4ba","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 11/11 [00:00&lt;00:00, 88.23it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b8dee04607614627a2c6a5f28370cc5b"}},"53d408c7a4844e8e9808fcc96f9e299d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"82d91769a2364fe8849bc7700e39bc7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f6c9ecad8f443ca9d0653a44b98d4ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b8dee04607614627a2c6a5f28370cc5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"SsKE9VwfucfE","colab_type":"code","outputId":"4dc16c24-c63c-4575-ca9f-61a70f419896","executionInfo":{"status":"ok","timestamp":1580708199486,"user_tz":300,"elapsed":4531,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["! pip install pytorch-nlp"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pytorch-nlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/51/f0ee1efb75f7cc2e3065c5da1363d6be2eec79691b2821594f3f2329528c/pytorch_nlp-0.5.0-py3-none-any.whl (90kB)\n","\r\u001b[K     |███▋                            | 10kB 23.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 51kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 61kB 3.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 71kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 81kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 3.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.17.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.28.1)\n","Installing collected packages: pytorch-nlp\n","Successfully installed pytorch-nlp-0.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wb2bLnjL3W88","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import torchtext\n","\n","from torchtext.data import TabularDataset, Field, Iterator\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import Vocab\n","from torch import optim \n","from torch.optim import lr_scheduler\n","import matplotlib.pyplot as plt\n","from torch.nn import functional as F\n","from torchnlp.nn import Attention\n","\n","import copy\n","import time\n","from collections import namedtuple\n","from tqdm import tqdm, tqdm_notebook"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rz8AqPnI3kNO","colab_type":"code","outputId":"bd35482b-15f2-4915-b019-fae1b476f624","executionInfo":{"status":"ok","timestamp":1580708226644,"user_tz":300,"elapsed":19492,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fxKQyqa438P0","colab_type":"code","outputId":"973706b0-dd16-47b4-932d-ec928faa3ece","executionInfo":{"status":"ok","timestamp":1580708227342,"user_tz":300,"elapsed":470,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd drive/\"My Drive\"/NN4NLP/topicclass"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/NN4NLP/topicclass\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X-nQhfuF4KOa","colab_type":"code","outputId":"0f4c1abf-ef7d-485b-e6c5-a65131a1e029","executionInfo":{"status":"ok","timestamp":1580708229991,"user_tz":300,"elapsed":2951,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["! ls"],"execution_count":5,"outputs":[{"output_type":"stream","text":["bestMobile_v2_NetModel.pt\n","Classifier.ipynb\n","ConvNetClassificationTesting_10.txt\n","ConvNetClassificationTesting_11.txt\n","ConvNetClassificationTesting_12.txt\n","ConvNetClassificationTesting_13.txt\n","ConvNetClassificationTesting_2.txt\n","ConvNetClassificationTesting_3.txt\n","ConvNetClassificationTesting_4.txt\n","ConvNetClassificationTesting_5.txt\n","ConvNetClassificationTesting_6.txt\n","ConvNetClassificationTesting_7.txt\n","ConvNetClassificationTesting_8.txt\n","ConvNetClassificationTesting_9.txt\n","ConvNetClassificationTesting_real_10.txt\n","ConvNetClassificationTesting_real_11.txt\n","ConvNetClassificationTesting_real_12_redo.txt\n","ConvNetClassificationTesting_real_12.txt\n","ConvNetClassificationTesting_real_13_redo.txt\n","ConvNetClassificationTesting_real_13.txt\n","ConvNetClassificationTesting_real_14_redo.txt\n","ConvNetClassificationTesting_real_14.txt\n","ConvNetClassificationTesting_real_15_redo.txt\n","ConvNetClassificationTesting_real_15.txt\n","ConvNetClassificationTesting_real_16_redo.txt\n","ConvNetClassificationTesting_real_16.txt\n","ConvNetClassificationTesting_real_17_redo.txt\n","ConvNetClassificationTesting_real_17.txt\n","ConvNetClassificationTesting_real_18.txt\n","ConvNetClassificationTesting_real_19.txt\n","ConvNetClassificationTesting_real_1.txt\n","ConvNetClassificationTesting_real_20.txt\n","ConvNetClassificationTesting_real_21.txt\n","ConvNetClassificationTesting_real_22.txt\n","ConvNetClassificationTesting_real_23.txt\n","ConvNetClassificationTesting_real_24.txt\n","ConvNetClassificationTesting_real_25.txt\n","ConvNetClassificationTesting_real_26.txt\n","ConvNetClassificationTesting_real_27.txt\n","ConvNetClassificationTesting_real_28.txt\n","ConvNetClassificationTesting_real_29.txt\n","ConvNetClassificationTesting_real_2.txt\n","ConvNetClassificationTesting_real_31.txt\n","ConvNetClassificationTesting_real_32.txt\n","ConvNetClassificationTesting_real_33.txt\n","ConvNetClassificationTesting_real_34.txt\n","ConvNetClassificationTesting_real_35.txt\n","ConvNetClassificationTesting_real_36.txt\n","ConvNetClassificationTesting_real_37.txt\n","ConvNetClassificationTesting_real_38.txt\n","ConvNetClassificationTesting_real_39.txt\n","ConvNetClassificationTesting_real_3.txt\n","ConvNetClassificationTesting_real_40.txt\n","ConvNetClassificationTesting_real_41.txt\n","ConvNetClassificationTesting_real_42.txt\n","ConvNetClassificationTesting_real_43.txt\n","ConvNetClassificationTesting_real_45.txt\n","ConvNetClassificationTesting_real_46.txt\n","ConvNetClassificationTesting_real_47.txt\n","ConvNetClassificationTesting_real_48.txt\n","ConvNetClassificationTesting_real_4_pt2.txt\n","ConvNetClassificationTesting_real_4.txt\n","ConvNetClassificationTesting_real_5.txt\n","ConvNetClassificationTesting_real_6_pt2.txt\n","ConvNetClassificationTesting_real_6.txt\n","ConvNetClassificationTesting_real_7.txt\n","ConvNetClassificationTesting_real_9.txt\n","ConvNetClassificationTesting.txt\n","crawl-300d-2M-subword.bin\n","crawl-300d-2M-subword.vec\n","crawl-300d-2M-subword.zip\n","crawl-300d-2M.vec\n","crawl-300d-2M.vec.zip\n","embedding_mtx.pt\n","int2Class.csv\n","MobileNetModel_10.pt\n","MobileNetModel_11.pt\n","MobileNetModel_2.pt\n","MobileNetModel_3.pt\n","MobileNetModel_4.pt\n","MobileNetModel_5.pt\n","MobileNetModel_6.pt\n","MobileNetModel_7.pt\n","MobileNetModel_8.pt\n","MobileNetModel_9.pt\n","Model_10.png\n","Model_10.pt\n","Model_11.png\n","Model_11.pt\n","Model_12.png\n","Model_12.pt\n","Model2.png\n","Model2.pt\n","Model_3.png\n","Model_3.pt\n","Model_4.png\n","Model_4.pt\n","Model_5.png\n","Model_5.pt\n","Model_6.png\n","Model_6.pt\n","Model_7.png\n","Model_7.pt\n","Model_8.png\n","Model_8.pt\n","Model_9.png\n","Model_9.pt\n","Model_real_10.png\n","Model_real_10.pt\n","Model_real_11.png\n","Model_real_11.pt\n","Model_real_12.png\n","Model_real_12.pt\n","Model_real_12_redo.png\n","Model_real_12_redo.pt\n","Model_real_13.png\n","Model_real_13.pt\n","Model_real_13_redo.png\n","Model_real_13_redo.pt\n","Model_real_14.png\n","Model_real_14.pt\n","Model_real_14_redo.png\n","Model_real_14_redo.pt\n","Model_real_15.png\n","Model_real_15.pt\n","Model_real_15_redo.png\n","Model_real_15_redo.pt\n","Model_real_16.png\n","Model_real_16.pt\n","Model_real_16_redo.png\n","Model_real_16_redo.pt\n","Model_real_17.png\n","Model_real_17.pt\n","Model_real_17_redo.png\n","Model_real_17_redo.pt\n","Model_real_18.png\n","Model_real_18.pt\n","Model_real_19.png\n","Model_real_19.pt\n","Model_real_1.png\n","Model_real_1.pt\n","Model_real_20.png\n","Model_real_20.pt\n","Model_real_21.png\n","Model_real_21.pt\n","Model_real_22.png\n","Model_real_22.pt\n","Model_real_23.png\n","Model_real_23.pt\n","Model_real_24.png\n","Model_real_24.pt\n","Model_real_25.png\n","Model_real_25.pt\n","Model_real_26.png\n","Model_real_26.pt\n","Model_real_27.png\n","Model_real_27.pt\n","Model_real_28.png\n","Model_real_28.pt\n","Model_real_29.png\n","Model_real_29.pt\n","Model_real_2.png\n","Model_real_2.pt\n","Model_real_31.png\n","Model_real_31.pt\n","Model_real_32.png\n","Model_real_32.pt\n","Model_real_33.png\n","Model_real_33.pt\n","Model_real_34.png\n","Model_real_34.pt\n","Model_real_35.png\n","Model_real_35.pt\n","Model_real_36.png\n","Model_real_36.pt\n","Model_real_37.png\n","Model_real_37.pt\n","Model_real_38.png\n","Model_real_38.pt\n","Model_real_39.png\n","Model_real_39.pt\n","Model_real_3.png\n","Model_real_3.pt\n","Model_real_40.png\n","Model_real_40.pt\n","Model_real_41.png\n","Model_real_41.pt\n","Model_real_42.png\n","Model_real_42.pt\n","Model_real_43.png\n","Model_real_43.pt\n","Model_real_45.png\n","Model_real_45.pt\n","Model_real_46.png\n","Model_real_46.pt\n","Model_real_47.png\n","Model_real_47.pt\n","Model_real_4.png\n","Model_real_4.pt\n","Model_real_4_pt2.png\n","Model_real_4_pt2.pt\n","Model_real_5.png\n","Model_real_5.pt\n","Model_real_6.png\n","Model_real_6.pt\n","Model_real_6_pt2.png\n","Model_real_6_pt2.pt\n","Model_real_7.png\n","Model_real_7.pt\n","Model_real_9.png\n","Model_real_9.pt\n","topicclass_test.csv\n","topicclass_test.txt\n","topicclass_train.csv\n","topicclass_train.txt\n","topicclass_valid.csv\n","topicclass_valid_fixed.csv\n","topicclass_valid_fixed.gsheet\n","topicclass_valid_fixed.txt\n","topicclass_valid.txt\n","Training1_NetPlot.png\n","try1_NetModel.pt\n","txtToCsv.ipynb\n","VerboseConvNetClassificationTesting_10.txt\n","VerboseConvNetClassificationTesting_11.txt\n","VerboseConvNetClassificationTesting_12.txt\n","VerboseConvNetClassificationTesting_13.txt\n","VerboseConvNetClassificationTesting_2.txt\n","VerboseConvNetClassificationTesting_3.txt\n","VerboseConvNetClassificationTesting_4.txt\n","VerboseConvNetClassificationTesting_5.txt\n","VerboseConvNetClassificationTesting_6.txt\n","VerboseConvNetClassificationTesting_7.txt\n","VerboseConvNetClassificationTesting_8.txt\n","VerboseConvNetClassificationTesting_9.txt\n","VerboseConvNetClassificationTesting_real_10.txt\n","VerboseConvNetClassificationTesting_real_11.txt\n","VerboseConvNetClassificationTesting_real_12_redo.txt\n","VerboseConvNetClassificationTesting_real_12.txt\n","VerboseConvNetClassificationTesting_real_13_redo.txt\n","VerboseConvNetClassificationTesting_real_13.txt\n","VerboseConvNetClassificationTesting_real_14_redo.txt\n","VerboseConvNetClassificationTesting_real_14.txt\n","VerboseConvNetClassificationTesting_real_15_redo.txt\n","VerboseConvNetClassificationTesting_real_15.txt\n","VerboseConvNetClassificationTesting_real_16_redo.txt\n","VerboseConvNetClassificationTesting_real_16.txt\n","VerboseConvNetClassificationTesting_real_17_redo.txt\n","VerboseConvNetClassificationTesting_real_17.txt\n","VerboseConvNetClassificationTesting_real_18.txt\n","VerboseConvNetClassificationTesting_real_19.txt\n","VerboseConvNetClassificationTesting_real_1.txt\n","VerboseConvNetClassificationTesting_real_20.txt\n","VerboseConvNetClassificationTesting_real_21.txt\n","VerboseConvNetClassificationTesting_real_22.txt\n","VerboseConvNetClassificationTesting_real_23.txt\n","VerboseConvNetClassificationTesting_real_24.txt\n","VerboseConvNetClassificationTesting_real_25.txt\n","VerboseConvNetClassificationTesting_real_26.txt\n","VerboseConvNetClassificationTesting_real_27.txt\n","VerboseConvNetClassificationTesting_real_28.txt\n","VerboseConvNetClassificationTesting_real_29.txt\n","VerboseConvNetClassificationTesting_real_2.txt\n","VerboseConvNetClassificationTesting_real_31.txt\n","VerboseConvNetClassificationTesting_real_32.txt\n","VerboseConvNetClassificationTesting_real_33.txt\n","VerboseConvNetClassificationTesting_real_34.txt\n","VerboseConvNetClassificationTesting_real_35.txt\n","VerboseConvNetClassificationTesting_real_36.txt\n","VerboseConvNetClassificationTesting_real_37.txt\n","VerboseConvNetClassificationTesting_real_38.txt\n","VerboseConvNetClassificationTesting_real_39.txt\n","VerboseConvNetClassificationTesting_real_3.txt\n","VerboseConvNetClassificationTesting_real_40.txt\n","VerboseConvNetClassificationTesting_real_41.txt\n","VerboseConvNetClassificationTesting_real_42.txt\n","VerboseConvNetClassificationTesting_real_43.txt\n","VerboseConvNetClassificationTesting_real_45.txt\n","VerboseConvNetClassificationTesting_real_46.txt\n","VerboseConvNetClassificationTesting_real_47.txt\n","VerboseConvNetClassificationTesting_real_48.txt\n","VerboseConvNetClassificationTesting_real_4_pt2.txt\n","VerboseConvNetClassificationTesting_real_4.txt\n","VerboseConvNetClassificationTesting_real_5.txt\n","VerboseConvNetClassificationTesting_real_6_pt2.txt\n","VerboseConvNetClassificationTesting_real_6.txt\n","VerboseConvNetClassificationTesting_real_7.txt\n","VerboseConvNetClassificationTesting_real_9.txt\n","VerboseConvNetClassificationTesting.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MZkErq3QwtBH","colab_type":"code","colab":{}},"source":["# ! wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S6_S8UjXw9Wl","colab_type":"code","colab":{}},"source":["#! wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G24B1SFmxfAO","colab_type":"code","colab":{}},"source":["# ! unzip crawl-300d-2M-subword.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zeEqXBSl45ke","colab_type":"code","colab":{}},"source":["# ! unzip crawl-300d-2M.vec.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cquJQfjE3W9F","colab_type":"code","colab":{}},"source":["int2Label = \\\n","{0: 'Miscellaneous',\n"," 1: 'Video games',\n"," 2: 'Language and literature',\n"," 3: 'Music',\n"," 4: 'Social sciences and society',\n"," 5: 'Sports and recreation',\n"," 6: 'Natural sciences',\n"," 7: 'Art and architecture',\n"," 8: 'History',\n"," 9: 'Warfare',\n"," 10: 'Engineering and technology',\n"," 11: 'Philosophy and religion',\n"," 12: 'Agriculture, food and drink',\n"," 13: 'Geography and places',\n"," 14: 'Mathematics',\n"," 15: 'Media and drama'}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sulAfm2L3W9J","colab_type":"code","colab":{}},"source":["np.random.seed(11747)\n","LOWER = False\n","LEARNING_RATE = 3e-4\n","N_CLASS = len(int2Label.keys())\n","MODELNO = \"real_50\"\n","LOG_FILE = \"ConvNetClassificationTesting_\" + str(MODELNO) + \".txt\"\n","VERBOSE_LOG_FILE = \"VerboseConvNetClassificationTesting_\" + str(MODELNO) + \".txt\"\n","STEP_SIZE = 5\n","GAMMA = 1.0\n","NUMBER_EPOCHS = 10\n","EMBEDDING_DIM = 300"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtjvDxEI3W9M","colab_type":"code","colab":{}},"source":["f = open(LOG_FILE,\"w+\")\n","v = open(VERBOSE_LOG_FILE, \"w+\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVOqYVZQ3W9P","colab_type":"code","colab":{}},"source":["tokenizer = get_tokenizer(\"spacy\")\n","\n","TEXT = Field(sequential=True, tokenize=tokenizer, lower=LOWER, batch_first=True )\n","\n","LABEL = Field(sequential=False, use_vocab=False, batch_first=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-YDPjyO_3W9T","colab_type":"code","colab":{}},"source":["train, val, test = TabularDataset.splits(\".\", \n","                                            train = \"topicclass_train.csv\", \n","                                            validation = \"topicclass_valid_fixed.csv\", \n","                                            test = \"topicclass_test.csv\", \n","                                            format = \"csv\", \n","                                            skip_header = True,\n","                                            fields = [('label', LABEL), ('text', TEXT)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sC-QO3I5JowB","colab_type":"code","colab":{}},"source":["TEXT.build_vocab(train, val, test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PHvrNYVzN_uG","colab_type":"code","colab":{}},"source":["embedding_mtx = torch.load(\"embedding_mtx.pt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXbIf-r-N7Fh","colab_type":"code","colab":{}},"source":["# defined_vocab = set()\n","# num_lines = 2000000\n","\n","# with open(\"crawl-300d-2M.vec\") as vec_file: \n","#   for i, line in enumerate(tqdm_notebook(vec_file, total=num_lines)): \n","#     if i == 0: \n","#       pass\n","#     else: \n","#       word, vector = line.split(\" \", 1)\n","#       if word in TEXT.vocab.stoi: \n","#         embedding_mtx[TEXT.vocab.stoi.get(word)] =  torch.from_numpy(np.fromstring(vector, sep = \" \"))\n","#         defined_vocab.add(word)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvnabwCr3W9Z","colab_type":"code","outputId":"2126fab5-9ea9-465c-d429-31840fe04c4e","executionInfo":{"status":"ok","timestamp":1580708357069,"user_tz":300,"elapsed":120205,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"3iAfx5AD3W9c","colab_type":"code","outputId":"c00aa2ed-ffa8-47e4-b7b5-e01a8f305e02","executionInfo":{"status":"ok","timestamp":1580708357377,"user_tz":300,"elapsed":120318,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print(\"device is: {}\".format(device))\n","f.write(\"device is: {}\\n\".format(device))\n","v.write(\"device is: {}\\n\".format(device))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["device is: cuda:0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["18"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"jICQYWlF3W9g","colab_type":"code","colab":{}},"source":["train_loader, val_loader = Iterator.splits(\n","                                    (train, val), \n","                                    batch_sizes = (64, 64), \n","                                    shuffle = True, \n","                                    sort_key = lambda x: len(x.text), \n","                                    device = device, \n","                                    )\n","\n","test_loader = Iterator(test, batch_size=64, device = device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9GcQlvBx3W9j","colab_type":"code","colab":{}},"source":["dataloaders = {'train': train_loader, 'val': val_loader}\n","dataset_sizes = {'train': len(train), 'val': len(val)}\n","\n","Metric = namedtuple('Metric', ['loss', 'train_error', 'val_error'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DColD6xm7rG","colab_type":"code","colab":{}},"source":["class ConvClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, channels_first = 64, channels_second = 64, kernels = (2,3,4,5), dropout = 0.1): \n","        super().__init__()\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","        \n","        # self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        self.bn4 = nn.BatchNorm1d(channels_first)\n","    \n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(channels_first, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","        conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2,)))\n","\n","        conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out), dim = 1)\n","        conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        pool = nn.MaxPool1d(kernel_size = conv_out.shape[2])\n","#         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","        pooled_out = pool(conv_out).squeeze(2)\n","        #print(f\"embeddings shape {embeddings.shape} and conv out is {conv_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","        pooled_out = self.dropout(pooled_out)\n","        out = self.fc(pooled_out)\n","        \n","        return out\n","        \n","        \n","        \n","class LSTMClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, hidden_size = 256, num_layers = 2, dropout = 0.1): \n","        super().__init__()\n","        self.n_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","\n","        self.rnn = nn.LSTM(input_size = self.embedding_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True, bidirectional = True)\n","        \n","        # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # self.bn4 = nn.BatchNorm1d(channels_first)\n","        \n","    \n","        self.relu = nn.ReLU()\n","        self.tanh = nn.Tanh()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_size*2, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2)))\n","        # conv5_out = self.conv5(F.pad(embeddings.transpose(1,2), (3,2)))\n","        # conv6_out = self.conv6(F.pad(embeddings.transpose(1,2), (3,3)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out, conv5_out, conv6_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        outputs, hiddens = self.rnn(embeddings)\n","        # hiddens[0] -> (num_layers * num_directions, batch, hidden_size)\n","        h_n, _ = hiddens\n","        #print(h_n.shape)\n","        #print(h_n.transpose(0,1).shape)\n","        \n","        # num_layers * num_directions, batch, hidden_size\n","        #hidden_out = h_n.view(embeddings.shape[0], self.n_layers, 2, self.hidden_size)[:, self.n_layers-1].view(embeddings.shape[0], 2*self.hidden_size)\n","        \n","        hidden_out = h_n.reshape(self.n_layers, 2, -1, self.hidden_size)\n","        #print(f\"hidden_out shape right after reshape is {hidden_out.shape}\")\n","        hidden_out = hidden_out[self.n_layers-1]\n","        #print(f\"hidden_out after indexing is {hidden_out.shape}\")\n","        hidden_out = hidden_out.transpose(0,1).reshape(-1, 2*self.hidden_size)\n","        \n","        #hidden_out = h_n.transpose(0,1).reshape(-1, self.hidden_size * 2)\n","        #print(f\"hidden_out shape is {hidden_out.shape}, h_n is {h_n.shape}\")\n","\n","#         pool = nn.MaxPool1d(kernel_size = conv_out.shape[2])\n","# #         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","#         pooled_out = pool(conv_out).squeeze(2)\n","        #print(f\"embeddings shape {embeddings.shape} and conv out is {conv_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","\n","        hidden_out = self.dropout(hidden_out)\n","\n","        out = self.fc(hidden_out)\n","        \n","        return out\n","        \n","        \n","        \n","        \n","        \n","class AttnClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, channels_first = 64, channels_second = 64, kernels = (2,3,4,5), dropout = 0.1): \n","        super().__init__()\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","        \n","        # self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # self.bn4 = nn.BatchNorm1d(channels_first)\n","\n","        self.self_attention1 = Attention(self.embedding_size)\n","        self.self_attention2 = Attention(self.embedding_size)\n","    \n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(self.embedding_size, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","\n","        attn_out = self.self_attention1(embeddings, embeddings)[0]\n","        attn_out = self.self_attention2(attn_out, attn_out)[0] + attn_out\n","\n","\n","\n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2,)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        pool = nn.MaxPool1d(kernel_size = attn_out.shape[1])\n","#         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","        pooled_out = pool(attn_out.transpose(1,2)).squeeze(2)\n","        # print(f\"embeddings shape {embeddings.shape} and attn out is {attn_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","        pooled_out = self.dropout(pooled_out)\n","        out = self.fc(pooled_out)\n","        \n","        return out\n","        \n","        \n","        \n","        \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruh-qXaLmDkq","colab_type":"code","outputId":"828cf213-e94c-4f1b-e077-6d0e74ae8ea0","executionInfo":{"status":"ok","timestamp":1580708363899,"user_tz":300,"elapsed":125166,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":245}},"source":["model1 = torch.load(\"Model_real_22.pt\", map_location=torch.device('cpu'))\n","\n","model2 = torch.load(\"Model_real_27.pt\", map_location=torch.device('cpu'))\n","\n","model3 = torch.load(\"Model_real_31.pt\", map_location=torch.device('cpu'))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"7GkCs6Swp_Jv","colab_type":"code","outputId":"002920cb-3c6a-40bc-af04-ffaa29bfb43f","executionInfo":{"status":"ok","timestamp":1580708374974,"user_tz":300,"elapsed":134968,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":329}},"source":["model1.eval()\n","model2.eval()\n","model3.eval()\n","model1.to(device)\n","model2.to(device)\n","model3.to(device)"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AttnClassifier(\n","  (embedding): Embedding(137968, 300)\n","  (self_attention1): Attention(\n","    (linear_in): Linear(in_features=300, out_features=300, bias=False)\n","    (linear_out): Linear(in_features=600, out_features=300, bias=False)\n","    (softmax): Softmax(dim=-1)\n","    (tanh): Tanh()\n","  )\n","  (self_attention2): Attention(\n","    (linear_in): Linear(in_features=300, out_features=300, bias=False)\n","    (linear_out): Linear(in_features=600, out_features=300, bias=False)\n","    (softmax): Softmax(dim=-1)\n","    (tanh): Tanh()\n","  )\n","  (relu): ReLU()\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=300, out_features=16, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"ckhg0MPFm1Vx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":748},"outputId":"8d84ba81-d06d-48a0-e42e-f39d699cfa91","executionInfo":{"status":"ok","timestamp":1580710210257,"user_tz":300,"elapsed":6565,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["class ConvClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, channels_first = 128, channels_second = 128, kernel_first = 2, kernel_second = 2): \n","        super().__init__()\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","        \n","        #self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first, dilation = 1, kernel_size = kernel_first, padding = kernel_first - 1)\n","        self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        self.conv2 = nn.Conv1d(in_channels=channels_first, out_channels=channels_second, dilation = 1, kernel_size = kernel_second, padding = kernel_second - 1)\n","        self.bn2 = nn.BatchNorm1d(channels_second)\n","    \n","        self.relu = nn.ReLU()\n","\n","        self.dropout = nn.Dropout(0.2)\n","        \n","        self.fc = nn.Linear(channels_first, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            static_embeddings = self.embedding(texts)\n","        embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","        conv1_out = self.conv1(embeddings.transpose(1,2))\n","        conv1_out = self.bn1(conv1_out)\n","        conv1_out = self.relu(conv1_out)\n","        \n","        # conv2_out = self.conv2(conv1_out)\n","        # conv2_out = self.bn2(conv2_out)\n","        # conv2_out = self.relu(conv2_out) \n","        \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        pool = nn.MaxPool1d(kernel_size = conv1_out.shape[2])\n","#         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","        pooled_out = pool(conv1_out).squeeze(2)\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","\n","        #pooled_out = self.dropout(pooled_out)\n","        \n","        out = self.fc(pooled_out)\n","        \n","        return out\n","        \n","model4 = torch.load(\"Model_real_2.pt\", map_location=torch.device('cpu'))\n","model4.to(device)       \n","model4.eval()\n","\n","class ConvClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, channels_first = 64, channels_second = 64, kernels = (2,3,4,5,6,7), dropout = 0.1): \n","        super().__init__()\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","        \n","        # self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        self.bn4 = nn.BatchNorm1d(channels_first)\n","\n","        self.conv5 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[4])\n","        self.bn4 = nn.BatchNorm1d(channels_first)\n","\n","        self.conv6 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[5])\n","        self.bn4 = nn.BatchNorm1d(channels_first)\n","    \n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(channels_first, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","        conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2)))\n","        conv5_out = self.conv5(F.pad(embeddings.transpose(1,2), (3,2)))\n","        conv6_out = self.conv6(F.pad(embeddings.transpose(1,2), (3,3)))\n","\n","        conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out, conv5_out, conv6_out), dim = 1)\n","        conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        pool = nn.MaxPool1d(kernel_size = conv_out.shape[2])\n","#         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","        pooled_out = pool(conv_out).squeeze(2)\n","        #print(f\"embeddings shape {embeddings.shape} and conv out is {conv_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","        pooled_out = self.dropout(pooled_out)\n","        out = self.fc(pooled_out)\n","        \n","        return out\n","        \n","        \n","model5 = torch.load(\"Model_real_24.pt\", map_location=torch.device('cpu'))\n","model5.to(device)       \n","model5.eval()\n","        \n","\n","\n","class LSTMClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, hidden_size = 256, num_layers = 2, dropout = 0.1): \n","        super().__init__()\n","        self.n_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","\n","        self.rnn = nn.LSTM(input_size = self.embedding_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True, bidirectional = True)\n","        \n","        # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # self.bn4 = nn.BatchNorm1d(channels_first)\n","        \n","    \n","        self.relu = nn.ReLU()\n","        self.tanh = nn.Tanh()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_size*2, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2)))\n","        # conv5_out = self.conv5(F.pad(embeddings.transpose(1,2), (3,2)))\n","        # conv6_out = self.conv6(F.pad(embeddings.transpose(1,2), (3,3)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out, conv5_out, conv6_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        outputs, hiddens = self.rnn(embeddings)\n","        # hiddens[0] -> (num_layers * num_directions, batch, hidden_size)\n","        h_n, _ = hiddens\n","        #print(h_n.shape)\n","        #print(h_n.transpose(0,1).shape)\n","        \n","        # num_layers * num_directions, batch, hidden_size\n","        #hidden_out = h_n.view(embeddings.shape[0], self.n_layers, 2, self.hidden_size)[:, self.n_layers-1].view(embeddings.shape[0], 2*self.hidden_size)\n","        \n","        hidden_out = h_n.reshape(self.n_layers, 2, -1, self.hidden_size)\n","        #print(f\"hidden_out shape right after reshape is {hidden_out.shape}\")\n","        hidden_out = hidden_out[self.n_layers-1]\n","        #print(f\"hidden_out after indexing is {hidden_out.shape}\")\n","        hidden_out = hidden_out.transpose(0,1).reshape(-1, 2*self.hidden_size)\n","        \n","        #hidden_out = h_n.transpose(0,1).reshape(-1, self.hidden_size * 2)\n","        #print(f\"hidden_out shape is {hidden_out.shape}, h_n is {h_n.shape}\")\n","\n","#         pool = nn.MaxPool1d(kernel_size = conv_out.shape[2])\n","# #         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","#         pooled_out = pool(conv_out).squeeze(2)\n","        #print(f\"embeddings shape {embeddings.shape} and conv out is {conv_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","\n","        hidden_out = self.dropout(hidden_out)\n","\n","        out = self.fc(hidden_out)\n","        \n","        return out\n","        \n","model6 = torch.load(\"Model_real_28.pt\", map_location=torch.device('cpu'))\n","model6.to(device)       \n","model6.eval()      \n","\n","class LSTMClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, hidden_size = 256, num_layers = 2, dropout = 0.1): \n","        super().__init__()\n","        self.n_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","\n","        self.rnn = nn.LSTM(input_size = self.embedding_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True, bidirectional = True)\n","        \n","        # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # self.bn4 = nn.BatchNorm1d(channels_first)\n","        \n","    \n","        self.relu = nn.ReLU()\n","        self.tanh = nn.Tanh()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_size*2, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2)))\n","        # conv5_out = self.conv5(F.pad(embeddings.transpose(1,2), (3,2)))\n","        # conv6_out = self.conv6(F.pad(embeddings.transpose(1,2), (3,3)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out, conv5_out, conv6_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        outputs, hiddens = self.rnn(embeddings)\n","        # hiddens[0] -> (num_layers * num_directions, batch, hidden_size)\n","        h_n, _ = hiddens\n","        #print(h_n.shape)\n","        #print(h_n.transpose(0,1).shape)\n","        \n","        # num_layers * num_directions, batch, hidden_size\n","        #hidden_out = h_n.view(embeddings.shape[0], self.n_layers, 2, self.hidden_size)[:, self.n_layers-1].view(embeddings.shape[0], 2*self.hidden_size)\n","        \n","        hidden_out = h_n.reshape(self.n_layers, 2, -1, self.hidden_size)\n","        #print(f\"hidden_out shape right after reshape is {hidden_out.shape}\")\n","        hidden_out = hidden_out[self.n_layers-1]\n","        #print(f\"hidden_out after indexing is {hidden_out.shape}\")\n","        hidden_out = hidden_out.transpose(0,1).reshape(-1, 2*self.hidden_size)\n","        \n","        #hidden_out = h_n.transpose(0,1).reshape(-1, self.hidden_size * 2)\n","        #print(f\"hidden_out shape is {hidden_out.shape}, h_n is {h_n.shape}\")\n","\n","#         pool = nn.MaxPool1d(kernel_size = conv_out.shape[2])\n","# #         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","#         pooled_out = pool(conv_out).squeeze(2)\n","        #print(f\"embeddings shape {embeddings.shape} and conv out is {conv_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","\n","        hidden_out = self.dropout(hidden_out)\n","\n","        out = self.fc(hidden_out)\n","        \n","        return out\n","        \n","model7 = torch.load(\"Model_real_29.pt\", map_location=torch.device('cpu'))\n","model7.to(device)       \n","model7.eval()      \n","                \n","\n","class AttnClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, channels_first = 64, channels_second = 64, kernels = (2,3,4,5), dropout = 0.1): \n","        super().__init__()\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","        \n","        # self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # self.bn4 = nn.BatchNorm1d(channels_first)\n","\n","        self.self_attention = Attention(self.embedding_size)\n","    \n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(self.embedding_size, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","\n","        attn_out, _ = self.self_attention(embeddings, embeddings)\n","\n","\n","\n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2,)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        pool = nn.MaxPool1d(kernel_size = attn_out.shape[1])\n","#         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","        pooled_out = pool(attn_out.transpose(1,2)).squeeze(2)\n","        # print(f\"embeddings shape {embeddings.shape} and attn out is {attn_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","        pooled_out = self.dropout(pooled_out)\n","        out = self.fc(pooled_out)\n","        \n","        return out\n","\n","        \n","model8 = torch.load(\"Model_real_32.pt\", map_location=torch.device('cpu'))\n","model8.to(device)       \n","model8.eval()                     \n","\n","class AttnClassifier(nn.Module): \n","    def __init__(self, embeddings, n_class, channels_first = 64, channels_second = 64, kernels = (2,3,4,5), dropout = 0.1): \n","        super().__init__()\n","        self.vocab_size, self.embedding_size = embeddings.shape\n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        self.embedding.weight.data.copy_(embeddings)\n","        \n","        # self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # self.bn4 = nn.BatchNorm1d(channels_first)\n","\n","        self.self_attention1 = Attention(self.embedding_size)\n","        self.self_attention2 = Attention(self.embedding_size)\n","        self.self_attention_query = Attention(self.embedding_size)\n","\n","\n","        self.softmax = nn.Softmax(dim = 1)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(self.embedding_size, n_class)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            embeddings = self.embedding(texts)\n","        #embeddings = static_embeddings\n","        #embeddings = static_embeddings + self.embedding_delta(texts)\n","\n","        attn_out = self.self_attention1(embeddings, embeddings)[0]\n","        attn_out = self.self_attention2(attn_out, attn_out)[0] + attn_out\n","\n","        attn_query = self.self_attention_query(attn_out, attn_out)[0]\n","        \n","        raw_energy = torch.mul(attn_query, attn_out).sum(axis = 2)\n","        normalized_energy = self.softmax(raw_energy).reshape(embeddings.shape[0], 1, -1)\n","        \n","        attn_contexts = torch.bmm(normalized_energy, attn_out).reshape(embeddings.shape[0], self.embedding_size)\n","    \n","\n","\n","\n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2,)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","        # pool = nn.MaxPool1d(kernel_size = attn_out.shape[1])\n","\n","#         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","        # pooled_out = pool(attn_out.transpose(1,2)).squeeze(2)\n","        # print(f\"embeddings shape {embeddings.shape} and attn out is {attn_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","#         print(f\"pooled out shape is {pooled_out.shape}\")\n","        pooled_out = self.dropout(attn_contexts)\n","        out = self.fc(pooled_out)\n","        \n","        return out\n","\n","        \n","model9 = torch.load(\"Model_real_38.pt\", map_location=torch.device('cpu'))\n","model9.to(device)       \n","model9.eval()             \n","        "],"execution_count":53,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Softmax' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Tanh' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["AttnClassifier(\n","  (embedding): Embedding(137968, 300)\n","  (self_attention1): Attention(\n","    (linear_in): Linear(in_features=300, out_features=300, bias=False)\n","    (linear_out): Linear(in_features=600, out_features=300, bias=False)\n","    (softmax): Softmax(dim=-1)\n","    (tanh): Tanh()\n","  )\n","  (self_attention2): Attention(\n","    (linear_in): Linear(in_features=300, out_features=300, bias=False)\n","    (linear_out): Linear(in_features=600, out_features=300, bias=False)\n","    (softmax): Softmax(dim=-1)\n","    (tanh): Tanh()\n","  )\n","  (self_attention_query): Attention(\n","    (linear_in): Linear(in_features=300, out_features=300, bias=False)\n","    (linear_out): Linear(in_features=600, out_features=300, bias=False)\n","    (softmax): Softmax(dim=-1)\n","    (tanh): Tanh()\n","  )\n","  (softmax): Softmax(dim=1)\n","  (relu): ReLU()\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc): Linear(in_features=300, out_features=16, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"gFzqsPUMtwEw","colab_type":"code","outputId":"139d52c4-b159-4681-cb25-4c970e677b4d","executionInfo":{"status":"ok","timestamp":1580708374975,"user_tz":300,"elapsed":133967,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"SRYgdJaspGPB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":294},"outputId":"578bd5cc-cbda-44a2-a5f6-a6d79f4f2ce5","executionInfo":{"status":"ok","timestamp":1580709937800,"user_tz":300,"elapsed":181,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}}},"source":["print(model5)"],"execution_count":49,"outputs":[{"output_type":"stream","text":["ConvClassifier(\n","  (embedding): Embedding(137968, 300)\n","  (conv1): Conv1d(300, 86, kernel_size=(2,), stride=(1,))\n","  (bn1): BatchNorm1d(516, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv2): Conv1d(300, 86, kernel_size=(3,), stride=(1,))\n","  (bn2): BatchNorm1d(516, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv3): Conv1d(300, 86, kernel_size=(4,), stride=(1,))\n","  (bn3): BatchNorm1d(516, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv4): Conv1d(300, 86, kernel_size=(5,), stride=(1,))\n","  (bn4): BatchNorm1d(516, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv5): Conv1d(300, 86, kernel_size=(6,), stride=(1,))\n","  (conv6): Conv1d(300, 86, kernel_size=(7,), stride=(1,))\n","  (relu): ReLU()\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=516, out_features=16, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LNGvffDT3W9m","colab_type":"code","colab":{}},"source":["class EnsembleClassifier(nn.Module): \n","    def __init__(self, models, n_class, dropout = 0.5): \n","        super().__init__()\n","        # self.vocab_size, self.embedding_size = embeddings.shape\n","        # self.embedding = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        # self.embedding.weight.data.copy_(embeddings)\n","        \n","        # # self.embedding_delta = nn.Embedding(self.vocab_size, self.embedding_size, sparse = False)\n","        \n","        # # self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[0])\n","        # # self.bn1 = nn.BatchNorm1d(channels_first)\n","        \n","        # # self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[1])\n","        # # self.bn2 = nn.BatchNorm1d(channels_first)\n","\n","        # # self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[2])\n","        # # self.bn3 = nn.BatchNorm1d(channels_first)\n","\n","        # # self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=channels_first//len(kernels), dilation = 1, kernel_size = kernels[3])\n","        # # self.bn4 = nn.BatchNorm1d(channels_first)\n","\n","        # self.self_attention1 = Attention(self.embedding_size)\n","        # self.self_attention2 = Attention(self.embedding_size)\n","        # self.self_attention_query = Attention(self.embedding_size)\n","\n","\n","        # self.softmax = nn.Softmax(dim = 1)\n","        # self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.tanh = nn.Tanh()\n","\n","        self.model1 = models[0]\n","        self.model2 = models[1]\n","        self.model3 = models[2]\n","        # self.model4 = models[3]\n","        # self.model5 = models[4]\n","        # self.model6 = models[5]\n","        # self.model7 = models[6]\n","        # self.model8 = models[7]\n","        # self.model9 = models[8]\n","\n","        \n","        self.fc = nn.Linear(n_class*3, n_class)\n","        self.softmax = nn.Softmax(dim = 1)\n","        \n","    def forward(self, texts): \n","        with torch.no_grad(): \n","            # model1_out = 1/9 * self.softmax(self.model1(texts))\n","            # model2_out = 1/9 * self.softmax(self.model2(texts))\n","            # model3_out = 1/9 * self.softmax(self.model3(texts))\n","            # model4_out = 1/9 * self.softmax(self.model4(texts))\n","            # model5_out = 1/9 * self.softmax(self.model5(texts))\n","            # model6_out = 1/9 * self.softmax(self.model6(texts))\n","            # model7_out = 1/9 * self.softmax(self.model7(texts))\n","            # model8_out = 1/9 * self.softmax(self.model8(texts))\n","            # model9_out = 1/9 * self.softmax(self.model9(texts))\n","\n","            model1_out = 1/3 * torch.log((self.softmax(self.model1(texts))))\n","            model2_out = 1/3 * torch.log(self.softmax(self.model2(texts)))\n","            model3_out = 1/3 * torch.log(self.softmax(self.model3(texts)))\n","            # model4_out = 1/9 * torch.log(self.softmax(self.model4(texts)))\n","            # model5_out = 1/9 * torch.log(self.softmax(self.model5(texts)))\n","            # model6_out = 1/9 * torch.log(self.softmax(self.model6(texts)))\n","            # model7_out = 1/9 * torch.log(self.softmax(self.model7(texts)))\n","            # model8_out = 1/9 * torch.log(self.softmax(self.model8(texts)))\n","            # model9_out = 1/9 * torch.log(self.softmax(self.model9(texts)))\n","            \n","            out = model1_out+model2_out+model3_out#+model4_out+model5_out+model6_out+model7_out+model8_out+model9_out\n","            out = self.softmax(out)\n","\n","\n","            # stacked_out = torch.stack([model1_out, model2_out, model3_out])\n","\n","            \n","            # out = torch.mean(stacked_out, dim=0)\n","            #stacked_out = self.dropout(self.tanh(stacked_out))\n","            \n","        #out = self.fc(stacked_out)\n","\n","        # attn_query = self.self_attention_query(attn_out, attn_out)[0]\n","\n","        \n","        # raw_energy = torch.mul(attn_query, attn_out).sum(2)\n","        # normalized_energy = self.softmax(raw_energy).reshape(embeddings.shape[0], 1, -1)\n","        \n","        # attn_contexts = torch.bmm(normalized_energy, attn_out).reshape(embeddings.shape[0], self.embedding_size)\n","    \n","        # conv1_out = self.conv1(F.pad(embeddings.transpose(1,2), (1,0)))\n","        # conv2_out = self.conv2(F.pad(embeddings.transpose(1,2), (1,1)))\n","        # conv3_out = self.conv3(F.pad(embeddings.transpose(1,2), (2,1)))\n","        # conv4_out = self.conv4(F.pad(embeddings.transpose(1,2), (2,2,)))\n","\n","        # conv_out = torch.cat((conv1_out, conv2_out, conv3_out, conv4_out), dim = 1)\n","        # conv_out = self.relu(conv_out)\n","      \n","        # if conv1_out.shape == conv2_out.shape: \n","        #     conv2_out += conv1_out\n","        \n","#         pool = nn.MaxPool1d(kernel_size = attn_out.shape[1])\n","\n","# #         print(f\"conv2 shape is {conv2_out.shape}\")\n","        \n","#         pooled_out = pool(attn_out.transpose(1,2)).squeeze(2)\n","#         # print(f\"embeddings shape {embeddings.shape} and attn out is {attn_out.shape} and pooled_out is {pooled_out.shape}\")\n","        \n","# #         print(f\"pooled out shape is {pooled_out.shape}\")\n","        \n","        return out\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OyoVNg0WK_Pe","colab_type":"code","colab":{}},"source":["def init_weights(m):\n","    if type(m) == nn.Conv1d or type(m) == nn.Linear:\n","        nn.init.xavier_normal_(m.weight.data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlEnuEjy3W9p","colab_type":"code","colab":{}},"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25, lambda_reg = .0001):\n","\n","    since = time.time()\n","    \n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        f.write('Epoch {}/{}\\n'.format(epoch+1, num_epochs))\n","        v.write('Epoch {}/{}\\n'.format(epoch+1, num_epochs))\n","        f.write('-' *10)\n","        v.write('-' * 10)\n","\n","        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0.0\n","\n","            # Iterate over data.\n","            f.write(\"\\nstarting epoch {} for {} phase\\n\".format(epoch+1, phase))\n","            v.write(\"\\nstarting epoch {} for {} phase\\n\".format(epoch+1, phase))\n","            print(\"starting epoch {} for {} phase\".format(epoch+1, phase))\n","\n","            for i, data in enumerate(tqdm_notebook(dataloaders[phase])):\n","                #pdb.set_trace()\n","                inputs = data.text.to(device)\n","                labels = data.label.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels) \n","                    # for i, param in enumerate(model.parameters()): \n","                    #   if i == 1: \n","                    #     loss += lambda_reg * torch.norm(param)\n","\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","                if ((i%300) == 0):\n","                    #print(\"inside\")\n","                    v.write(\"inputs size: {}\\n\".format(inputs.size(0)))\n","                    v.write(\"epoch {}, batch {},  loss : {}\\n\".format(epoch+1, i, loss.item()))\n","                    v.write(\"percent correct: {}\\n\".format((torch.sum(preds == labels.data)/inputs.size(0))))\n","            \n","            \n","            epoch_loss = running_loss / dataset_sizes[phase]\n","\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            if phase == 'train':\n","\n","                train_loss = epoch_loss\n","                train_error = 1 - epoch_acc\n","                scheduler.step()\n","\n","            elif phase == 'val': \n","\n","                val_error = 1 - epoch_acc\n","                metrics.append(Metric(loss=train_loss, train_error=train_error,val_error=val_error))\n","            \n","            f.write('{} Loss: {:.4f} Acc: {:.4f}\\n'.format(phase, epoch_loss, epoch_acc))\n","            v.write('{} Loss: {:.4f} Acc: {:.4f}\\n'.format(phase, epoch_loss, epoch_acc))\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                PATH = 'MobileNetModel_' + str(epoch+2) + '.pt'\n","                torch.save(model, PATH)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    f.write('Training complete in {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n","    v.write('Training complete in {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n","\n","\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    f.write('Best val Acc: {:4f}'.format(best_acc))\n","    v.write('Best val Acc: {:4f}'.format(best_acc))\n","\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u_85BRlFtFm5","colab_type":"code","colab":{}},"source":["# for named_param in model.named_parameters(): \n","#   print(named_param)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQSVyWye3W9s","colab_type":"code","colab":{}},"source":["#model = EnsembleClassifier((model1, model2, model3, model4, model5, model6, model7, model8, model9), N_CLASS, dropout=0)\n","model = EnsembleClassifier((model1, model2, model3), N_CLASS, dropout=0)\n","model = model.to(device)\n","#model.apply(init_weights)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ogDkGH3K3W9w","colab_type":"code","outputId":"bcf91ddf-7648-499e-870e-aa23cd404ffc","executionInfo":{"status":"error","timestamp":1580704402825,"user_tz":300,"elapsed":106,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":236}},"source":["metrics = []\n","model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=NUMBER_EPOCHS, lambda_reg = 0)\n","\n","f.close()\n","v.close()\n","torch.save(model, \"Model_\" + str(MODELNO) + \".pt\")\n","\n","def training_plot(metrics):\n","    plt.figure(1)\n","    plt.plot([m.val_error for m in metrics], 'b')\n","    plt.plot([m.train_error for m in metrics], 'r')\n","    plt.title('Train Error (red), Val Error (blue)')\n","    plt.savefig('Model_' + str(MODELNO) + '.png')\n"," \n","training_plot(metrics)\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-c8f8287fde7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUMBER_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"KN5bLsQF_J7m","colab_type":"code","outputId":"9e1d6dd3-a58e-4cb7-dfb1-dac8b4321f0e","executionInfo":{"status":"ok","timestamp":1580710446344,"user_tz":300,"elapsed":471,"user":{"displayName":"Alexander Shypula","photoUrl":"","userId":"04633353775377489102"}},"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["865e31f304284ed4a3c24e34ab6e6de4","6b36a64f64b14738a2336802016da3b6","7a80dc295bef4611bba36c442578b738","e0ef0cd4ef654d058036fa2f705902cb","53d408c7a4844e8e9808fcc96f9e299d","82d91769a2364fe8849bc7700e39bc7f","4f6c9ecad8f443ca9d0653a44b98d4ba","b8dee04607614627a2c6a5f28370cc5b"]}},"source":["running_loss = 0.0\n","running_corrects = 0\n","for i, data in enumerate(tqdm_notebook(val_loader)):\n","  #pdb.set_trace()\n","  inputs = data.text.to(device)\n","  labels = data.label.to(device)\n","\n","  # zero the parameter gradients\n","  optimizer.zero_grad()\n","\n","  # forward\n","  # track history if only in train\n","  with torch.no_grad():\n","      outputs = model(inputs)\n","      #print(outputs.shape)\n","\n","      _, preds = torch.max(outputs, 1)\n","      loss = criterion(outputs, labels) \n","      # for i, param in enumerate(model.parameters()): \n","      #   if i == 1: \n","      #     loss += lambda_reg * torch.norm(param)\n","\n","\n","  # statistics\n","  running_loss += loss.item() * inputs.size(0)\n","  running_corrects += torch.sum(preds == labels.data)\n","\n","\n","epoch_loss = running_loss / len(val)\n","\n","epoch_acc = running_corrects.double() / len(val)\n","print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))"],"execution_count":65,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"865e31f304284ed4a3c24e34ab6e6de4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Loss: 2.0645 Acc: 0.8802\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NQCrYT2fyx8V","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}